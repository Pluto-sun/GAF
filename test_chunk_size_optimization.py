#!/usr/bin/env python3
"""
å—å¤§å°ä¼˜åŒ–æµ‹è¯•è„šæœ¬
æµ‹è¯•GAFè®¡ç®—å’Œæ•°æ®èŒƒå›´è½¬æ¢åœ¨ä¸åŒå—å¤§å°å’Œå¤„ç†æ¨¡å¼ä¸‹çš„æœ€ä¼˜é…ç½®

åŠŸèƒ½:
1. æµ‹è¯•GAFè®¡ç®—çš„æœ€ä¼˜å—å¤§å° (æ ‡å‡†å¤šè¿›ç¨‹ vs å…±äº«å†…å­˜)
2. æµ‹è¯•æ•°æ®ç±»å‹è½¬æ¢çš„æœ€ä¼˜å—å¤§å° (æ ‡å‡†å¤šè¿›ç¨‹ vs å…±äº«å†…å­˜)  
3. å•çº¿ç¨‹åŸºå‡†æ€§èƒ½æµ‹è¯•
4. ç³»ç»Ÿèµ„æºç›‘æ§å’Œæ¨è
5. æµ‹è¯•æ ·æœ¬æ•°: 5000

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import os
import sys
import time
import numpy as np
import multiprocessing as mp
import psutil
import gc
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from contextlib import contextmanager
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')
from data_provider.data_loader.DualGAFDataLoader import DualGAFDataManager


class ChunkSizeOptimizer:
    """å—å¤§å°ä¼˜åŒ–æµ‹è¯•å™¨"""
    
    def __init__(self, test_samples=5000):
        self.test_samples = test_samples
        self.cpu_count = mp.cpu_count()
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        
        print(f"=== å—å¤§å°ä¼˜åŒ–æµ‹è¯•å™¨åˆå§‹åŒ– ===")
        print(f"ğŸ–¥ï¸  CPUæ ¸å¿ƒæ•°: {self.cpu_count}")
        print(f"ğŸ’¾ æ€»å†…å­˜: {self.memory_gb:.1f}GB")
        print(f"ğŸ”¢ æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
        print()
        
        # æµ‹è¯•é…ç½® - ä¼˜åŒ–åçš„æµ‹è¯•èŒƒå›´ï¼ˆå‡å°‘æµ‹è¯•æ—¶é—´ï¼‰
        self.chunk_sizes = [100, 200, 500, 1000, 1500]  # å‡å°‘æµ‹è¯•ç‚¹
        self.n_jobs_list = [4, 8, min(12, self.cpu_count)]  # é‡ç‚¹æµ‹è¯•å¸¸ç”¨é…ç½®
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        self.test_data = self._generate_test_data()
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ: {self.test_data.shape}")
        print(f"ğŸ’» æ•°æ®å¤§å°: {self.test_data.nbytes / 1024**3:.2f}GB")
        
        # ç»“æœå­˜å‚¨
        self.gaf_results = []
        self.conversion_results = []
        
    def _generate_test_data(self):
        """ç”Ÿæˆæ ‡å‡†åŒ–çš„æµ‹è¯•æ•°æ®"""
        print("ğŸ”„ ç”Ÿæˆæµ‹è¯•æ•°æ®...")
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        np.random.seed(42)
        
        # ç”Ÿæˆç±»ä¼¼å®é™…å·¥å†µæ•°æ®çš„ç‰¹å¾
        seq_len = 96  # æ—¶é—´åºåˆ—é•¿åº¦
        n_features = 26  # ç‰¹å¾æ•°é‡
        
        # ç”Ÿæˆå¤šç§æ¨¡å¼çš„æ—¶é—´åºåˆ—æ•°æ®
        data = []
        for i in range(self.test_samples):
            if i % 1000 == 0:
                print(f"  ç”Ÿæˆæ ·æœ¬ {i}/{self.test_samples}")
                
            # æ··åˆä¸åŒçš„ä¿¡å·æ¨¡å¼
            t = np.linspace(0, 24, seq_len)  # 24å°æ—¶
            sample = np.zeros((seq_len, n_features))
            
            for j in range(n_features):
                # åŸºç¡€è¶‹åŠ¿ + å‘¨æœŸæ€§ + å™ªå£°
                trend = 0.1 * t + np.random.normal(0, 0.05)
                periodic = np.sin(2 * np.pi * t / 12) * (0.3 + 0.2 * np.random.random())
                noise = np.random.normal(0, 0.1, seq_len)
                
                # å¶å°”æ·»åŠ å¼‚å¸¸æ¨¡å¼
                if np.random.random() < 0.1:
                    anomaly_start = np.random.randint(0, seq_len - 10)
                    anomaly_end = anomaly_start + np.random.randint(5, 15)
                    anomaly_end = min(anomaly_end, seq_len)
                    trend[anomaly_start:anomaly_end] += np.random.normal(0.5, 0.2)
                
                sample[:, j] = trend + periodic + noise
            
            # å½’ä¸€åŒ–åˆ°[-1, 1]åŒºé—´
            sample = 2 * (sample - sample.min()) / (sample.max() - sample.min()) - 1
            data.append(sample)
        
        return np.array(data, dtype=np.float32)
    
    @contextmanager
    def monitor_performance(self, test_name):
        """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        print(f"ğŸš€ å¼€å§‹æµ‹è¯•: {test_name}")
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_memory = psutil.virtual_memory().used / 1024**3
        start_cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        try:
            yield
        finally:
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            end_memory = psutil.virtual_memory().used / 1024**3
            end_cpu_percent = psutil.cpu_percent(interval=0.1)
            
            duration = end_time - start_time
            memory_used = end_memory - start_memory
            
            print(f"âœ… {test_name} å®Œæˆ")
            print(f"   â±ï¸  è€—æ—¶: {duration:.2f}s")
            print(f"   ğŸ’¾ å†…å­˜å˜åŒ–: {memory_used:+.2f}GB")
            print(f"   ğŸ”¥ CPUä½¿ç”¨ç‡: {end_cpu_percent:.1f}%")
            print()
    
    def test_single_thread_baseline(self):
        """å•çº¿ç¨‹åŸºå‡†æµ‹è¯•"""
        print("=" * 60)
        print("ğŸ“ å•çº¿ç¨‹åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        results = {}
        
        # GAFç”ŸæˆåŸºå‡†æµ‹è¯•
        with self.monitor_performance("å•çº¿ç¨‹ GAF ç”Ÿæˆ (Summation)"):
            start_time = time.time()
            gaf_result = self._single_thread_gaf(self.test_data, "summation")
            gaf_time = time.time() - start_time
            results['gaf_summation_time'] = gaf_time
            print(f"   ğŸ“Š GAFè¾“å‡ºå½¢çŠ¶: {gaf_result.shape}")
        
        # with self.monitor_performance("å•çº¿ç¨‹ GAF ç”Ÿæˆ (Difference)"):
        #     start_time = time.time()
        #     gaf_diff_result = self._single_thread_gaf(self.test_data, "difference")
        #     gaf_diff_time = time.time() - start_time
        #     results['gaf_difference_time'] = gaf_diff_time
        
        # æ•°æ®ç±»å‹è½¬æ¢åŸºå‡†æµ‹è¯•
        with self.monitor_performance("å•çº¿ç¨‹æ•°æ®è½¬æ¢ (uint8)"):
            start_time = time.time()
            uint8_result = self._single_thread_conversion(gaf_result, np.uint8, 255)
            uint8_time = time.time() - start_time
            results['conversion_uint8_time'] = uint8_time
            print(f"   ğŸ“Š è½¬æ¢åæ•°æ®èŒƒå›´: [{uint8_result.min()}, {uint8_result.max()}]")
        
        # with self.monitor_performance("å•çº¿ç¨‹æ•°æ®è½¬æ¢ (uint16)"):
        #     start_time = time.time()
        #     uint16_result = self._single_thread_conversion(gaf_result, np.uint16, 65535)
        #     uint16_time = time.time() - start_time
        #     results['conversion_uint16_time'] = uint16_time
        
        # with self.monitor_performance("å•çº¿ç¨‹æ•°æ®è½¬æ¢ (float32)"):
        #     start_time = time.time()
        #     float32_result = self._single_thread_conversion_float32(gaf_result)
        #     float32_time = time.time() - start_time
        #     results['conversion_float32_time'] = float32_time
        
        # æ¸…ç†å†…å­˜
        del gaf_result, uint8_result
        gc.collect()
        
        self.baseline_results = results
        print(f"ğŸ“ˆ å•çº¿ç¨‹åŸºå‡†ç»“æœ:")
        for key, value in results.items():
            print(f"   {key}: {value:.2f}s")
        print()
        
        return results
    
    def _create_mock_baseline(self):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„åŸºå‡†æµ‹è¯•ç»“æœï¼ˆé¿å…è€—æ—¶çš„å•çº¿ç¨‹æµ‹è¯•ï¼‰"""
        print("ğŸ”¢ åˆ›å»ºä¼°ç®—åŸºå‡†æ—¶é—´...")
        
        # åŸºäºç»éªŒå…¬å¼ä¼°ç®—å•çº¿ç¨‹æ—¶é—´
        # GAFè®¡ç®—å¤æ‚åº¦å¤§çº¦æ˜¯ O(N * D * T^2)
        N, T, D = self.test_data.shape
        
        # ä¼°ç®—GAFè®¡ç®—æ—¶é—´ï¼ˆåŸºäºç»éªŒå€¼ï¼‰
        gaf_time_per_sample = 0.015  # æ¯ä¸ªæ ·æœ¬å¤§çº¦15msï¼ˆç»éªŒå€¼ï¼‰
        estimated_gaf_time = N * gaf_time_per_sample
        
        # ä¼°ç®—æ•°æ®è½¬æ¢æ—¶é—´ï¼ˆåŸºäºå†…å­˜å¸¦å®½ï¼‰
        data_size_gb = self.test_data.nbytes / (1024**3)
        estimated_conversion_time = data_size_gb * 8  # å‡è®¾8ç§’/GBçš„è½¬æ¢æ—¶é—´
        
        self.baseline_results = {
            'gaf_summation_time': estimated_gaf_time,
            'conversion_uint8_time': estimated_conversion_time,
        }
        
        print(f"ğŸ“Š ä¼°ç®—åŸºå‡†æ—¶é—´:")
        print(f"   GAFç”Ÿæˆ: {estimated_gaf_time:.2f}s")
        print(f"   æ•°æ®è½¬æ¢: {estimated_conversion_time:.2f}s")
        print(f"ğŸ’¡ æ³¨æ„: è¿™äº›æ˜¯ä¼°ç®—å€¼ï¼Œå®é™…åŠ é€Ÿæ¯”å¯èƒ½æœ‰åå·®")
        print()
        
        return self.baseline_results
    
    def test_gaf_chunk_sizes(self):
        """æµ‹è¯•GAFè®¡ç®—çš„æœ€ä¼˜å—å¤§å°"""
        print("=" * 60)
        print("ğŸ§® GAFè®¡ç®—å—å¤§å°ä¼˜åŒ–æµ‹è¯•")
        print("=" * 60)
        
        baseline_time = self.baseline_results['gaf_summation_time']
        
        for n_jobs in [4, 8, min(12, self.cpu_count)]:
            print(f"\n--- æµ‹è¯•è¿›ç¨‹æ•°: {n_jobs} ---")
            
            for chunk_size in self.chunk_sizes:
                if chunk_size > self.test_samples // 2:
                    continue  # è·³è¿‡è¿‡å¤§çš„å—å¤§å°
                
                # æµ‹è¯•æ ‡å‡†å¤šè¿›ç¨‹
                with self.monitor_performance(f"æ ‡å‡†å¤šè¿›ç¨‹ GAF (å—å¤§å°: {chunk_size})"):
                    start_time = time.time()
                    result_standard = self._test_standard_multiprocess_gaf(
                        self.test_data, "summation", n_jobs, chunk_size
                    )
                    standard_time = time.time() - start_time
                    speedup_standard = baseline_time / standard_time if standard_time > 0 else 0
                
                # æµ‹è¯•å…±äº«å†…å­˜æ¨¡å¼
                with self.monitor_performance(f"å…±äº«å†…å­˜ GAF (å—å¤§å°: {chunk_size})"):
                    start_time = time.time()
                    result_shared = self._test_shared_memory_gaf(
                        self.test_data, "summation", n_jobs, chunk_size
                    )
                    shared_time = time.time() - start_time
                    speedup_shared = baseline_time / shared_time if shared_time > 0 else 0
                
                # éªŒè¯ç»“æœä¸€è‡´æ€§ (ä¼˜åŒ–ç‰ˆæœ¬ - é‡‡æ ·æ£€æŸ¥)
                # consistency_check = self._fast_consistency_check(result_standard, result_shared)
                
                # è®°å½•ç»“æœ
                result_entry = {
                    'test_type': 'gaf_generation',
                    'n_jobs': n_jobs,
                    'chunk_size': chunk_size,
                    'standard_time': standard_time,
                    'shared_time': shared_time,
                    'baseline_time': baseline_time,
                    'standard_speedup': speedup_standard,
                    'shared_speedup': speedup_shared,
                    'samples_per_chunk': chunk_size,
                    'total_chunks': max(1, self.test_samples // chunk_size)
                }
                self.gaf_results.append(result_entry)
                
                print(f"   ğŸ“Š æ ‡å‡†å¤šè¿›ç¨‹: {standard_time:.2f}s (åŠ é€Ÿ: {speedup_standard:.2f}x)")
                print(f"   ğŸš€ å…±äº«å†…å­˜: {shared_time:.2f}s (åŠ é€Ÿ: {speedup_shared:.2f}x)")
                # print(f"   âœ… ç»“æœä¸€è‡´æ€§: {consistency_check}")
                
                # æ¸…ç†å†…å­˜
                del result_standard, result_shared
                gc.collect()
        
        return self.gaf_results
    
    def test_conversion_chunk_sizes(self):
        """æµ‹è¯•æ•°æ®ç±»å‹è½¬æ¢çš„æœ€ä¼˜å—å¤§å°"""
        print("=" * 60)
        print("ğŸ”„ æ•°æ®ç±»å‹è½¬æ¢å—å¤§å°ä¼˜åŒ–æµ‹è¯•")
        print("=" * 60)
        
        # å…ˆç”ŸæˆGAFæ•°æ®ç”¨äºè½¬æ¢æµ‹è¯•
        print("ğŸ”„ ç”ŸæˆGAFæ•°æ®ç”¨äºè½¬æ¢æµ‹è¯•...")
        gaf_data = self._single_thread_gaf(self.test_data, "summation")
        print(f"ğŸ“Š GAFæ•°æ®å½¢çŠ¶: {gaf_data.shape}, å¤§å°: {gaf_data.nbytes / 1024**3:.2f}GB")
        
        baseline_time = self.baseline_results['conversion_uint8_time']
        
        for n_jobs in [4, 8, min(12, self.cpu_count)]:
            print(f"\n--- æµ‹è¯•çº¿ç¨‹æ•°: {n_jobs} ---")
            
            for chunk_size in self.chunk_sizes:
                if chunk_size > self.test_samples // 2:
                    continue
                
                # æµ‹è¯•æ ‡å‡†å¤šçº¿ç¨‹è½¬æ¢
                with self.monitor_performance(f"æ ‡å‡†å¤šçº¿ç¨‹è½¬æ¢ (å—å¤§å°: {chunk_size})"):
                    start_time = time.time()
                    result_standard = self._test_standard_multithread_conversion(
                        gaf_data, np.uint8, 255, n_jobs, chunk_size
                    )
                    standard_time = time.time() - start_time
                    speedup_standard = baseline_time / standard_time if standard_time > 0 else 0
                
                # æµ‹è¯•å…±äº«å†…å­˜è½¬æ¢
                with self.monitor_performance(f"å…±äº«å†…å­˜è½¬æ¢ (å—å¤§å°: {chunk_size})"):
                    start_time = time.time()
                    result_shared = self._test_shared_memory_conversion(
                        gaf_data, np.uint8, 255, n_jobs, chunk_size
                    )
                    shared_time = time.time() - start_time
                    speedup_shared = baseline_time / shared_time if shared_time > 0 else 0
                
                # éªŒè¯ç»“æœä¸€è‡´æ€§ (ä¼˜åŒ–ç‰ˆæœ¬ - é‡‡æ ·æ£€æŸ¥)
                # consistency_check = self._fast_consistency_check(result_standard, result_shared)
                
                # è®°å½•ç»“æœ
                result_entry = {
                    'test_type': 'data_conversion',
                    'n_jobs': n_jobs,
                    'chunk_size': chunk_size,
                    'standard_time': standard_time,
                    'shared_time': shared_time,
                    'baseline_time': baseline_time,
                    'standard_speedup': speedup_standard,
                    'shared_speedup': speedup_shared,
                    # 'consistency_check': consistency_check,
                    'samples_per_chunk': chunk_size,
                    'total_chunks': max(1, self.test_samples // chunk_size)
                }
                self.conversion_results.append(result_entry)
                
                print(f"   ğŸ“Š æ ‡å‡†å¤šçº¿ç¨‹: {standard_time:.2f}s (åŠ é€Ÿ: {speedup_standard:.2f}x)")
                print(f"   ğŸš€ å…±äº«å†…å­˜: {shared_time:.2f}s (åŠ é€Ÿ: {speedup_shared:.2f}x)")
                # print(f"   âœ… ç»“æœä¸€è‡´æ€§: {consistency_check}")
                
                # æ¸…ç†å†…å­˜
                del result_standard, result_shared
                gc.collect()
        
        # æ¸…ç†GAFæ•°æ®
        del gaf_data
        gc.collect()
        
        return self.conversion_results
    
    def _single_thread_gaf(self, data, method):
        """å•çº¿ç¨‹GAFç”Ÿæˆ"""
        from pyts.image import GramianAngularField
        
        N, T, D = data.shape
        transposed_data = data.transpose(0, 2, 1)
        flattened_data = transposed_data.reshape(-1, T)
        gasf = GramianAngularField(method=method)
        batch_gaf = gasf.fit_transform(flattened_data)
        reshaped_gaf = batch_gaf.reshape(N, D, T, T)
        return reshaped_gaf.astype(np.float32)
    
    def _single_thread_conversion(self, data, dtype, max_val):
        """å•çº¿ç¨‹æ•°æ®ç±»å‹è½¬æ¢"""
        data_min, data_max = data.min(), data.max()
        if data_min >= -1.0 and data_max <= 1.0:
            normalized = (data.astype(np.float64) + 1.0) * (max_val / 2.0)
        else:
            clipped = np.clip(data, -1, 1)
            normalized = (clipped.astype(np.float64) + 1.0) * (max_val / 2.0)
        return np.round(normalized).astype(dtype)
    
    def _single_thread_conversion_float32(self, data):
        """å•çº¿ç¨‹Float32è½¬æ¢"""
        data_min, data_max = data.min(), data.max()
        if data_min >= -1.0 and data_max <= 1.0:
            result = data.astype(np.float32) if data.dtype != np.float32 else data.copy()
            result += 1.0
            result *= 127.5
        else:
            result = np.clip(data, -1, 1, dtype=np.float32)
            result += 1.0
            result *= 127.5
        return result
    
    def _test_standard_multiprocess_gaf(self, data, method, n_jobs, chunk_size):
        """æµ‹è¯•æ ‡å‡†å¤šè¿›ç¨‹GAFç”Ÿæˆ"""
        chunks = self._split_data_into_chunks(data, chunk_size)
        
        from functools import partial
        process_func = partial(ChunkSizeOptimizer._process_gaf_chunk_static, method=method)
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            results = list(executor.map(process_func, chunks))
        
        return np.concatenate(results, axis=0)
    
    def _test_shared_memory_gaf(self, data, method, n_jobs, chunk_size):
        """æµ‹è¯•å…±äº«å†…å­˜GAFç”Ÿæˆ"""
        # ç®€åŒ–ç‰ˆå…±äº«å†…å­˜æµ‹è¯•ï¼ŒåŸºäºDualGAFDataManagerçš„å®ç°
        try:
            from multiprocessing import shared_memory
            from concurrent.futures import as_completed
            
            N, T, D = data.shape
            
            # åˆ›å»ºè¾“å…¥å…±äº«å†…å­˜
            input_shm = shared_memory.SharedMemory(create=True, size=data.nbytes)
            input_array = np.ndarray(data.shape, dtype=data.dtype, buffer=input_shm.buf)
            input_array[:] = data[:]
            
            # åˆ›å»ºç»“æœå…±äº«å†…å­˜
            result_shape = (N, D, T, T)
            result_dtype = np.float32
            result_size = N * D * T * T * np.dtype(result_dtype).itemsize
            result_shm = shared_memory.SharedMemory(create=True, size=result_size)
            result_array = np.ndarray(result_shape, dtype=result_dtype, buffer=result_shm.buf)
            
            # è®¡ç®—å—ç´¢å¼•
            chunk_indices = []
            for i in range(0, N, chunk_size):
                end_idx = min(i + chunk_size, N)
                chunk_indices.append((i, end_idx))
            
            # å¹¶è¡Œå¤„ç†
            with ProcessPoolExecutor(max_workers=n_jobs) as executor:
                futures = []
                for start_idx, end_idx in chunk_indices:
                    future = executor.submit(
                        self._process_gaf_chunk_shared_memory_static,
                        input_shm.name,
                        data.shape,
                        data.dtype,
                        start_idx,
                        end_idx,
                        method,
                        result_shm.name,
                        result_shape,
                        result_dtype
                    )
                    futures.append(future)
                
                # ç­‰å¾…å®Œæˆ
                for future in as_completed(futures):
                    future.result()
            
            # å¤åˆ¶ç»“æœ
            final_result = result_array.copy()
            
            # æ¸…ç†
            input_shm.close()
            input_shm.unlink()
            result_shm.close()
            result_shm.unlink()
            
            return final_result
            
        except Exception as e:
            print(f"å…±äº«å†…å­˜GAFæµ‹è¯•å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
            return self._test_standard_multiprocess_gaf(data, method, n_jobs, chunk_size)
    
    def _test_standard_multithread_conversion(self, data, dtype, max_val, n_jobs, chunk_size):
        """æµ‹è¯•æ ‡å‡†å¤šçº¿ç¨‹æ•°æ®è½¬æ¢"""
        chunks = self._split_data_into_chunks(data, chunk_size)
        
        from functools import partial
        process_func = partial(ChunkSizeOptimizer._process_conversion_chunk_static, dtype=dtype, max_val=max_val)
        
        with ThreadPoolExecutor(max_workers=n_jobs) as executor:
            results = list(executor.map(process_func, chunks))
        
        return np.concatenate(results, axis=0)
    
    def _test_shared_memory_conversion(self, data, dtype, max_val, n_jobs, chunk_size):
        """æµ‹è¯•å…±äº«å†…å­˜æ•°æ®è½¬æ¢"""
        try:
            from multiprocessing import shared_memory
            from concurrent.futures import as_completed
            
            N = data.shape[0]
            
            # åˆ›å»ºè¾“å…¥å…±äº«å†…å­˜
            input_shm = shared_memory.SharedMemory(create=True, size=data.nbytes)
            input_array = np.ndarray(data.shape, dtype=data.dtype, buffer=input_shm.buf)
            input_array[:] = data[:]
            
            # åˆ›å»ºç»“æœå…±äº«å†…å­˜
            result_size = data.size * np.dtype(dtype).itemsize
            result_shm = shared_memory.SharedMemory(create=True, size=result_size)
            result_array = np.ndarray(data.shape, dtype=dtype, buffer=result_shm.buf)
            
            # è®¡ç®—å—ç´¢å¼•
            chunk_indices = []
            for i in range(0, N, chunk_size):
                end_idx = min(i + chunk_size, N)
                chunk_indices.append((i, end_idx))
            
            # å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=n_jobs) as executor:
                futures = []
                for start_idx, end_idx in chunk_indices:
                    future = executor.submit(
                        self._process_conversion_chunk_shared_memory_static,
                        input_shm.name,
                        data.shape,
                        data.dtype,
                        result_shm.name,
                        data.shape,
                        dtype,
                        start_idx,
                        end_idx,
                        dtype,
                        max_val
                    )
                    futures.append(future)
                
                # ç­‰å¾…å®Œæˆ
                for future in as_completed(futures):
                    future.result()
            
            # å¤åˆ¶ç»“æœ
            final_result = result_array.copy()
            
            # æ¸…ç†
            input_shm.close()
            input_shm.unlink()
            result_shm.close()
            result_shm.unlink()
            
            return final_result
            
        except Exception as e:
            print(f"å…±äº«å†…å­˜è½¬æ¢æµ‹è¯•å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
            return self._test_standard_multithread_conversion(data, dtype, max_val, n_jobs, chunk_size)
    
    def _split_data_into_chunks(self, data, chunk_size):
        """å°†æ•°æ®åˆ†å‰²æˆå—"""
        N = data.shape[0]
        chunks = []
        for i in range(0, N, chunk_size):
            end_idx = min(i + chunk_size, N)
            chunks.append(data[i:end_idx])
        return chunks
    
    def _fast_consistency_check(self, result1, result2, sample_ratio=0.01, rtol=1e-5):
        """
        å¿«é€Ÿä¸€è‡´æ€§æ£€æŸ¥ï¼šé€šè¿‡é‡‡æ ·çš„æ–¹å¼æ£€æŸ¥ä¸¤ä¸ªå¤§æ•°ç»„æ˜¯å¦ä¸€è‡´
        
        Args:
            result1, result2: è¦æ¯”è¾ƒçš„æ•°ç»„
            sample_ratio: é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤1%, å¯¹äºå¤§æ•°ç»„è¶³å¤Ÿ)
            rtol: ç›¸å¯¹å®¹å·®
            
        Returns:
            bool: æ˜¯å¦ä¸€è‡´
        """
        # åŸºæœ¬å½¢çŠ¶æ£€æŸ¥
        if result1.shape != result2.shape:
            print(f"   âš ï¸  å½¢çŠ¶ä¸ä¸€è‡´: {result1.shape} vs {result2.shape}")
            return False
        
        # åŸºæœ¬ç»Ÿè®¡æ£€æŸ¥ï¼ˆéå¸¸å¿«ï¼‰
        if abs(result1.mean() - result2.mean()) > rtol:
            print(f"   âš ï¸  å‡å€¼å·®å¼‚è¿‡å¤§: {result1.mean():.6f} vs {result2.mean():.6f}")
            return False
            
        if abs(result1.std() - result2.std()) > rtol:
            print(f"   âš ï¸  æ ‡å‡†å·®å·®å¼‚è¿‡å¤§: {result1.std():.6f} vs {result2.std():.6f}")
            return False
        
        # é‡‡æ ·æ£€æŸ¥
        total_elements = result1.size
        sample_size = max(1000, int(total_elements * sample_ratio))  # è‡³å°‘æ£€æŸ¥1000ä¸ªå…ƒç´ 
        
        # éšæœºé‡‡æ ·ç´¢å¼•
        np.random.seed(42)  # ç¡®ä¿å¯é‡å¤
        flat1 = result1.flatten()
        flat2 = result2.flatten()
        
        # éšæœºé‡‡æ ·
        sample_indices = np.random.choice(total_elements, size=sample_size, replace=False)
        sample1 = flat1[sample_indices]
        sample2 = flat2[sample_indices]
        
        # å¯¹é‡‡æ ·æ•°æ®è¿›è¡Œç²¾ç¡®æ¯”è¾ƒ
        is_close = np.allclose(sample1, sample2, rtol=rtol)
        
        if not is_close:
            # é¢å¤–çš„è¯Šæ–­ä¿¡æ¯
            diff = np.abs(sample1 - sample2)
            max_diff = diff.max()
            avg_diff = diff.mean()
            print(f"   âš ï¸  é‡‡æ ·æ£€æŸ¥å¤±è´¥: æœ€å¤§å·®å¼‚={max_diff:.8f}, å¹³å‡å·®å¼‚={avg_diff:.8f}")
        
        return is_close
    
    @staticmethod
    def _process_gaf_chunk_static(chunk_data, method):
        """é™æ€æ–¹æ³•ï¼šå¤„ç†GAFå—"""
        from pyts.image import GramianAngularField
        
        N, T, D = chunk_data.shape
        transposed_data = chunk_data.transpose(0, 2, 1)
        flattened_data = transposed_data.reshape(-1, T)
        gasf = GramianAngularField(method=method)
        batch_gaf = gasf.fit_transform(flattened_data)
        reshaped_gaf = batch_gaf.reshape(N, D, T, T)
        return reshaped_gaf.astype(np.float32)
    
    @staticmethod
    def _process_conversion_chunk_static(chunk_data, dtype, max_val):
        """é™æ€æ–¹æ³•ï¼šå¤„ç†è½¬æ¢å—"""
        data_min, data_max = chunk_data.min(), chunk_data.max()
        if data_min >= -1.0 and data_max <= 1.0:
            normalized = (chunk_data.astype(np.float64) + 1.0) * (max_val / 2.0)
        else:
            clipped = np.clip(chunk_data, -1, 1)
            normalized = (clipped.astype(np.float64) + 1.0) * (max_val / 2.0)
        return np.round(normalized).astype(dtype)
    
    @staticmethod
    def _process_gaf_chunk_shared_memory_static(shm_name, shape, dtype, start_idx, end_idx, 
                                              method, result_shm_name, result_shape, result_dtype):
        """é™æ€æ–¹æ³•ï¼šå…±äº«å†…å­˜GAFå¤„ç†"""
        from multiprocessing import shared_memory
        from pyts.image import GramianAngularField
        
        # è¿æ¥è¾“å…¥å…±äº«å†…å­˜
        input_shm = shared_memory.SharedMemory(name=shm_name)
        input_array = np.ndarray(shape, dtype=dtype, buffer=input_shm.buf)
        
        # æå–å—æ•°æ®
        chunk_data = input_array[start_idx:end_idx].copy()
        input_shm.close()
        
        # GAFè½¬æ¢
        N, T, D = chunk_data.shape
        transposed_data = chunk_data.transpose(0, 2, 1)
        flattened_data = transposed_data.reshape(-1, T)
        gasf = GramianAngularField(method=method)
        batch_gaf = gasf.fit_transform(flattened_data)
        reshaped_gaf = batch_gaf.reshape(N, D, T, T).astype(np.float32)
        
        # å†™å…¥ç»“æœå…±äº«å†…å­˜
        result_shm = shared_memory.SharedMemory(name=result_shm_name)
        result_array = np.ndarray(result_shape, dtype=result_dtype, buffer=result_shm.buf)
        result_array[start_idx:start_idx + reshaped_gaf.shape[0]] = reshaped_gaf
        result_shm.close()
    
    @staticmethod
    def _process_conversion_chunk_shared_memory_static(input_shm_name, input_shape, input_dtype,
                                                     result_shm_name, result_shape, result_dtype,
                                                     start_idx, end_idx, target_dtype, max_val):
        """é™æ€æ–¹æ³•ï¼šå…±äº«å†…å­˜è½¬æ¢å¤„ç†"""
        from multiprocessing import shared_memory
        
        # è¿æ¥è¾“å…¥å…±äº«å†…å­˜
        input_shm = shared_memory.SharedMemory(name=input_shm_name)
        input_array = np.ndarray(input_shape, dtype=input_dtype, buffer=input_shm.buf)
        
        # æå–å—æ•°æ®
        chunk_data = input_array[start_idx:end_idx].copy()
        input_shm.close()
        
        # æ•°æ®è½¬æ¢
        data_min, data_max = chunk_data.min(), chunk_data.max()
        if data_min >= -1.0 and data_max <= 1.0:
            normalized = (chunk_data.astype(np.float64) + 1.0) * (max_val / 2.0)
        else:
            clipped = np.clip(chunk_data, -1, 1)
            normalized = (clipped.astype(np.float64) + 1.0) * (max_val / 2.0)
        
        converted_data = np.round(normalized).astype(target_dtype)
        
        # å†™å…¥ç»“æœå…±äº«å†…å­˜
        result_shm = shared_memory.SharedMemory(name=result_shm_name)
        result_array = np.ndarray(result_shape, dtype=result_dtype, buffer=result_shm.buf)
        result_array[start_idx:end_idx] = converted_data
        result_shm.close()
    
    def analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœå¹¶æä¾›æ¨è"""
        print("=" * 60)
        print("ğŸ“ˆ ç»“æœåˆ†æä¸æ¨è")
        print("=" * 60)
        
        # åˆ†æGAFç»“æœ
        if self.gaf_results:
            print("\nğŸ§® GAFè®¡ç®—ä¼˜åŒ–åˆ†æ:")
            gaf_df = pd.DataFrame(self.gaf_results)
            
            # æ‰¾å‡ºæœ€ä½³é…ç½®
            best_standard = gaf_df.loc[gaf_df['standard_speedup'].idxmax()]
            best_shared = gaf_df.loc[gaf_df['shared_speedup'].idxmax()]
            
            print(f"   ğŸ“Š æœ€ä½³æ ‡å‡†å¤šè¿›ç¨‹é…ç½®:")
            print(f"      è¿›ç¨‹æ•°: {best_standard['n_jobs']}, å—å¤§å°: {best_standard['chunk_size']}")
            print(f"      åŠ é€Ÿæ¯”: {best_standard['standard_speedup']:.2f}x, è€—æ—¶: {best_standard['standard_time']:.2f}s")
            
            print(f"   ğŸš€ æœ€ä½³å…±äº«å†…å­˜é…ç½®:")
            print(f"      è¿›ç¨‹æ•°: {best_shared['n_jobs']}, å—å¤§å°: {best_shared['chunk_size']}")
            print(f"      åŠ é€Ÿæ¯”: {best_shared['shared_speedup']:.2f}x, è€—æ—¶: {best_shared['shared_time']:.2f}s")
            
            # æ•ˆç‡åˆ†æ
            avg_standard_speedup = gaf_df['standard_speedup'].mean()
            avg_shared_speedup = gaf_df['shared_speedup'].mean()
            print(f"   ğŸ“ˆ å¹³å‡åŠ é€Ÿæ¯” - æ ‡å‡†: {avg_standard_speedup:.2f}x, å…±äº«å†…å­˜: {avg_shared_speedup:.2f}x")
        
        # åˆ†æè½¬æ¢ç»“æœ
        if self.conversion_results:
            print("\nğŸ”„ æ•°æ®è½¬æ¢ä¼˜åŒ–åˆ†æ:")
            conv_df = pd.DataFrame(self.conversion_results)
            
            # æ‰¾å‡ºæœ€ä½³é…ç½®
            best_standard = conv_df.loc[conv_df['standard_speedup'].idxmax()]
            best_shared = conv_df.loc[conv_df['shared_speedup'].idxmax()]
            
            print(f"   ğŸ“Š æœ€ä½³æ ‡å‡†å¤šçº¿ç¨‹é…ç½®:")
            print(f"      çº¿ç¨‹æ•°: {best_standard['n_jobs']}, å—å¤§å°: {best_standard['chunk_size']}")
            print(f"      åŠ é€Ÿæ¯”: {best_standard['standard_speedup']:.2f}x, è€—æ—¶: {best_standard['standard_time']:.2f}s")
            
            print(f"   ğŸš€ æœ€ä½³å…±äº«å†…å­˜é…ç½®:")
            print(f"      çº¿ç¨‹æ•°: {best_shared['n_jobs']}, å—å¤§å°: {best_shared['chunk_size']}")
            print(f"      åŠ é€Ÿæ¯”: {best_shared['shared_speedup']:.2f}x, è€—æ—¶: {best_shared['shared_time']:.2f}s")
            
            # æ•ˆç‡åˆ†æ
            avg_standard_speedup = conv_df['standard_speedup'].mean()
            avg_shared_speedup = conv_df['shared_speedup'].mean()
            print(f"   ğŸ“ˆ å¹³å‡åŠ é€Ÿæ¯” - æ ‡å‡†: {avg_standard_speedup:.2f}x, å…±äº«å†…å­˜: {avg_shared_speedup:.2f}x")
        
        # ç³»ç»Ÿæ¨è
        print(f"\nğŸ¯ ç³»ç»Ÿæ¨èé…ç½® (åŸºäºå½“å‰ç¡¬ä»¶: {self.cpu_count}æ ¸, {self.memory_gb:.1f}GB):")
        
        # GAFè®¡ç®—æ¨è
        if self.gaf_results:
            gaf_df = pd.DataFrame(self.gaf_results)
            gaf_recommendations = []
            
            # ä¸åŒè¿›ç¨‹æ•°ä¸‹çš„æœ€ä½³å—å¤§å°
            for n_jobs in gaf_df['n_jobs'].unique():
                subset = gaf_df[gaf_df['n_jobs'] == n_jobs]
                best_standard_idx = subset['standard_speedup'].idxmax()
                best_shared_idx = subset['shared_speedup'].idxmax()
                
                best_standard_config = subset.loc[best_standard_idx]
                best_shared_config = subset.loc[best_shared_idx]
                
                gaf_recommendations.append({
                    'n_jobs': n_jobs,
                    'standard_chunk_size': best_standard_config['chunk_size'],
                    'standard_speedup': best_standard_config['standard_speedup'],
                    'shared_chunk_size': best_shared_config['chunk_size'],
                    'shared_speedup': best_shared_config['shared_speedup']
                })
            
            print(f"   ğŸ§® GAFè®¡ç®—æ¨è:")
            for rec in gaf_recommendations:
                print(f"      {rec['n_jobs']}è¿›ç¨‹ - æ ‡å‡†: {rec['standard_chunk_size']} ({rec['standard_speedup']:.2f}x), "
                      f"å…±äº«å†…å­˜: {rec['shared_chunk_size']} ({rec['shared_speedup']:.2f}x)")
        
        # è½¬æ¢æ¨è
        if self.conversion_results:
            conv_df = pd.DataFrame(self.conversion_results)
            conv_recommendations = []
            
            for n_jobs in conv_df['n_jobs'].unique():
                subset = conv_df[conv_df['n_jobs'] == n_jobs]
                best_standard_idx = subset['standard_speedup'].idxmax()
                best_shared_idx = subset['shared_speedup'].idxmax()
                
                best_standard_config = subset.loc[best_standard_idx]
                best_shared_config = subset.loc[best_shared_idx]
                
                conv_recommendations.append({
                    'n_jobs': n_jobs,
                    'standard_chunk_size': best_standard_config['chunk_size'],
                    'standard_speedup': best_standard_config['standard_speedup'],
                    'shared_chunk_size': best_shared_config['chunk_size'],
                    'shared_speedup': best_shared_config['shared_speedup']
                })
            
            print(f"   ğŸ”„ æ•°æ®è½¬æ¢æ¨è:")
            for rec in conv_recommendations:
                print(f"      {rec['n_jobs']}çº¿ç¨‹ - æ ‡å‡†: {rec['standard_chunk_size']} ({rec['standard_speedup']:.2f}x), "
                      f"å…±äº«å†…å­˜: {rec['shared_chunk_size']} ({rec['shared_speedup']:.2f}x)")
    
    def save_results(self, output_dir="test_results"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜GAFç»“æœ
        if self.gaf_results:
            gaf_df = pd.DataFrame(self.gaf_results)
            gaf_df.to_csv(os.path.join(output_dir, "gaf_chunk_size_results.csv"), index=False)
            print(f"ğŸ“ GAFæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/gaf_chunk_size_results.csv")
        
        # ä¿å­˜è½¬æ¢ç»“æœ
        if self.conversion_results:
            conv_df = pd.DataFrame(self.conversion_results)
            conv_df.to_csv(os.path.join(output_dir, "conversion_chunk_size_results.csv"), index=False)
            print(f"ğŸ“ è½¬æ¢æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/conversion_chunk_size_results.csv")
        
        # ä¿å­˜åŸºå‡†ç»“æœ
        if hasattr(self, 'baseline_results'):
            baseline_df = pd.DataFrame([self.baseline_results])
            baseline_df.to_csv(os.path.join(output_dir, "baseline_results.csv"), index=False)
            print(f"ğŸ“ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/baseline_results.csv")
    
    def run_full_test(self):
        """è¿è¡Œå®Œæ•´çš„å—å¤§å°ä¼˜åŒ–å®Œæ•´æµ‹è¯•"""
        print("ğŸ”¬ å¼€å§‹å—å¤§å°ä¼˜åŒ–å®Œæ•´æµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
        
        # ä¼°ç®—æµ‹è¯•æ—¶é—´
        n_gaf_tests = len(self.chunk_sizes) * len([n for n in self.n_jobs_list if n <= self.cpu_count]) * 2  # æ ‡å‡†+å…±äº«å†…å­˜
        n_conv_tests = len(self.chunk_sizes) * len([n for n in self.n_jobs_list if n <= self.cpu_count]) * 2
        total_tests = n_gaf_tests + n_conv_tests
        estimated_time_per_test = 8  # æ¯ä¸ªæµ‹è¯•çº¦8ç§’
        estimated_total_time = total_tests * estimated_time_per_test / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
        
        print(f"â±ï¸  é¢„ä¼°æµ‹è¯•æ—¶é—´: {estimated_total_time:.1f}åˆ†é’Ÿ ({total_tests}ä¸ªæµ‹è¯•)")
        print(f"ğŸ¯ æµ‹è¯•é…ç½®: å—å¤§å°{len(self.chunk_sizes)}ç§, è¿›ç¨‹æ•°{len(self.n_jobs_list)}ç§")
        print()
        
        try:
            # 1. å•çº¿ç¨‹åŸºå‡†æµ‹è¯• - æš‚æ—¶æ³¨é‡Šæ‰ï¼ˆå¤ªæ…¢ï¼‰
            print("â­ï¸  è·³è¿‡å•çº¿ç¨‹åŸºå‡†æµ‹è¯•ï¼ˆå¤ªè€—æ—¶ï¼‰ï¼Œä½¿ç”¨ä¼°ç®—åŸºå‡†æ—¶é—´")
            self._create_mock_baseline()
            # self.test_single_thread_baseline()
            
            # 2. GAFå—å¤§å°æµ‹è¯•
            self.test_gaf_chunk_sizes()
            
            # 3. è½¬æ¢å—å¤§å°æµ‹è¯•
            self.test_conversion_chunk_sizes()
            
            # 4. ç»“æœåˆ†æ
            self.analyze_results()
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results()
            
            print("\nâœ… å—å¤§å°ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
            print("ğŸ“ˆ æŸ¥çœ‹test_results/ç›®å½•è·å–è¯¦ç»†ç»“æœ")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å—å¤§å°ä¼˜åŒ–æµ‹è¯•è„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
    if mp.cpu_count() < 4:
        print("âš ï¸  è­¦å‘Š: ç³»ç»ŸCPUæ ¸å¿ƒæ•°è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æµ‹è¯•æ•ˆæœ")
    
    if psutil.virtual_memory().total / (1024**3) < 8:
        print("âš ï¸  è­¦å‘Š: ç³»ç»Ÿå†…å­˜è¾ƒå°‘ï¼Œå»ºè®®é™ä½æµ‹è¯•æ ·æœ¬æ•°")
    
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œ
    optimizer = ChunkSizeOptimizer(test_samples=5000)
    optimizer.run_full_test()


if __name__ == "__main__":
    main() 