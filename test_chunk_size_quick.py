#!/usr/bin/env python3
"""
å¿«é€Ÿå—å¤§å°ä¼˜åŒ–æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å—å¤§å°ä¼˜åŒ–åŠŸèƒ½çš„å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬ï¼ˆæ ·æœ¬æ•°è¾ƒå°‘ï¼‰

æµ‹è¯•æ ·æœ¬æ•°: 1000 (ç›¸æ¯”å®Œæ•´ç‰ˆçš„5000)
åŠŸèƒ½ä¸å®Œæ•´ç‰ˆç›¸åŒï¼Œä½†æµ‹è¯•è§„æ¨¡è¾ƒå°ï¼Œç”¨äºå¿«é€ŸéªŒè¯
"""

import os
import sys
import time
import numpy as np
import multiprocessing as mp
import psutil
import gc

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

# å¯¼å…¥å®Œæ•´ç‰ˆæµ‹è¯•å™¨
from test_chunk_size_optimization import ChunkSizeOptimizer


class QuickChunkSizeOptimizer(ChunkSizeOptimizer):
    """å¿«é€Ÿå—å¤§å°ä¼˜åŒ–æµ‹è¯•å™¨ï¼ˆç»§æ‰¿å®Œæ•´ç‰ˆï¼Œåªæ”¹å˜æ ·æœ¬æ•°å’Œæµ‹è¯•é…ç½®ï¼‰"""
    
    def __init__(self, test_samples=1000):
        """ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬æ•°è¿›è¡Œå¿«é€Ÿæµ‹è¯•"""
        print("ğŸš€ å¿«é€Ÿå—å¤§å°ä¼˜åŒ–æµ‹è¯•å™¨")
        print("=" * 40)
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä½†ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬
        super().__init__(test_samples)
        
        # ç®€åŒ–æµ‹è¯•é…ç½®ï¼Œå‡å°‘æµ‹è¯•æ—¶é—´
        self.chunk_sizes = [50, 100, 200, 500]  # å‡å°‘æµ‹è¯•çš„å—å¤§å°æ•°é‡
        self.n_jobs_list = [4, 8]  # åªæµ‹è¯•ä¸¤ç§è¿›ç¨‹æ•°é…ç½®
        
        print("ğŸ“ å¿«é€Ÿæµ‹è¯•é…ç½®:")
        print(f"   æ ·æœ¬æ•°: {self.test_samples}")
        print(f"   æµ‹è¯•å—å¤§å°: {self.chunk_sizes}")
        print(f"   æµ‹è¯•è¿›ç¨‹æ•°: {self.n_jobs_list}")
        print()
    
    def test_gaf_chunk_sizes_quick(self):
        """å¿«é€ŸGAFå—å¤§å°æµ‹è¯•ï¼ˆå‡å°‘æµ‹è¯•ç»„åˆï¼‰"""
        print("=" * 40)
        print("ğŸ§® å¿«é€ŸGAFå—å¤§å°æµ‹è¯•")
        print("=" * 40)
        
        baseline_time = self.baseline_results['gaf_summation_time']
        
        # åªæµ‹è¯•ä¸€ç§è¿›ç¨‹æ•°é…ç½®ä»¥èŠ‚çœæ—¶é—´
        n_jobs = min(8, self.cpu_count)
        print(f"\n--- æµ‹è¯•è¿›ç¨‹æ•°: {n_jobs} ---")
        
        for chunk_size in self.chunk_sizes:
            if chunk_size > self.test_samples // 2:
                continue
                
            print(f"\nğŸ”„ æµ‹è¯•å—å¤§å°: {chunk_size}")
            
            # æµ‹è¯•æ ‡å‡†å¤šè¿›ç¨‹
            print(f"   ğŸ“Š æµ‹è¯•æ ‡å‡†å¤šè¿›ç¨‹...")
            start_time = time.time()
            result_standard = self._test_standard_multiprocess_gaf(
                self.test_data, "summation", n_jobs, chunk_size
            )
            standard_time = time.time() - start_time
            speedup_standard = baseline_time / standard_time if standard_time > 0 else 0
            
            # æµ‹è¯•å…±äº«å†…å­˜æ¨¡å¼
            print(f"   ğŸš€ æµ‹è¯•å…±äº«å†…å­˜...")
            start_time = time.time()
            result_shared = self._test_shared_memory_gaf(
                self.test_data, "summation", n_jobs, chunk_size
            )
            shared_time = time.time() - start_time
            speedup_shared = baseline_time / shared_time if shared_time > 0 else 0
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§ (å¿«é€Ÿé‡‡æ ·æ£€æŸ¥)
            consistency_check = self._fast_consistency_check(result_standard, result_shared)
            
            # è®°å½•ç»“æœ
            result_entry = {
                'test_type': 'gaf_generation',
                'n_jobs': n_jobs,
                'chunk_size': chunk_size,
                'standard_time': standard_time,
                'shared_time': shared_time,
                'baseline_time': baseline_time,
                'standard_speedup': speedup_standard,
                'shared_speedup': speedup_shared,
                'consistency_check': consistency_check,
                'samples_per_chunk': chunk_size,
                'total_chunks': max(1, self.test_samples // chunk_size)
            }
            self.gaf_results.append(result_entry)
            
            print(f"   ğŸ“Š æ ‡å‡†å¤šè¿›ç¨‹: {standard_time:.2f}s (åŠ é€Ÿ: {speedup_standard:.2f}x)")
            print(f"   ğŸš€ å…±äº«å†…å­˜: {shared_time:.2f}s (åŠ é€Ÿ: {speedup_shared:.2f}x)")
            print(f"   âœ… ç»“æœä¸€è‡´æ€§: {consistency_check}")
            
            # æ¸…ç†å†…å­˜
            del result_standard, result_shared
            gc.collect()
        
        return self.gaf_results
    
    def test_conversion_chunk_sizes_quick(self):
        """å¿«é€Ÿæ•°æ®è½¬æ¢å—å¤§å°æµ‹è¯•"""
        print("=" * 40)
        print("ğŸ”„ å¿«é€Ÿæ•°æ®è½¬æ¢å—å¤§å°æµ‹è¯•")
        print("=" * 40)
        
        # ç”ŸæˆGAFæ•°æ®ç”¨äºè½¬æ¢æµ‹è¯•
        print("ğŸ”„ ç”ŸæˆGAFæ•°æ®...")
        gaf_data = self._single_thread_gaf(self.test_data, "summation")
        print(f"ğŸ“Š GAFæ•°æ®å¤§å°: {gaf_data.nbytes / 1024**3:.2f}GB")
        
        baseline_time = self.baseline_results['conversion_uint8_time']
        
        # åªæµ‹è¯•ä¸€ç§çº¿ç¨‹æ•°é…ç½®
        n_jobs = min(8, self.cpu_count)
        print(f"\n--- æµ‹è¯•çº¿ç¨‹æ•°: {n_jobs} ---")
        
        for chunk_size in self.chunk_sizes:
            if chunk_size > self.test_samples // 2:
                continue
                
            print(f"\nğŸ”„ æµ‹è¯•å—å¤§å°: {chunk_size}")
            
            # æµ‹è¯•æ ‡å‡†å¤šçº¿ç¨‹è½¬æ¢
            print(f"   ğŸ“Š æµ‹è¯•æ ‡å‡†å¤šçº¿ç¨‹è½¬æ¢...")
            start_time = time.time()
            result_standard = self._test_standard_multithread_conversion(
                gaf_data, np.uint8, 255, n_jobs, chunk_size
            )
            standard_time = time.time() - start_time
            speedup_standard = baseline_time / standard_time if standard_time > 0 else 0
            
            # æµ‹è¯•å…±äº«å†…å­˜è½¬æ¢
            print(f"   ğŸš€ æµ‹è¯•å…±äº«å†…å­˜è½¬æ¢...")
            start_time = time.time()
            result_shared = self._test_shared_memory_conversion(
                gaf_data, np.uint8, 255, n_jobs, chunk_size
            )
            shared_time = time.time() - start_time
            speedup_shared = baseline_time / shared_time if shared_time > 0 else 0
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§ (å¿«é€Ÿé‡‡æ ·æ£€æŸ¥)
            consistency_check = self._fast_consistency_check(result_standard, result_shared)
            
            # è®°å½•ç»“æœ
            result_entry = {
                'test_type': 'data_conversion',
                'n_jobs': n_jobs,
                'chunk_size': chunk_size,
                'standard_time': standard_time,
                'shared_time': shared_time,
                'baseline_time': baseline_time,
                'standard_speedup': speedup_standard,
                'shared_speedup': speedup_shared,
                'consistency_check': consistency_check,
                'samples_per_chunk': chunk_size,
                'total_chunks': max(1, self.test_samples // chunk_size)
            }
            self.conversion_results.append(result_entry)
            
            print(f"   ğŸ“Š æ ‡å‡†å¤šçº¿ç¨‹: {standard_time:.2f}s (åŠ é€Ÿ: {speedup_standard:.2f}x)")
            print(f"   ğŸš€ å…±äº«å†…å­˜: {shared_time:.2f}s (åŠ é€Ÿ: {speedup_shared:.2f}x)")
            print(f"   âœ… ç»“æœä¸€è‡´æ€§: {consistency_check}")
            
            # æ¸…ç†å†…å­˜
            del result_standard, result_shared
            gc.collect()
        
        # æ¸…ç†GAFæ•°æ®
        del gaf_data
        gc.collect()
        
        return self.conversion_results
    
    def run_quick_test(self):
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å¿«é€Ÿå—å¤§å°ä¼˜åŒ–æµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {self.test_samples}")
        print()
        
        try:
            # 1. å•çº¿ç¨‹åŸºå‡†æµ‹è¯•
            print("1ï¸âƒ£ åŸºå‡†æµ‹è¯•...")
            self.test_single_thread_baseline()
            
            # 2. å¿«é€ŸGAFå—å¤§å°æµ‹è¯•
            print("2ï¸âƒ£ GAFå—å¤§å°æµ‹è¯•...")
            self.test_gaf_chunk_sizes_quick()
            
            # 3. å¿«é€Ÿè½¬æ¢å—å¤§å°æµ‹è¯•
            print("3ï¸âƒ£ æ•°æ®è½¬æ¢å—å¤§å°æµ‹è¯•...")
            self.test_conversion_chunk_sizes_quick()
            
            # 4. ç»“æœåˆ†æ
            print("4ï¸âƒ£ ç»“æœåˆ†æ...")
            self.analyze_results()
            
            # 5. ä¿å­˜ç»“æœ
            print("5ï¸âƒ£ ä¿å­˜ç»“æœ...")
            self.save_results("quick_test_results")
            
            print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ!")
            print("ğŸ“ˆ æŸ¥çœ‹quick_test_results/ç›®å½•è·å–è¯¦ç»†ç»“æœ")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å¿«é€Ÿå—å¤§å°ä¼˜åŒ–æµ‹è¯•")
    print("=" * 50)
    print("è¿™æ˜¯ä¸€ä¸ªéªŒè¯æ€§çš„å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬æ•°")
    print("å¦‚éœ€å®Œæ•´æµ‹è¯•ï¼Œè¯·è¿è¡Œ test_chunk_size_optimization.py")
    print()
    
    # æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
    cpu_count = mp.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
    print(f"   æ€»å†…å­˜: {memory_gb:.1f}GB")
    print()
    
    if cpu_count < 4:
        print("âš ï¸  è­¦å‘Š: ç³»ç»ŸCPUæ ¸å¿ƒæ•°è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æµ‹è¯•æ•ˆæœ")
    
    if memory_gb < 8:
        print("âš ï¸  è­¦å‘Š: ç³»ç»Ÿå†…å­˜è¾ƒå°‘ï¼Œå»ºè®®é™ä½æµ‹è¯•æ ·æœ¬æ•°")
    
    # åˆ›å»ºå¿«é€Ÿæµ‹è¯•å™¨å¹¶è¿è¡Œ
    optimizer = QuickChunkSizeOptimizer(test_samples=1000)
    optimizer.run_quick_test()


if __name__ == "__main__":
    main() 