import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from models.MultiImageFeatureNet import (
    NoPaddingLargeKernelFeatureExtractor, 
    InceptionFeatureExtractor,
    LargeKernelDilatedFeatureExtractor,
    MultiScaleStackedFeatureExtractor
)


class BasicBlock(nn.Module):
    """ResNetåŸºç¡€å— - ä»MultiImageFeatureNetç§»æ¤å¹¶ä¼˜åŒ–"""
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out


class GAFOptimizedResNet(nn.Module):
    """ä¸“é—¨ä¸ºGAFå›¾åƒä¼˜åŒ–çš„ResNet - æ¸è¿›å¼ä¸‹é‡‡æ ·ä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯"""
    def __init__(self, feature_dim=128, depth='resnet18'):
        super().__init__()
        self.depth = depth
        
        if depth == 'resnet18_gaf':
            # GAFä¼˜åŒ–ç‰ˆæœ¬ï¼šæ¸è¿›å¼ä¸‹é‡‡æ ·ï¼Œæ›´å¥½ä¿ç•™ç©ºé—´ä¿¡æ¯
            # ç¬¬ä¸€å±‚ï¼šè½»å¾®ä¸‹é‡‡æ ·ï¼Œä¿ç•™æ›´å¤šç»†èŠ‚
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False),  # 96->96ï¼Œä¿æŒå°ºå¯¸
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True)
            )
            
            # æ¸è¿›å¼ä¸‹é‡‡æ ·çš„æ®‹å·®å±‚
            self.layer1 = self._make_layer(32, 64, 2, stride=2)    # 96->48
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 2, stride=2)  # 12->6
            final_channels = 512
            
        elif depth == 'resnet18_gaf_light':
            # è½»é‡çº§GAFç‰ˆæœ¬ï¼šé€‚åˆå¤§æ•°æ®é›†
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False),  # 96->96
                nn.BatchNorm2d(16),
                nn.ReLU(inplace=True)
            )
            
            self.layer1 = self._make_layer(16, 32, 2, stride=2)    # 96->48
            self.layer2 = self._make_layer(32, 64, 2, stride=2)    # 48->24
            self.layer3 = self._make_layer(64, 128, 2, stride=2)   # 24->12
            self.layer4 = self._make_layer(128, 256, 2, stride=2)  # 12->6
            final_channels = 256
            
        elif depth == 'resnet_gaf_deep':
            # æ·±åº¦GAFç‰ˆæœ¬ï¼šæ›´å¤šå±‚ä½†ä¿æŒæ¸è¿›ä¸‹é‡‡æ ·
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False),  # 96->96
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True),
                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False),  # é¢å¤–çš„3x3å·ç§¯
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True)
            )
            
            self.layer1 = self._make_layer(32, 64, 3, stride=2)    # 96->48
            self.layer2 = self._make_layer(64, 128, 4, stride=2)   # 48->24  
            self.layer3 = self._make_layer(128, 256, 6, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 3, stride=2)  # 12->6
            final_channels = 512
            
        elif depth == 'resnet_gaf_preserve':
            # é«˜ä¿çœŸç‰ˆæœ¬ï¼šæœ€å¤§ç¨‹åº¦ä¿ç•™ç©ºé—´ä¿¡æ¯
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),  # 96->96
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )
            
            self.layer1 = self._make_layer(64, 64, 2, stride=1)    # 96->96ï¼Œä¸ä¸‹é‡‡æ ·
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 96->48
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 48->24
            self.layer4 = self._make_layer(256, 512, 2, stride=2)  # 24->12
            final_channels = 512
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„GAF ResNetæ·±åº¦: {depth}")
        
        # è‡ªé€‚åº”å¹³å‡æ± åŒ–å’Œå…¨è¿æ¥å±‚
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(final_channels, feature_dim)
        
        print(f"GAFä¼˜åŒ–ResNet ({depth}) æ„å»ºå®Œæˆ:")
        print(f"  - ç¬¬ä¸€å±‚ä¿æŒç©ºé—´åˆ†è¾¨ç‡ï¼š96x96")
        print(f"  - æ¸è¿›å¼ä¸‹é‡‡æ ·ç­–ç•¥")
        print(f"  - æœ€ç»ˆé€šé“æ•°ï¼š{final_channels}")

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """æ„å»ºResNetå±‚"""
        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
        layers.append(BasicBlock(in_channels, out_channels, stride))
        # åç»­å—ä¸éœ€è¦ä¸‹é‡‡æ ·
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        # x: [N, 1, 96, 96]
        x = self.conv1(x)       # [N, channels, 96, 96] - ä¿æŒç©ºé—´åˆ†è¾¨ç‡ï¼
        x = self.layer1(x)      # æ ¹æ®é…ç½®è¿›è¡Œä¸‹é‡‡æ ·
        x = self.layer2(x)      
        x = self.layer3(x)      
        x = self.layer4(x)      
        
        x = self.avgpool(x)     # [N, final_channels, 1, 1]
        x = torch.flatten(x, 1) # [N, final_channels]
        x = self.fc(x)          # [N, feature_dim]
        return x


class ResNetFeatureExtractor(nn.Module):
    """ResNetç‰¹å¾æå–å™¨ - ä¸“é—¨ä¸º96x96 GAFå›¾åƒä¼˜åŒ–"""
    def __init__(self, feature_dim=128, depth='resnet18'):
        super().__init__()
        self.depth = depth
        
        # æ–°çš„GAFä¼˜åŒ–æ¶æ„
        if depth in ['resnet18_gaf', 'resnet18_gaf_light', 'resnet_gaf_deep', 'resnet_gaf_preserve']:
            self.resnet = GAFOptimizedResNet(feature_dim, depth)
            
        # ä¿ç•™åŸæœ‰æ¶æ„ä»¥ä¿æŒå…¼å®¹æ€§
        elif depth == 'resnet18':
            # æ”¹è¿›çš„ResNet18ï¼šå‡å°‘åˆå§‹ä¸‹é‡‡æ ·
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2, bias=False),  # 96->48ï¼Œä½¿ç”¨5x5è€Œä¸æ˜¯7x7
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
                # å»æ‰MaxPoolï¼Œå‡å°‘ä¸‹é‡‡æ ·
            )
            
            self.layer1 = self._make_layer(64, 64, 2, stride=1)    # 48->48ï¼Œä¸ä¸‹é‡‡æ ·
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 2, stride=2)  # 12->6
            
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512, feature_dim)
            
        elif depth == 'resnet34':
            # æ”¹è¿›çš„ResNet34
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )
            
            self.layer1 = self._make_layer(64, 64, 3, stride=1)    # 48->48
            self.layer2 = self._make_layer(64, 128, 4, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 6, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 3, stride=2)  # 12->6
            
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512, feature_dim)
            
        elif depth == 'resnet_light':
            # è½»é‡çº§ç‰ˆæœ¬ï¼Œé€‚åˆDAHUæ•°æ®é›†çš„120ä¸ªä¿¡å·
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False),  # 96->96ï¼Œä¸ä¸‹é‡‡æ ·
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True)
            )
            
            self.layer1 = self._make_layer(32, 64, 2, stride=2)    # 96->48
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 24->12
            self.layer4 = None  # å»æ‰æœ€åä¸€å±‚å‡å°‘å‚æ•°
            
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(256, feature_dim)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ResNetæ·±åº¦: {depth}")

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """æ„å»ºResNetå±‚"""
        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
        layers.append(BasicBlock(in_channels, out_channels, stride))
        # åç»­å—ä¸éœ€è¦ä¸‹é‡‡æ ·
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        # å¦‚æœæ˜¯æ–°çš„GAFä¼˜åŒ–æ¶æ„ï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(self, 'resnet'):
            return self.resnet(x)
            
        # åŸæœ‰æ¶æ„çš„å‰å‘ä¼ æ’­
        x = self.conv1(x)       
        x = self.layer1(x)      
        x = self.layer2(x)      
        x = self.layer3(x)      
        
        if self.layer4 is not None:
            x = self.layer4(x)  
        
        x = self.avgpool(x)     
        x = torch.flatten(x, 1) 
        x = self.fc(x)          
        return x


class OptimizedLargeKernelFeatureExtractor(nn.Module):
    """é’ˆå¯¹96x96ä¼˜åŒ–çš„å¤§æ ¸ç‰¹å¾æå–å™¨"""
    def __init__(self, feature_dim=128):
        super().__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),  # 96->48
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),  # 48->24
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # 24->12
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # 12->6
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, feature_dim)

    def forward(self, x):
        # x: [N, 1, 96, 96]
        x = self.conv1(x)  # [N, 32, 48, 48]
        x = self.conv2(x)  # [N, 64, 24, 24]
        x = self.conv3(x)  # [N, 128, 12, 12]
        x = self.conv4(x)  # [N, 256, 6, 6]
        x = self.avgpool(x)  # [N, 256, 1, 1]
        x = x.view(x.size(0), -1)  # [N, 256]
        x = self.fc(x)  # [N, feature_dim]
        return x


class OptimizedDilatedFeatureExtractor(nn.Module):
    """é’ˆå¯¹96x96ä¼˜åŒ–çš„è†¨èƒ€å·ç§¯ç‰¹å¾æå–å™¨"""
    def __init__(self, feature_dim=128):
        super().__init__()
        # åˆå§‹å·ç§¯
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # 96->48
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # è†¨èƒ€å·ç§¯å±‚ï¼Œä¿æŒç©ºé—´åˆ†è¾¨ç‡åŒæ—¶å¢å¤§æ„Ÿå—é‡
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2, dilation=2),  # ä¿æŒ48x48
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=4, dilation=4),  # ä¿æŒ48x48
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        # æœ€ç»ˆä¸‹é‡‡æ ·
        self.conv4 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # 48->24
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )
        
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(512, feature_dim)

    def forward(self, x):
        # x: [N, 1, 96, 96]
        x = self.conv1(x)  # [N, 64, 48, 48]
        x = self.conv2(x)  # [N, 128, 48, 48] (è†¨èƒ€å·ç§¯)
        x = self.conv3(x)  # [N, 256, 48, 48] (è†¨èƒ€å·ç§¯)
        x = self.conv4(x)  # [N, 512, 24, 24]
        x = self.avgpool(x)  # [N, 512, 1, 1]
        x = x.view(x.size(0), -1)  # [N, 512]
        x = self.fc(x)  # [N, feature_dim]
        return x


class AdaptiveFusion(nn.Module):
    """è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—"""
    def __init__(self, feature_dim=128):
        super().__init__()
        self.weight_net = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )
    
    def forward(self, sum_feat, diff_feat):
        concat_feat = torch.cat([sum_feat, diff_feat], dim=-1)
        weights = self.weight_net(concat_feat)
        return sum_feat * weights + diff_feat * (1 - weights)


class ConcatFusion(nn.Module):
    """ç®€å•æ‹¼æ¥èåˆæ¨¡å—"""
    def __init__(self, feature_dim=128):
        super().__init__()
        self.fc = nn.Linear(feature_dim * 2, feature_dim)
    
    def forward(self, sum_feat, diff_feat):
        concat_feat = torch.cat([sum_feat, diff_feat], dim=-1)
        return self.fc(concat_feat)


class ElementwiseFusion(nn.Module):
    """é€å…ƒç´ èåˆæ¨¡å—"""
    def __init__(self, feature_dim=128, fusion_type='add'):
        super().__init__()
        self.fusion_type = fusion_type
        if fusion_type == 'weighted_add':
            self.alpha = nn.Parameter(torch.tensor(0.5))
    
    def forward(self, sum_feat, diff_feat):
        if self.fusion_type == 'add':
            return sum_feat + diff_feat
        elif self.fusion_type == 'mul':
            return sum_feat * diff_feat
        elif self.fusion_type == 'weighted_add':
            return self.alpha * sum_feat + (1 - self.alpha) * diff_feat
        else:
            raise ValueError(f"Unknown fusion type: {self.fusion_type}")


class BiDirectionalFusion(nn.Module):
    """åŒå‘æ³¨æ„åŠ›èåˆæ¨¡å—"""
    def __init__(self, feature_dim=128, num_heads=4):
        super().__init__()
        self.feature_dim = feature_dim
        self.sum_to_diff = nn.MultiheadAttention(feature_dim, num_heads=num_heads, batch_first=True)
        self.diff_to_sum = nn.MultiheadAttention(feature_dim, num_heads=num_heads, batch_first=True)
        
        # æ·»åŠ ç»´åº¦å˜æ¢å±‚ï¼š2*feature_dim -> feature_dim
        self.fusion_projection = nn.Sequential(
            nn.Linear(2 * feature_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(inplace=True)
        )
        
    def forward(self, sum_feat, diff_feat):
        # ç¡®ä¿è¾“å…¥æ˜¯3Då¼ é‡ [batch_size, seq_len, feature_dim]
        if sum_feat.dim() == 2:
            sum_feat = sum_feat.unsqueeze(1)  # [B, 1, D]
            diff_feat = diff_feat.unsqueeze(1)  # [B, 1, D]
            squeeze_output = True
        else:
            squeeze_output = False
        
        # sumå…³æ³¨diffçš„ä¿¡æ¯
        sum_enhanced, _ = self.sum_to_diff(sum_feat, diff_feat, diff_feat)
        # diffå…³æ³¨sumçš„ä¿¡æ¯  
        diff_enhanced, _ = self.diff_to_sum(diff_feat, sum_feat, sum_feat)
        
        # æ‹¼æ¥å¢å¼ºåçš„ç‰¹å¾ [B, seq_len, 2*feature_dim]
        concatenated = torch.cat([sum_enhanced, diff_enhanced], dim=-1)
        
        # é€šè¿‡æŠ•å½±å±‚å˜æ¢å›æ ‡å‡†ç»´åº¦ [B, seq_len, feature_dim]
        result = self.fusion_projection(concatenated)
        
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œåˆ™å°†è¾“å‡ºä¹Ÿè½¬æ¢ä¸º2D
        if squeeze_output:
            result = result.squeeze(1)
        
        return result


class GatedFusion(nn.Module):
    """é—¨æ§æœºåˆ¶èåˆæ¨¡å—"""
    def __init__(self, feature_dim=128):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )
        self.transform = nn.Linear(feature_dim * 2, feature_dim)
        
    def forward(self, sum_feat, diff_feat):
        concat_feat = torch.cat([sum_feat, diff_feat], dim=-1)
        gate_weights = self.gate(concat_feat)
        transformed_feat = self.transform(concat_feat)
        return transformed_feat * gate_weights


class TimeSeriesStatisticalExtractor(nn.Module):
    """æ—¶åºç»Ÿè®¡ç‰¹å¾æå–å™¨
    
    ä»åŸå§‹æ—¶åºæ•°æ®ä¸­æå–ç»Ÿè®¡ç‰¹å¾å’Œä¿¡å·é—´å…³è”ç‰¹å¾ï¼Œ
    é‡ç‚¹å…³æ³¨ä¿¡å·é—´çš„é™æ€å…³ç³»è€Œéæ—¶é—´ä¾èµ–æ€§
    """
    def __init__(self, num_signals, time_length, feature_dim=128, stat_type='comprehensive'):
        super().__init__()
        self.num_signals = num_signals
        self.time_length = time_length
        self.feature_dim = feature_dim
        self.stat_type = stat_type
        
        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾çš„ç»´åº¦
        self.stat_feature_dim = self._calculate_stat_dim()
        
        # ç‰¹å¾æŠ•å½±å±‚
        self.feature_projection = nn.Sequential(
            nn.Linear(self.stat_feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        print(f"æ—¶åºç»Ÿè®¡ç‰¹å¾æå–å™¨åˆå§‹åŒ–:")
        print(f"  - ä¿¡å·æ•°é‡: {num_signals}")
        print(f"  - æ—¶é—´é•¿åº¦: {time_length}")
        print(f"  - ç»Ÿè®¡ç‰¹å¾ç»´åº¦: {self.stat_feature_dim}")
        print(f"  - è¾“å‡ºç‰¹å¾ç»´åº¦: {feature_dim}")
        print(f"  - ç»Ÿè®¡ç±»å‹: {stat_type}")
    
    def _calculate_stat_dim(self):
        """è®¡ç®—ç»Ÿè®¡ç‰¹å¾çš„æ€»ç»´åº¦"""
        if self.stat_type == 'basic':
            # åŸºç¡€ç»Ÿè®¡ï¼šå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€ä¸­ä½æ•°
            return self.num_signals * 5
        elif self.stat_type == 'comprehensive':
            # ç»¼åˆç»Ÿè®¡ï¼šåŸºç¡€ç»Ÿè®¡ + ç›¸å…³æ€§çŸ©é˜µ + é«˜é˜¶ç»Ÿè®¡
            basic_dim = self.num_signals * 8  # å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€ä¸­ä½æ•°ã€ååº¦ã€å³°åº¦ã€å˜å¼‚ç³»æ•°
            corr_dim = self.num_signals * (self.num_signals - 1) // 2  # ä¸Šä¸‰è§’ç›¸å…³æ€§çŸ©é˜µ
            return basic_dim + corr_dim
        elif self.stat_type == 'correlation_focused':
            # ä¸“æ³¨äºç›¸å…³æ€§çš„ç»Ÿè®¡
            basic_dim = self.num_signals * 5  # å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€ä¸­ä½æ•°
            corr_dim = self.num_signals * (self.num_signals - 1) // 2  # ç›¸å…³æ€§çŸ©é˜µ
            cross_dim = self.num_signals * (self.num_signals - 1) // 2 * 2  # äº¤å‰ç»Ÿè®¡ç‰¹å¾ï¼šæ¯å¯¹ä¿¡å·2ä¸ªç‰¹å¾
            return basic_dim + corr_dim + cross_dim
        else:
            raise ValueError(f"Unknown stat_type: {self.stat_type}")
    
    def extract_statistical_features(self, x):
        """æå–ç»Ÿè®¡ç‰¹å¾
        
        Args:
            x: [B, T, C] åŸå§‹æ—¶åºæ•°æ®
            
        Returns:
            features: [B, stat_feature_dim] ç»Ÿè®¡ç‰¹å¾
        """
        B, T, C = x.shape
        features = []
        
        if self.stat_type in ['basic', 'comprehensive', 'correlation_focused']:
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾ï¼ˆæŒ‰ä¿¡å·è®¡ç®—ï¼‰
            mean_vals = torch.mean(x, dim=1)  # [B, C]
            std_vals = torch.std(x, dim=1)    # [B, C]
            max_vals = torch.max(x, dim=1)[0] # [B, C]
            min_vals = torch.min(x, dim=1)[0] # [B, C]
            median_vals = torch.median(x, dim=1)[0]  # [B, C]
            
            features.extend([mean_vals, std_vals, max_vals, min_vals, median_vals])
            
            if self.stat_type == 'comprehensive':
                # é«˜é˜¶ç»Ÿè®¡ç‰¹å¾
                # ååº¦ï¼ˆä¸‰é˜¶çŸ©ï¼‰
                centered = x - mean_vals.unsqueeze(1)
                skewness = torch.mean(centered**3, dim=1) / (std_vals**3 + 1e-8)  # [B, C]
                
                # å³°åº¦ï¼ˆå››é˜¶çŸ©ï¼‰
                kurtosis = torch.mean(centered**4, dim=1) / (std_vals**4 + 1e-8)  # [B, C]
                
                # å˜å¼‚ç³»æ•°
                cv = std_vals / (torch.abs(mean_vals) + 1e-8)  # [B, C]
                
                features.extend([skewness, kurtosis, cv])
        
        # ä¿¡å·é—´ç›¸å…³æ€§ç‰¹å¾
        if self.stat_type in ['comprehensive', 'correlation_focused']:
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            corr_features = []
            for b in range(B):
                # è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„ç›¸å…³æ€§çŸ©é˜µ
                sample = x[b]  # [T, C]
                
                # æ ‡å‡†åŒ–
                sample_mean = torch.mean(sample, dim=0, keepdim=True)
                sample_std = torch.std(sample, dim=0, keepdim=True)
                sample_normalized = (sample - sample_mean) / (sample_std + 1e-8)
                
                # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
                corr_matrix = torch.mm(sample_normalized.T, sample_normalized) / (T - 1)
                
                # æå–ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
                triu_indices = torch.triu_indices(C, C, offset=1)
                corr_values = corr_matrix[triu_indices[0], triu_indices[1]]
                corr_features.append(corr_values)
            
            corr_features = torch.stack(corr_features, dim=0)  # [B, corr_dim]
            features.append(corr_features)
        
        # äº¤å‰ç»Ÿè®¡ç‰¹å¾ï¼ˆä»…ç”¨äºcorrelation_focusedï¼‰
        if self.stat_type == 'correlation_focused':
            # ä¿¡å·é—´çš„æœ€å¤§å·®å¼‚å’Œæœ€å°å·®å¼‚
            signal_diffs = []
            for i in range(C):
                for j in range(i+1, C):
                    diff = x[:, :, i] - x[:, :, j]  # [B, T]
                    max_diff = torch.max(torch.abs(diff), dim=1)[0]  # [B]
                    mean_abs_diff = torch.mean(torch.abs(diff), dim=1)  # [B]
                    signal_diffs.extend([max_diff, mean_abs_diff])
            
            if signal_diffs:
                cross_features = torch.stack(signal_diffs, dim=1)  # [B, cross_dim]
                features.append(cross_features)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        all_features = torch.cat(features, dim=1)  # [B, stat_feature_dim]
        return all_features
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­
        
        Args:
            x: [B, T, C] æˆ– [B, C, T] åŸå§‹æ—¶åºæ•°æ®
            
        Returns:
            features: [B, feature_dim] æŠ•å½±åçš„ç‰¹å¾
        """
        # ç¡®ä¿è¾“å…¥æ ¼å¼ä¸º [B, T, C]
        if x.dim() == 3:
            # ä¼˜å…ˆæ£€æŸ¥æ—¶é—´ç»´åº¦ï¼Œç„¶åæ£€æŸ¥ä¿¡å·ç»´åº¦
            if x.shape[1] == self.time_length and x.shape[2] == self.num_signals:
                # [B, T, C] - æ­£ç¡®æ ¼å¼
                pass
            elif x.shape[1] == self.num_signals and x.shape[2] == self.time_length:
                # [B, C, T] - éœ€è¦è½¬ç½®
                x = x.transpose(1, 2)  # [B, T, C]
            else:
                raise ValueError(f"è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¸åŒ¹é…ï¼šæœŸæœ›å½¢çŠ¶ä¸º[B, {self.time_length}, {self.num_signals}]æˆ–[B, {self.num_signals}, {self.time_length}]ï¼Œä½†å¾—åˆ°å½¢çŠ¶{x.shape}")
        else:
            raise ValueError(f"è¾“å…¥ç»´åº¦åº”ä¸º3ï¼Œå½“å‰ä¸º{x.dim()}")
        
        # æå–ç»Ÿè®¡ç‰¹å¾
        stat_features = self.extract_statistical_features(x)  # [B, stat_feature_dim]
        
        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        projected_features = self.feature_projection(stat_features)  # [B, feature_dim]
        
        return projected_features


class MultiModalFusion(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å—
    
    èåˆGAFå›¾åƒç‰¹å¾å’Œæ—¶åºç»Ÿè®¡ç‰¹å¾
    """
    def __init__(self, feature_dim=128, fusion_strategy='concat'):
        super().__init__()
        self.feature_dim = feature_dim
        self.fusion_strategy = fusion_strategy
        
        if fusion_strategy == 'concat':
            # ç®€å•æ‹¼æ¥åæŠ•å½±
            self.fusion_layer = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        elif fusion_strategy == 'attention':
            # æ³¨æ„åŠ›èåˆ
            self.attention = nn.MultiheadAttention(feature_dim, num_heads=4, batch_first=True)
            self.norm = nn.LayerNorm(feature_dim)
        elif fusion_strategy == 'gated':
            # é—¨æ§èåˆ
            self.gate = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.Sigmoid()
            )
            self.transform = nn.Linear(feature_dim * 2, feature_dim)
        elif fusion_strategy == 'adaptive':
            # è‡ªé€‚åº”èåˆ
            self.weight_net = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.Sigmoid()
            )
        else:
            raise ValueError(f"Unknown fusion strategy: {fusion_strategy}")
    
    def forward(self, gaf_features, stat_features):
        """
        Args:
            gaf_features: [B, C, feature_dim] GAFå›¾åƒç‰¹å¾
            stat_features: [B, feature_dim] ç»Ÿè®¡ç‰¹å¾
            
        Returns:
            fused_features: [B, C, feature_dim] èåˆåçš„ç‰¹å¾
        """
        B, C, D = gaf_features.shape
        
        # å°†ç»Ÿè®¡ç‰¹å¾æ‰©å±•åˆ°æ¯ä¸ªä¿¡å·
        stat_features_expanded = stat_features.unsqueeze(1).expand(B, C, D)  # [B, C, feature_dim]
        
        if self.fusion_strategy == 'concat':
            # æ‹¼æ¥èåˆ
            concat_features = torch.cat([gaf_features, stat_features_expanded], dim=-1)  # [B, C, 2*feature_dim]
            fused_features = self.fusion_layer(concat_features)  # [B, C, feature_dim]
            
        elif self.fusion_strategy == 'attention':
            # æ³¨æ„åŠ›èåˆ
            # å°†GAFç‰¹å¾ä½œä¸ºqueryï¼Œç»Ÿè®¡ç‰¹å¾ä½œä¸ºkeyå’Œvalue
            gaf_flat = gaf_features.reshape(B*C, 1, D)  # [B*C, 1, D]
            stat_flat = stat_features_expanded.contiguous().view(B*C, 1, D)  # [B*C, 1, D]
            
            attended, _ = self.attention(gaf_flat, stat_flat, stat_flat)  # [B*C, 1, D]
            attended = attended.view(B, C, D)  # [B, C, D]
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            fused_features = self.norm(gaf_features + attended)
            
        elif self.fusion_strategy == 'gated':
            # é—¨æ§èåˆ
            concat_features = torch.cat([gaf_features, stat_features_expanded], dim=-1)
            gate_weights = self.gate(concat_features)  # [B, C, feature_dim]
            transformed_features = self.transform(concat_features)  # [B, C, feature_dim]
            fused_features = transformed_features * gate_weights
            
        elif self.fusion_strategy == 'adaptive':
            # è‡ªé€‚åº”èåˆ
            concat_features = torch.cat([gaf_features, stat_features_expanded], dim=-1)
            weights = self.weight_net(concat_features)  # [B, C, feature_dim]
            fused_features = gaf_features * weights + stat_features_expanded * (1 - weights)
        
        return fused_features


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å— - å¯¹ä¿¡å·é€šé“ç»´åº¦è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—"""
    def __init__(self, num_channels, reduction=8):
        super().__init__()
        # å¯¹ç‰¹å¾ç»´åº¦è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        
        # ç¡®ä¿reductionåçš„é€šé“æ•°è‡³å°‘ä¸º1
        reduced_channels = max(1, num_channels // reduction)
        
        # MLPç”¨äºå­¦ä¹ é€šé“é—´çš„å…³ç³»
        self.fc = nn.Sequential(
            nn.Linear(num_channels, reduced_channels),
            nn.ReLU(),
            nn.Linear(reduced_channels, num_channels)
        )
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x: [B, C, feature_dim] å…¶ä¸­Cæ˜¯ä¿¡å·æ•°é‡
        B, C, D = x.shape
        
        # å¯¹æ¯ä¸ªä¿¡å·çš„ç‰¹å¾ç»´åº¦è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        # [B, C, D] -> [B, C, 1] -> [B, C]
        avg_out = self.avg_pool(x).squeeze(-1)  # [B, C]
        max_out = self.max_pool(x).squeeze(-1)  # [B, C]
        
        # é€šè¿‡MLPå­¦ä¹ é€šé“æ³¨æ„åŠ›æƒé‡
        avg_attention = self.fc(avg_out)  # [B, C]
        max_attention = self.fc(max_out)  # [B, C]
        
        # èåˆå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–çš„æ³¨æ„åŠ›
        attention = self.sigmoid(avg_attention + max_attention)  # [B, C]
        
        # æ‰©å±•ç»´åº¦å¹¶åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attention = attention.unsqueeze(-1)  # [B, C, 1]
        return x * attention  # [B, C, D]


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆå¯¹ä¿¡å·ç»´åº¦ï¼‰"""
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv1 = nn.Conv1d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x: [B, C, feature_dim]
        # å¯¹ç‰¹å¾ç»´åº¦è¿›è¡Œå¹³å‡å’Œæœ€å¤§æ“ä½œï¼Œå¾—åˆ°æ¯ä¸ªä¿¡å·çš„ä»£è¡¨æ€§ç‰¹å¾
        avg_out = torch.mean(x, dim=2, keepdim=True)  # [B, C, 1]
        max_out, _ = torch.max(x, dim=2, keepdim=True)  # [B, C, 1]
        
        # æ‹¼æ¥å¹³å‡å’Œæœ€å¤§ç‰¹å¾
        x_cat = torch.cat([avg_out, max_out], dim=2)  # [B, C, 2]
        x_cat = x_cat.transpose(1, 2)  # [B, 2, C]
        
        # é€šè¿‡1Då·ç§¯å­¦ä¹ ä¿¡å·é—´çš„ç©ºé—´å…³ç³»
        attention = self.conv1(x_cat)  # [B, 1, C]
        attention = attention.transpose(1, 2)  # [B, C, 1]
        attention = self.sigmoid(attention)
        
        return x * attention  # [B, C, feature_dim]


class SignalAttention(nn.Module):
    """ä¿¡å·æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, feature_dim, num_signals, attention_type='channel'):
        super().__init__()
        self.attention_type = attention_type
        
        if attention_type == 'channel':
            self.attention = ChannelAttention(num_signals)  # ä¼ å…¥ä¿¡å·æ•°é‡è€Œä¸æ˜¯ç‰¹å¾ç»´åº¦
        elif attention_type == 'spatial':
            self.attention = SpatialAttention()
        elif attention_type == 'cbam':
            self.channel_attention = ChannelAttention(num_signals)  # ä¼ å…¥ä¿¡å·æ•°é‡
            self.spatial_attention = SpatialAttention()
        elif attention_type == 'self':
            self.self_attention = nn.MultiheadAttention(feature_dim, num_heads=8, batch_first=True)
        else:
            raise ValueError(f"Unknown attention type: {attention_type}")
    
    def forward(self, x):
        # x: [B, C, feature_dim]
        if self.attention_type == 'channel':
            return self.attention(x)
        elif self.attention_type == 'spatial':
            return self.attention(x)
        elif self.attention_type == 'cbam':
            x = self.channel_attention(x)
            x = self.spatial_attention(x)
            return x
        elif self.attention_type == 'self':
            # x: [B, C, feature_dim] -> [B, C, feature_dim]
            x_reshaped = x.reshape(x.size(0), x.size(1), -1)
            attn_out, _ = self.self_attention(x_reshaped, x_reshaped, x_reshaped)
            return attn_out


class ResidualBlock(nn.Module):
    """
    æ®‹å·®å—ï¼Œç”¨äºåˆ†ç±»å™¨ä¸­çš„ç‰¹å¾å­¦ä¹ 
    
    æ”¯æŒä¸åŒç±»å‹çš„æ®‹å·®è¿æ¥ï¼š
    - 'basic': åŸºç¡€æ®‹å·®å—
    - 'bottleneck': ç“¶é¢ˆæ®‹å·®å—ï¼ˆé€‚ç”¨äºé«˜ç»´ç‰¹å¾ï¼‰
    - 'dense': å¯†é›†è¿æ¥æ®‹å·®å—
    """
    def __init__(self, in_dim, out_dim, hidden_dim=None, block_type='basic', dropout=0.1):
        super().__init__()
        self.block_type = block_type
        self.in_dim = in_dim
        self.out_dim = out_dim
        
        if hidden_dim is None:
            hidden_dim = max(in_dim, out_dim) // 2
        
        if block_type == 'basic':
            # åŸºç¡€æ®‹å·®å—ï¼šè¾“å…¥ -> çº¿æ€§ -> æ¿€æ´» -> Dropout -> çº¿æ€§ -> è¾“å‡º
            self.main_path = nn.Sequential(
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, out_dim)
            )
        elif block_type == 'bottleneck':
            # ç“¶é¢ˆæ®‹å·®å—ï¼šé™ç»´ -> å¤„ç† -> å‡ç»´
            bottleneck_dim = hidden_dim // 2
            self.main_path = nn.Sequential(
                nn.Linear(in_dim, bottleneck_dim),
                nn.ReLU(inplace=True),
                nn.Linear(bottleneck_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, out_dim)
            )
        elif block_type == 'dense':
            # å¯†é›†è¿æ¥æ®‹å·®å—ï¼šæ›´å¤æ‚çš„ç‰¹å¾ç»„åˆ
            self.main_path = nn.Sequential(
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, out_dim)
            )
        
        # å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒï¼Œéœ€è¦æŠ•å½±å±‚
        if in_dim != out_dim:
            self.shortcut = nn.Linear(in_dim, out_dim)
        else:
            self.shortcut = nn.Identity()
        
        # æœ€ç»ˆæ¿€æ´»å‡½æ•°
        self.final_activation = nn.ReLU(inplace=True)
        
    def forward(self, x):
        residual = self.shortcut(x)
        out = self.main_path(x)
        out = out + residual
        out = self.final_activation(out)
        return out


class ResidualClassifier(nn.Module):
    """
    åŸºäºæ®‹å·®å—çš„åˆ†ç±»å™¨
    
    Args:
        input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        num_classes (int): åˆ†ç±»ç±»åˆ«æ•°
        hidden_dims (list): éšè—å±‚ç»´åº¦åˆ—è¡¨
        block_type (str): æ®‹å·®å—ç±»å‹
        dropout (float): Dropoutæ¯”ç‡
        use_batch_norm (bool): æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
    """
    def __init__(self, input_dim, num_classes, hidden_dims=None, 
                 block_type='basic', dropout=0.1, use_batch_norm=False):
        super().__init__()
        
        if hidden_dims is None:
            # æ ¹æ®è¾“å…¥ç»´åº¦è‡ªåŠ¨è®¾è®¡ç½‘ç»œç»“æ„
            if input_dim > 2048:
                hidden_dims = [2048, 1024, 512, 256]
            elif input_dim > 1024:
                hidden_dims = [1024, 512, 256]
            else:
                hidden_dims = [512, 256]
        
        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList() if use_batch_norm else None
        
        # æ„å»ºæ®‹å·®å±‚åºåˆ—
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            self.layers.append(ResidualBlock(
                in_dim=prev_dim,
                out_dim=hidden_dim,
                block_type=block_type,
                dropout=dropout
            ))
            
            if use_batch_norm:
                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))
            
            prev_dim = hidden_dim
        
        # æœ€ç»ˆåˆ†ç±»å±‚
        self.classifier = nn.Linear(prev_dim, num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if self.batch_norms is not None:
                x = self.batch_norms[i](x)
        
        x = self.classifier(x)
        return x


class DualGAFNet(nn.Module):
    """åŒè·¯GAFç½‘ç»œ"""
    def __init__(
        self, 
        feature_dim=32, 
        num_classes=4, 
        num_images=30,
        time_length=96,  # æ–°å¢ï¼šæ—¶é—´åºåˆ—é•¿åº¦
        # ç‰¹å¾æå–å™¨é…ç½®
        extractor_type='large_kernel',  # 'large_kernel', 'inception', 'dilated', 'multiscale'
        # èåˆæ¨¡å—é…ç½®
        fusion_type='adaptive',  # 'adaptive', 'concat', 'add', 'mul', 'weighted_add'
        # æ³¨æ„åŠ›æ¨¡å—é…ç½®
        attention_type='channel',  # 'channel', 'spatial', 'cbam', 'self', 'none'
        # åˆ†ç±»å™¨é…ç½®
        classifier_type='mlp',  # 'mlp', 'simple'
        # ç»Ÿè®¡ç‰¹å¾é…ç½®
        use_statistical_features=True,  # æ˜¯å¦ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾
        stat_type='comprehensive',  # 'basic', 'comprehensive', 'correlation_focused'
        multimodal_fusion_strategy='concat',  # 'concat', 'attention', 'gated', 'adaptive'
        # æ¶ˆèå®éªŒå¼€å…³
        use_diff_branch=True,  # æ˜¯å¦ä½¿ç”¨diffåˆ†æ”¯è¿›è¡Œèåˆï¼ˆæ¶ˆèå®éªŒï¼‰
        # å…¶ä»–é…ç½®
        hvac_groups=None,
        feature_columns=None,
        nhead=4,
        num_layers=2
    ):
        super().__init__()
        self.num_images = num_images
        self.time_length = time_length
        self.feature_dim = feature_dim
        self.extractor_type = extractor_type
        self.fusion_type = fusion_type
        self.attention_type = attention_type
        self.classifier_type = classifier_type
        self.use_statistical_features = use_statistical_features
        self.stat_type = stat_type
        self.multimodal_fusion_strategy = multimodal_fusion_strategy
        # æ¶ˆèå®éªŒå¼€å…³
        self.use_diff_branch = use_diff_branch
        
        # HVACåˆ†ç»„é…ç½®
        self.hvac_groups = hvac_groups
        self.feature_columns = feature_columns if feature_columns else []
        self.use_grouping = hvac_groups is not None
        
        # æ™ºèƒ½æ¨èç‰¹å¾æå–å™¨ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼‰
        if extractor_type == 'auto':
            recommended_extractor = self._get_recommended_extractor_for_signal_count(num_images)
            print(f"æ ¹æ®ä¿¡å·æ•°é‡ {num_images} è‡ªåŠ¨æ¨èç‰¹å¾æå–å™¨: {recommended_extractor}")
            self.extractor_type = recommended_extractor
        
        # åˆ›å»ºç‰¹å¾æå–å™¨
        self._build_extractors()
        
        # åˆ›å»ºèåˆæ¨¡å—ï¼ˆä»…åœ¨ä½¿ç”¨diffåˆ†æ”¯æ—¶ï¼‰
        if self.use_diff_branch:
            self._build_fusion_module()
        
        # åˆ›å»ºç»Ÿè®¡ç‰¹å¾æå–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_statistical_features:
            self._build_statistical_extractor()
            self._build_multimodal_fusion()
        
        # åˆ›å»ºä¿¡å·æ³¨æ„åŠ›æ¨¡å—
        self._build_attention_module()
        
        # åˆ›å»ºåˆ†ç±»å™¨
        self._build_classifier(num_classes)
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        print(f"å¢å¼ºåŒè·¯GAFç½‘ç»œæ„å»ºå®Œæˆ:")
        print(f"  - ç‰¹å¾æå–å™¨: {self.extractor_type}")
        if self.use_diff_branch:
            print(f"  - èåˆæ¨¡å—: {fusion_type}")
        else:
            print(f"  - èåˆæ¨¡å—: å·²ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒï¼šä»…ä½¿ç”¨sumåˆ†æ”¯ï¼‰")
        print(f"  - æ³¨æ„åŠ›æ¨¡å—: {attention_type}")
        if attention_type == 'none':
            print(f"    ğŸ’¡ æ³¨æ„åŠ›å·²ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒï¼‰")
        print(f"  - åˆ†ç±»å™¨: {classifier_type}")
        print(f"  - ä¿¡å·æ•°é‡: {num_images}")
        print(f"  - GAFå›¾åƒå°ºå¯¸: {time_length}x{time_length}")
        print(f"  - ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾: {use_statistical_features}")
        if not use_statistical_features:
            print(f"    ğŸ’¡ ç»Ÿè®¡ç‰¹å¾å·²ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒï¼‰")
        if use_statistical_features:
            print(f"  - ç»Ÿè®¡ç‰¹å¾ç±»å‹: {stat_type}")
            print(f"  - å¤šæ¨¡æ€èåˆç­–ç•¥: {multimodal_fusion_strategy}")
        print(f"  - ä½¿ç”¨åˆ†ç»„: {self.use_grouping}")
        
        # æ¶ˆèå®éªŒçŠ¶æ€æç¤º
        ablation_info = []
        if not self.use_diff_branch:
            ablation_info.append("GAFå·®åˆ†åˆ†æ”¯æ¶ˆè")
        if not self.use_statistical_features:
            ablation_info.append("ç»Ÿè®¡ç‰¹å¾æ¶ˆè")
        if self.attention_type == 'none':
            ablation_info.append("æ³¨æ„åŠ›æœºåˆ¶æ¶ˆè")
        
        if ablation_info:
            print(f"\nğŸ”¬ æ¶ˆèå®éªŒçŠ¶æ€: {' + '.join(ablation_info)}")
        else:
            print(f"\nğŸ”¬ æ¶ˆèå®éªŒçŠ¶æ€: å®Œæ•´æ¨¡å‹ï¼ˆæœªå¯ç”¨æ¶ˆèï¼‰")
        
        # æ‰“å°å¯ç”¨çš„ç‰¹å¾æå–å™¨ç±»å‹
        print(f"\næ”¯æŒçš„ç‰¹å¾æå–å™¨ç±»å‹:")
        print(f"ğŸ“Š GAFä¼˜åŒ–ResNetç³»åˆ— (æ¨è):")
        print(f"  - resnet18_gaf: GAFä¼˜åŒ–ResNet18 (é€‚åˆâ‰¤30ä¸ªä¿¡å·) â­")
        print(f"  - resnet18_gaf_light: è½»é‡çº§GAF ResNet (é€‚åˆ30-60ä¸ªä¿¡å·) â­")
        print(f"  - resnet_gaf_deep: æ·±å±‚GAF ResNet (æ›´å¥½è¡¨è¾¾èƒ½åŠ›)")
        print(f"  - resnet_gaf_preserve: é«˜ä¿çœŸGAF ResNet (æœ€å¤§ç¨‹åº¦ä¿ç•™ç©ºé—´ä¿¡æ¯)")
        print(f"ğŸ”§ ä¼ ç»ŸResNetç³»åˆ— (å…¼å®¹æ€§):")
        print(f"  - resnet18: æ ‡å‡†ResNet18 (è¿‡æ—©ä¸‹é‡‡æ ·)")
        print(f"  - resnet34: æ ‡å‡†ResNet34 (è¿‡æ—©ä¸‹é‡‡æ ·)")
        print(f"  - resnet_light: è½»é‡çº§ResNet")
        print(f"âš¡ å…¶ä»–ä¼˜åŒ–æ¶æ„:")
        print(f"  - optimized_large_kernel: ä¼˜åŒ–å¤§æ ¸å·ç§¯ (é€‚åˆ60-120ä¸ªä¿¡å·)")
        print(f"  - optimized_dilated: ä¼˜åŒ–è†¨èƒ€å·ç§¯ (é€‚åˆ>120ä¸ªä¿¡å·)")
        print(f"  - large_kernel: åŸå§‹å¤§æ ¸ç‰¹å¾æå–å™¨")
        print(f"  - inception: Inceptionç»“æ„ç‰¹å¾æå–å™¨")
        print(f"  - dilated: è†¨èƒ€å·ç§¯ç‰¹å¾æå–å™¨")
        print(f"  - multiscale: å¤šå°ºåº¦ç‰¹å¾æå–å™¨")
        print(f"ğŸ¤– æ™ºèƒ½é€‰æ‹©:")
        print(f"  - auto: æ ¹æ®ä¿¡å·æ•°é‡è‡ªåŠ¨é€‰æ‹© (æ¨èGAFä¼˜åŒ–ç‰ˆæœ¬)")

    def _build_extractors(self):
        """æ„å»ºç‰¹å¾æå–å™¨"""
        if self.use_grouping:
            # ä½¿ç”¨åˆ†ç»„ç‰¹å¾æå–
            self.channel_to_group = self._create_channel_mapping()
            self.num_groups = len(self.hvac_groups)
            
            # ä¸ºæ¯ä¸ªåˆ†æ”¯çš„æ¯ä¸ªç»„åˆ›å»ºç‰¹å¾æå–å™¨
            self.sum_extractors = nn.ModuleDict()
            if self.use_diff_branch:
                self.diff_extractors = nn.ModuleDict()
            
            for group_idx in range(self.num_groups):
                self.sum_extractors[f'group_{group_idx}'] = self._create_extractor()
                if self.use_diff_branch:
                    self.diff_extractors[f'group_{group_idx}'] = self._create_extractor()
            
            # å¦‚æœæœ‰æœªåˆ†ç»„çš„é€šé“ï¼Œåˆ›å»ºé»˜è®¤ç‰¹å¾æå–å™¨
            if -1 in self.channel_to_group.values():
                self.sum_extractors['default'] = self._create_extractor()
                if self.use_diff_branch:
                    self.diff_extractors['default'] = self._create_extractor()
            
            if self.use_diff_branch:
                print(f"åˆ›å»ºäº† {len(self.sum_extractors)} ä¸ªåˆ†ç»„ç‰¹å¾æå–å™¨ï¼ˆsum + diffåˆ†æ”¯ï¼‰")
            else:
                print(f"åˆ›å»ºäº† {len(self.sum_extractors)} ä¸ªåˆ†ç»„ç‰¹å¾æå–å™¨ï¼ˆä»…sumåˆ†æ”¯ï¼‰")
        else:
            # ä¸ä½¿ç”¨åˆ†ç»„ï¼Œæ‰€æœ‰é€šé“å…±ç”¨ç‰¹å¾æå–å™¨
            self.sum_extractor = self._create_extractor()
            if self.use_diff_branch:
                self.diff_extractor = self._create_extractor()
            
            if self.use_diff_branch:
                print("åˆ›å»ºäº†ç»Ÿä¸€çš„ç‰¹å¾æå–å™¨ï¼ˆsum + diffåˆ†æ”¯ï¼‰")
            else:
                print("åˆ›å»ºäº†ç»Ÿä¸€çš„ç‰¹å¾æå–å™¨ï¼ˆä»…sumåˆ†æ”¯ï¼‰")

    def _create_extractor(self):
        """æ ¹æ®é…ç½®åˆ›å»ºç‰¹å¾æå–å™¨"""
        if self.extractor_type == 'large_kernel':
            return NoPaddingLargeKernelFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'inception':
            return InceptionFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'dilated':
            return LargeKernelDilatedFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'multiscale':
            return MultiScaleStackedFeatureExtractor(self.feature_dim)
        # åŸå§‹ResNetæ¶æ„ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰
        elif self.extractor_type == 'resnet18':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet18')
        elif self.extractor_type == 'resnet34':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet34')
        elif self.extractor_type == 'resnet_light':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet_light')
        # æ–°çš„GAFä¼˜åŒ–ResNetæ¶æ„
        elif self.extractor_type == 'resnet18_gaf':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet18_gaf')
        elif self.extractor_type == 'resnet18_gaf_light':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet18_gaf_light')
        elif self.extractor_type == 'resnet_gaf_deep':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet_gaf_deep')
        elif self.extractor_type == 'resnet_gaf_preserve':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet_gaf_preserve')
        # å…¶ä»–ä¼˜åŒ–çš„ç‰¹å¾æå–å™¨
        elif self.extractor_type == 'optimized_large_kernel':
            return OptimizedLargeKernelFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'optimized_dilated':
            return OptimizedDilatedFeatureExtractor(self.feature_dim)
        else:
            raise ValueError(f"Unknown extractor type: {self.extractor_type}")

    def _get_recommended_extractor_for_signal_count(self, signal_count):
        """æ ¹æ®ä¿¡å·æ•°é‡æ¨èç‰¹å¾æå–å™¨"""
        if signal_count <= 30:
            return 'resnet18_gaf'  # å°æ•°æ®é›†ä½¿ç”¨GAFä¼˜åŒ–ResNet18
        elif signal_count <= 60:
            return 'resnet18_gaf_light'  # ä¸­ç­‰æ•°æ®é›†ä½¿ç”¨è½»é‡çº§GAF ResNet
        elif signal_count <= 120:
            return 'optimized_large_kernel'  # å¤§æ•°æ®é›†ä½¿ç”¨ä¼˜åŒ–çš„è½»é‡çº§æå–å™¨
        else:
            return 'optimized_dilated'  # è¶…å¤§æ•°æ®é›†ä½¿ç”¨è†¨èƒ€å·ç§¯å‡å°‘å‚æ•°

    def _create_channel_mapping(self):
        """åˆ›å»ºé€šé“ç´¢å¼•åˆ°ç»„çš„æ˜ å°„"""
        channel_to_group = {}
        
        if not self.feature_columns:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾åˆ—ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
            for i in range(self.num_images):
                channel_to_group[i] = 0
            return channel_to_group
        
        # ä¸ºæ¯ä¸ªé€šé“æ‰¾åˆ°å¯¹åº”çš„ç»„
        for channel_idx, column_name in enumerate(self.feature_columns):
            group_found = False
            
            for group_idx, group_signals in enumerate(self.hvac_groups):
                if any(signal in column_name.upper() for signal in [s.upper() for s in group_signals]):
                    channel_to_group[channel_idx] = group_idx
                    group_found = True
                    break
            
            if not group_found:
                channel_to_group[channel_idx] = -1
        
        return channel_to_group

    def _build_fusion_module(self):
        """æ„å»ºç‰¹å¾èåˆæ¨¡å—"""
        if self.fusion_type == 'adaptive':
            self.fusion = AdaptiveFusion(self.feature_dim)
        elif self.fusion_type == 'concat':
            self.fusion = ConcatFusion(self.feature_dim)
        elif self.fusion_type == 'bidirectional':
            self.fusion = BiDirectionalFusion(self.feature_dim, num_heads=4)
        elif self.fusion_type == 'gated':
            self.fusion = GatedFusion(self.feature_dim)
        elif self.fusion_type in ['add', 'mul', 'weighted_add']:
            self.fusion = ElementwiseFusion(self.feature_dim, self.fusion_type)
        else:
            raise ValueError(f"Unknown fusion type: {self.fusion_type}")

    def _build_statistical_extractor(self):
        """æ„å»ºç»Ÿè®¡ç‰¹å¾æå–å™¨"""
        self.statistical_extractor = TimeSeriesStatisticalExtractor(
            num_signals=self.num_images,
            time_length=self.time_length,
            feature_dim=self.feature_dim,
            stat_type=self.stat_type
        )

    def _build_multimodal_fusion(self):
        """æ„å»ºå¤šæ¨¡æ€èåˆæ¨¡å—"""
        self.multimodal_fusion = MultiModalFusion(
            feature_dim=self.feature_dim,
            fusion_strategy=self.multimodal_fusion_strategy
        )

    def _build_attention_module(self):
        """æ„å»ºæ³¨æ„åŠ›æ¨¡å—"""
        if self.attention_type == 'none':
            self.attention = nn.Identity()
        else:
            self.attention = SignalAttention(
                self.feature_dim, 
                self.num_images, 
                self.attention_type
            )

    def _build_classifier(self, num_classes):
        """æ„å»ºåˆ†ç±»å™¨"""
        # æ‰€æœ‰èåˆæ–¹å¼ç°åœ¨éƒ½è¾“å‡ºæ ‡å‡†çš„feature_dimç»´åº¦
        final_feature_dim = self.feature_dim * self.num_images
        
        if self.classifier_type == 'mlp':
            self.classifier = nn.Sequential(
                nn.Linear(final_feature_dim, 2048),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(2048, 512),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(512, 128),
                nn.ReLU(),
                nn.Linear(128, num_classes),
            )
        elif self.classifier_type == 'simple':
            self.classifier = nn.Sequential(
                nn.Linear(final_feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(512, num_classes),
            )
        elif self.classifier_type == 'residual':
            # åŸºç¡€æ®‹å·®åˆ†ç±»å™¨
            self.classifier = ResidualClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                hidden_dims=[1024, 512, 256],
                block_type='basic',
                dropout=0.1,
                use_batch_norm=False
            )
            print(f"ğŸ—ï¸ æ„å»ºæ®‹å·®åˆ†ç±»å™¨ (åŸºç¡€æ®‹å·®å—): {final_feature_dim} -> [1024, 512, 256] -> {num_classes}")
        elif self.classifier_type == 'residual_bottleneck':
            # ç“¶é¢ˆæ®‹å·®åˆ†ç±»å™¨ï¼ˆé€‚ç”¨äºé«˜ç»´ç‰¹å¾ï¼‰
            self.classifier = ResidualClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                hidden_dims=[2048, 1024, 512] if final_feature_dim > 1000 else [1024, 512, 256],
                block_type='bottleneck',
                dropout=0.15,
                use_batch_norm=True
            )
            hidden_info = "[2048, 1024, 512]" if final_feature_dim > 1000 else "[1024, 512, 256]"
            print(f"ğŸ—ï¸ æ„å»ºç“¶é¢ˆæ®‹å·®åˆ†ç±»å™¨ (å¸¦BatchNorm): {final_feature_dim} -> {hidden_info} -> {num_classes}")
        elif self.classifier_type == 'residual_dense':
            # å¯†é›†æ®‹å·®åˆ†ç±»å™¨ï¼ˆæœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼‰
            self.classifier = ResidualClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                hidden_dims=[1024, 512, 256, 128],
                block_type='dense',
                dropout=0.2,
                use_batch_norm=True
            )
            print(f"ğŸ—ï¸ æ„å»ºå¯†é›†æ®‹å·®åˆ†ç±»å™¨ (æœ€å¼ºè¡¨è¾¾èƒ½åŠ›): {final_feature_dim} -> [1024, 512, 256, 128] -> {num_classes}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {self.classifier_type}")
        
        print(f"âœ… åˆ†ç±»å™¨æ„å»ºå®Œæˆ: ç±»å‹={self.classifier_type}, è¾“å…¥ç»´åº¦={final_feature_dim}, è¾“å‡ºç±»åˆ«={num_classes}")

    def _extract_features(self, x, extractors_dict, default_extractor):
        """æå–ç‰¹å¾çš„é€šç”¨æ–¹æ³•"""
        B, C, H, W = x.shape
        
        if self.use_grouping:
            # ä½¿ç”¨åˆ†ç»„ç‰¹å¾æå–
            feats_list = []
            
            # æŒ‰ç»„æ‰¹é‡å¤„ç†
            for group_idx in range(self.num_groups):
                group_channels = [ch for ch, g in self.channel_to_group.items() if g == group_idx]
                
                if group_channels:
                    group_x = x[:, group_channels, :, :]
                    group_x = group_x.view(B * len(group_channels), 1, H, W)
                    
                    extractor = extractors_dict[f'group_{group_idx}']
                    group_feats = extractor(group_x)
                    group_feats = group_feats.view(B, len(group_channels), -1)
                    
                    for i, ch in enumerate(group_channels):
                        feats_list.append((ch, group_feats[:, i, :]))
            
            # å¤„ç†æœªåˆ†ç»„çš„é€šé“
            ungrouped_channels = [ch for ch, g in self.channel_to_group.items() if g == -1]
            if ungrouped_channels and 'default' in extractors_dict:
                for ch in ungrouped_channels:
                    channel_x = x[:, ch:ch+1, :, :]
                    channel_feat = extractors_dict['default'](channel_x)
                    feats_list.append((ch, channel_feat))
            
            # æŒ‰é€šé“ç´¢å¼•æ’åºå¹¶å †å 
            feats_list.sort(key=lambda x: x[0])
            feats = torch.stack([feat for _, feat in feats_list], dim=1)
        else:
            # ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾æå–å™¨
            x = x.view(B * C, 1, H, W)
            feats = default_extractor(x)
            feats = feats.view(B, C, -1)
        
        return feats

    def forward(self, sum_x, diff_x, time_series_x=None):
        """å‰å‘ä¼ æ’­
        
        Args:
            sum_x: [B, C, H, W] Summation GAFå›¾åƒ
            diff_x: [B, C, H, W] Difference GAFå›¾åƒï¼ˆæ¶ˆèå®éªŒæ—¶å¯å¿½ç•¥ï¼‰
            time_series_x: [B, C, T] æˆ– [B, T, C] åŸå§‹æ—¶åºæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        B, C, H, W = sum_x.shape
        if self.use_diff_branch:
            assert sum_x.shape == diff_x.shape, "ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å…¥å½¢çŠ¶å¿…é¡»ç›¸åŒ"
        assert C == self.num_images, f"è¾“å…¥é€šé“æ•°åº”ä¸º{self.num_images}ï¼Œå®é™…ä¸º{C}"
        
        # å¦‚æœä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ä½†æ²¡æœ‰æä¾›æ—¶åºæ•°æ®ï¼ŒæŠ›å‡ºé”™è¯¯
        if self.use_statistical_features and time_series_x is None:
            raise ValueError("å¯ç”¨ç»Ÿè®¡ç‰¹å¾æ—¶å¿…é¡»æä¾›åŸå§‹æ—¶åºæ•°æ® time_series_x")
        
        # æå–sumåˆ†æ”¯ç‰¹å¾
        if self.use_grouping:
            sum_feats = self._extract_features(sum_x, self.sum_extractors, None)
        else:
            sum_feats = self._extract_features(sum_x, None, self.sum_extractor)
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨diffåˆ†æ”¯å†³å®šç‰¹å¾èåˆç­–ç•¥
        if self.use_diff_branch:
            # æ ‡å‡†åŒè·¯GAFï¼šæå–diffåˆ†æ”¯ç‰¹å¾å¹¶èåˆ
            if self.use_grouping:
                diff_feats = self._extract_features(diff_x, self.diff_extractors, None)
            else:
                diff_feats = self._extract_features(diff_x, None, self.diff_extractor)
            
            # ç‰¹å¾èåˆ
            fused_feats = []
            for i in range(C):
                fused_feat = self.fusion(sum_feats[:, i, :], diff_feats[:, i, :])
                fused_feats.append(fused_feat)
            
            # æ‰€æœ‰èåˆç±»å‹ç°åœ¨éƒ½è¾“å‡ºæ ‡å‡†çš„feature_dimç»´åº¦
            fused_feats = torch.stack(fused_feats, dim=1)  # [B, C, feature_dim]
        else:
            # æ¶ˆèå®éªŒï¼šä»…ä½¿ç”¨sumåˆ†æ”¯ï¼Œä¸è¿›è¡Œèåˆ
            fused_feats = sum_feats  # [B, C, feature_dim]
        
        # å¦‚æœä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ï¼Œè¿›è¡Œå¤šæ¨¡æ€èåˆ
        if self.use_statistical_features:
            # æå–ç»Ÿè®¡ç‰¹å¾
            stat_features = self.statistical_extractor(time_series_x)  # [B, feature_dim]
            
            # å¤šæ¨¡æ€èåˆ
            fused_feats = self.multimodal_fusion(fused_feats, stat_features)  # [B, C, feature_dim]
        
        # ä¿¡å·æ³¨æ„åŠ›ï¼ˆå¦‚æœattention_type='none'ï¼Œåˆ™ç›¸å½“äºIdentityï¼‰
        attended_feats = self.attention(fused_feats)  # [B, C, feature_dim]
        
        # å±•å¹³ç”¨äºåˆ†ç±»
        merged = attended_feats.reshape(B, -1)  # [B, C * feature_dim]
        
        # åˆ†ç±»
        out = self.classifier(merged)
        return out


class Model(nn.Module):
    """åŒè·¯GAFç½‘ç»œçš„åŒ…è£…ç±»"""
    def __init__(self, configs):
        super().__init__()
        self.configs = configs
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°
        feature_dim = configs.feature_dim
        num_classes = configs.num_class
        num_images = configs.enc_in
        time_length = getattr(configs, 'seq_len', 96)  # æ—¶é—´åºåˆ—é•¿åº¦
        
        # è·å–æ¨¡å—é…ç½®
        extractor_type = getattr(configs, 'extractor_type', 'large_kernel')
        fusion_type = getattr(configs, 'fusion_type', 'adaptive')
        attention_type = getattr(configs, 'attention_type', 'channel')
        classifier_type = getattr(configs, 'classifier_type', 'mlp')
        
        # è·å–ç»Ÿè®¡ç‰¹å¾é…ç½®
        use_statistical_features = getattr(configs, 'use_statistical_features', True)
        stat_type = getattr(configs, 'stat_type', 'comprehensive')
        multimodal_fusion_strategy = getattr(configs, 'multimodal_fusion_strategy', 'concat')
        
        # è·å–æ¶ˆèå®éªŒé…ç½®
        use_diff_branch = getattr(configs, 'use_diff_branch', True)
        
        # è·å–HVACä¿¡å·åˆ†ç»„é…ç½®
        hvac_groups = getattr(configs, 'hvac_groups', None)
        feature_columns = getattr(configs, 'feature_columns', None)
        
        self.model = DualGAFNet(
            feature_dim=feature_dim,
            num_classes=num_classes,
            num_images=num_images,
            time_length=time_length,
            extractor_type=extractor_type,
            fusion_type=fusion_type,
            attention_type=attention_type,
            classifier_type=classifier_type,
            use_statistical_features=use_statistical_features,
            stat_type=stat_type,
            multimodal_fusion_strategy=multimodal_fusion_strategy,
            use_diff_branch=use_diff_branch,
            hvac_groups=hvac_groups,
            feature_columns=feature_columns
        )

    def forward(self, sum_x, diff_x, time_series_x=None):
        return self.model(sum_x, diff_x, time_series_x)


if __name__ == "__main__":
    print("="*100)
    print("æµ‹è¯•åŒè·¯GAFç½‘ç»œ - é’ˆå¯¹96x96 GAFå›¾åƒä¼˜åŒ–")
    print("="*100)
    
    # æµ‹è¯•ä¸åŒä¿¡å·æ•°é‡å’Œç‰¹å¾æå–å™¨
    test_configs = [
        (26, 96, "resnet18", "SAHUæ•°æ®é›†"),
        (120, 96, "auto", "DAHUæ•°æ®é›†ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰"),
        (120, 96, "resnet_light", "DAHUæ•°æ®é›†ï¼ˆæ‰‹åŠ¨è½»é‡çº§ï¼‰"),
        (120, 96, "optimized_large_kernel", "DAHUæ•°æ®é›†ï¼ˆä¼˜åŒ–å¤§æ ¸ï¼‰"),
    ]
    
    for signal_count, gaf_size, extractor_type, dataset_name in test_configs:
        print(f"\n{'-'*60}")
        print(f"æµ‹è¯•é…ç½®: {dataset_name}")
        print(f"ä¿¡å·æ•°é‡: {signal_count}, GAFå°ºå¯¸: {gaf_size}x{gaf_size}")
        print(f"ç‰¹å¾æå–å™¨: {extractor_type}")
        print(f"{'-'*60}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        B = 2  # å°æ‰¹æ¬¡æµ‹è¯•
        C, H, W = signal_count, gaf_size, gaf_size
        sum_x = torch.randn(B, C, H, W)
        diff_x = torch.randn(B, C, H, W)
        time_series_x = torch.randn(B, gaf_size, C)  # æ—¶åºæ•°æ®
        
        # åˆ›å»ºé…ç½®
        configs = type("cfg", (), {
            "feature_dim": 64,  # ä½¿ç”¨è¾ƒå°çš„ç‰¹å¾ç»´åº¦è¿›è¡Œæµ‹è¯•
            "num_class": 6,     # DAHUæ•°æ®é›†æœ‰æ›´å¤šç±»åˆ«
            "enc_in": C,
            "seq_len": gaf_size,
            "extractor_type": extractor_type,
            "fusion_type": "adaptive", 
            "attention_type": "channel",
            "classifier_type": "mlp",
            "use_statistical_features": True,
            "stat_type": "basic",  # ä½¿ç”¨åŸºç¡€ç»Ÿè®¡ç‰¹å¾å‡å°‘è®¡ç®—
            "multimodal_fusion_strategy": "concat",
            "hvac_groups": None,
            "feature_columns": None
        })()
        
        try:
            model = Model(configs)
            
            print(f"\nè¾“å…¥æ•°æ®:")
            print(f"  - Summation GAF: {sum_x.shape}")
            print(f"  - Difference GAF: {diff_x.shape}")
            print(f"  - Time Series: {time_series_x.shape}")
            
            # æµ‹è¯•forwardè¿‡ç¨‹
            out = model(sum_x, diff_x, time_series_x)
            print(f"\næ¨¡å‹è¾“å‡º:")
            print(f"  - è¾“å‡º shape: {out.shape}")
            print(f"  - è¾“å‡ºèŒƒå›´: [{out.min().item():.4f}, {out.max().item():.4f}]")
            
            # è®¡ç®—å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"\næ¨¡å‹å¤æ‚åº¦:")
            print(f"  - æ€»å‚æ•°æ•°é‡: {total_params:,}")
            print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"  - æ¨¡å‹å¤§å°ä¼°è®¡: {total_params * 4 / (1024**2):.2f} MB")
            
            # è®¡ç®—å†…å­˜ä¼°è®¡
            input_memory = (sum_x.numel() + diff_x.numel() + time_series_x.numel()) * 4 / (1024**2)
            print(f"  - å•batchè¾“å…¥å†…å­˜: {input_memory:.2f} MB")
            
            print(f"âœ… æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
    
    print(f"\n{'='*100}")
    print("æµ‹è¯•å®Œæˆ")
    print("="*100) 