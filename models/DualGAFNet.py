import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from models.MultiImageFeatureNet import (
    NoPaddingLargeKernelFeatureExtractor,
    InceptionFeatureExtractor,
    LargeKernelDilatedFeatureExtractor,
    MultiScaleStackedFeatureExtractor
)
from models.ChannelCompressionModule import (
    SignalCompressionModule,
    ChannelCompressionModule,
    AdaptiveChannelCompressionModule,
    HVACSignalGroupCompressionModule
)


# ===== å‚æ•°é«˜æ•ˆåˆ†ç±»å™¨ç»„ä»¶ =====

class EfficientClassifier(nn.Module):
    """å‚æ•°é«˜æ•ˆçš„åˆ†ç±»å™¨ï¼Œé€šè¿‡é™ç»´æŠ€æœ¯å‡å°‘å‚æ•°é‡"""
    
    def __init__(self, input_dim, num_classes, reduction_method='pooling_projection', 
                 intermediate_dim=512, classifier_type='mlp'):
        super().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.reduction_method = reduction_method
        self.intermediate_dim = intermediate_dim
        
        if reduction_method == 'pooling_projection':
            # æ–¹æ³•1: å…ˆåšçº¿æ€§æŠ•å½±é™ç»´ï¼Œå†åˆ†ç±»
            self.reducer = nn.Sequential(
                nn.Linear(input_dim, intermediate_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            )
            final_dim = intermediate_dim
            
        elif reduction_method == 'attention_pooling':
            # æ–¹æ³•2: æ³¨æ„åŠ›æœºåˆ¶é™ç»´
            self.attention = nn.Sequential(
                nn.Linear(input_dim, intermediate_dim),
                nn.Tanh(),
                nn.Linear(intermediate_dim, 1),
                nn.Softmax(dim=1)
            )
            self.projection = nn.Linear(input_dim, intermediate_dim)
            final_dim = intermediate_dim
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {reduction_method}")
        
        # æ„å»ºåˆ†ç±»å™¨
        if classifier_type == 'mlp':
            self.classifier = nn.Sequential(
                nn.Linear(final_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Linear(128, num_classes)
            )
        else:  # simple
            self.classifier = nn.Sequential(
                nn.Linear(final_dim, 128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, num_classes)
            )
    
    def forward(self, x):
        # x: [batch_size, input_dim]
        if self.reduction_method == 'pooling_projection':
            x = self.reducer(x)
        elif self.reduction_method == 'attention_pooling':
            # æ³¨æ„åŠ›åŠ æƒå¹³å‡
            attention_weights = self.attention(x)  # [batch_size, 1]
            x = self.projection(x) * attention_weights  # åŠ æƒæŠ•å½±
        
        return self.classifier(x)


class GlobalPoolingClassifier(nn.Module):
    """å…¨å±€æ± åŒ–åˆ†ç±»å™¨ï¼Œæœ€å‚æ•°é«˜æ•ˆçš„é€‰æ‹©"""
    
    def __init__(self, input_feature_dim, num_signals, num_classes, pooling_type='adaptive'):
        super().__init__()
        self.input_feature_dim = input_feature_dim
        self.num_signals = num_signals
        self.pooling_type = pooling_type
        
        if pooling_type == 'mean':
            # ç®€å•å¹³å‡æ± åŒ–
            self.pooling = lambda x: torch.mean(x, dim=1)
            final_dim = input_feature_dim
            
        elif pooling_type == 'max':
            # æœ€å¤§æ± åŒ–
            self.pooling = lambda x: torch.max(x, dim=1)[0]
            final_dim = input_feature_dim
            
        elif pooling_type == 'attention':
            # æ³¨æ„åŠ›æ± åŒ–
            self.attention = nn.Sequential(
                nn.Linear(input_feature_dim, input_feature_dim // 4),
                nn.ReLU(),
                nn.Linear(input_feature_dim // 4, 1),
                nn.Softmax(dim=1)
            )
            final_dim = input_feature_dim
            
        elif pooling_type == 'adaptive':
            # è‡ªé€‚åº”æ± åŒ–ï¼ˆç»“åˆå¤šç§æ± åŒ–æ–¹å¼ï¼‰
            self.attention = nn.Sequential(
                nn.Linear(input_feature_dim, input_feature_dim // 4),
                nn.ReLU(),
                nn.Linear(input_feature_dim // 4, 1),
                nn.Softmax(dim=1)
            )
            # èåˆä¸åŒæ± åŒ–ç»“æœ
            final_dim = input_feature_dim * 3  # mean + max + attention
            
        # è½»é‡çº§åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(final_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # x: [batch_size, num_signals * feature_dim] -> [batch_size, num_signals, feature_dim]
        x = x.view(x.size(0), self.num_signals, self.input_feature_dim)
        
        if self.pooling_type == 'mean':
            pooled = torch.mean(x, dim=1)  # [batch_size, feature_dim]
        elif self.pooling_type == 'max':
            pooled = torch.max(x, dim=1)[0]  # [batch_size, feature_dim]
        elif self.pooling_type == 'attention':
            attention_weights = self.attention(x)  # [batch_size, num_signals, 1]
            pooled = torch.sum(x * attention_weights, dim=1)  # [batch_size, feature_dim]
        elif self.pooling_type == 'adaptive':
            # ç»“åˆå¤šç§æ± åŒ–
            mean_pooled = torch.mean(x, dim=1)
            max_pooled = torch.max(x, dim=1)[0]
            attention_weights = self.attention(x)
            att_pooled = torch.sum(x * attention_weights, dim=1)
            pooled = torch.cat([mean_pooled, max_pooled, att_pooled], dim=1)
        
        return self.classifier(pooled)


class Conv1DClassifier(nn.Module):
    """1Då·ç§¯åˆ†ç±»å™¨ï¼Œå°†ç‰¹å¾åºåˆ—å½“ä½œ1Dä¿¡å·å¤„ç†"""
    
    def __init__(self, input_feature_dim, num_signals, num_classes, hidden_channels=[256, 128, 64]):
        super().__init__()
        self.input_feature_dim = input_feature_dim
        self.num_signals = num_signals
        
        # 1Då·ç§¯ç½‘ç»œ
        layers = []
        in_channels = input_feature_dim
        
        for hidden_dim in hidden_channels:
            layers.extend([
                nn.Conv1d(in_channels, hidden_dim, kernel_size=3, padding=1),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            in_channels = hidden_dim
        
        # å…¨å±€æ± åŒ–
        layers.append(nn.AdaptiveAvgPool1d(1))
        
        self.conv_layers = nn.Sequential(*layers)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_channels[-1], 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        # x: [batch_size, num_signals * feature_dim] -> [batch_size, feature_dim, num_signals]
        batch_size = x.size(0)
        x = x.view(batch_size, self.num_signals, self.input_feature_dim)
        x = x.transpose(1, 2)  # [batch_size, feature_dim, num_signals]
        
        # 1Då·ç§¯å¤„ç†
        x = self.conv_layers(x)  # [batch_size, hidden_channels[-1], 1]
        x = x.squeeze(-1)  # [batch_size, hidden_channels[-1]]
        
        return self.classifier(x)


class SeparableClassifier(nn.Module):
    """åˆ†ç¦»å¼åˆ†ç±»å™¨ï¼Œå…ˆå¯¹æ¯ä¸ªä¿¡å·ç‹¬ç«‹åˆ†ç±»ï¼Œå†èåˆç»“æœ"""
    
    def __init__(self, input_feature_dim, num_signals, num_classes, fusion_method='attention'):
        super().__init__()
        self.input_feature_dim = input_feature_dim
        self.num_signals = num_signals
        self.fusion_method = fusion_method
        
        # æ¯ä¸ªä¿¡å·çš„ç‹¬ç«‹åˆ†ç±»å™¨ï¼ˆå‚æ•°å…±äº«ï¼‰
        self.signal_classifier = nn.Sequential(
            nn.Linear(input_feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )
        
        if fusion_method == 'attention':
            # æ³¨æ„åŠ›èåˆ
            self.attention = nn.Sequential(
                nn.Linear(num_classes, 64),
                nn.ReLU(),
                nn.Linear(64, 1),
                nn.Softmax(dim=1)
            )
        elif fusion_method == 'weighted':
            # å¯å­¦ä¹ æƒé‡èåˆ
            self.weights = nn.Parameter(torch.ones(num_signals) / num_signals)
    
    def forward(self, x):
        # x: [batch_size, num_signals * feature_dim] -> [batch_size, num_signals, feature_dim]
        batch_size = x.size(0)
        x = x.view(batch_size, self.num_signals, self.input_feature_dim)
        
        # å¯¹æ¯ä¸ªä¿¡å·ç‹¬ç«‹åˆ†ç±»
        signal_outputs = []
        for i in range(self.num_signals):
            signal_out = self.signal_classifier(x[:, i, :])  # [batch_size, num_classes]
            signal_outputs.append(signal_out)
        
        # å †å æ‰€æœ‰ä¿¡å·çš„è¾“å‡º
        signal_outputs = torch.stack(signal_outputs, dim=1)  # [batch_size, num_signals, num_classes]
        
        # èåˆç­–ç•¥
        if self.fusion_method == 'mean':
            output = torch.mean(signal_outputs, dim=1)
        elif self.fusion_method == 'max':
            output = torch.max(signal_outputs, dim=1)[0]
        elif self.fusion_method == 'attention':
            attention_weights = self.attention(signal_outputs)  # [batch_size, num_signals, 1]
            output = torch.sum(signal_outputs * attention_weights, dim=1)
        elif self.fusion_method == 'weighted':
            weights = F.softmax(self.weights, dim=0)  # å½’ä¸€åŒ–æƒé‡
            output = torch.sum(signal_outputs * weights.view(1, -1, 1), dim=1)
        
        return output


class FeatureCompressionClassifier(nn.Module):
    """
    ç‰¹å¾ç»´åº¦å‹ç¼©åˆ†ç±»å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
    
    åªå‹ç¼©ç‰¹å¾ç»´åº¦ï¼Œä¸å‹ç¼©ä¿¡å·ç»´åº¦ï¼Œä¿æŒè¾ƒå¥½çš„æ€§èƒ½å¹³è¡¡
    - ç‰¹å¾å‹ç¼©ï¼š128ç»´ â†’ 32ç»´ 
    - ä¿æŒä¿¡å·æ•°é‡ä¸å˜
    - ç®€å•çš„MLPåˆ†ç±»å™¨
    """
    
    def __init__(self, input_feature_dim, num_signals, num_classes, 
                 compressed_feature_dim=32):
        super().__init__()
        self.input_feature_dim = input_feature_dim
        self.num_signals = num_signals
        self.num_classes = num_classes
        
        # è‡ªé€‚åº”è®¡ç®—å‹ç¼©ç‰¹å¾ç»´åº¦
        if compressed_feature_dim is None:
            if input_feature_dim >= 128:
                self.compressed_feature_dim = 32
            elif input_feature_dim >= 64:
                self.compressed_feature_dim = 16
            else:
                self.compressed_feature_dim = max(8, input_feature_dim // 4)
        else:
            self.compressed_feature_dim = compressed_feature_dim
        
        print(f"ğŸ¯ æ„å»ºç‰¹å¾å‹ç¼©åˆ†ç±»å™¨: ç‰¹å¾{input_feature_dim}â†’{self.compressed_feature_dim}, ä¿¡å·{num_signals}(ä¿æŒä¸å˜) -> {num_classes}")
        print(f"   ğŸ”¸ ç‰¹å¾å‹ç¼©æ¯”: {self.compressed_feature_dim/input_feature_dim:.3f}")
        print(f"   ğŸ”¸ åŸå§‹ç»´åº¦: {num_signals * input_feature_dim} â†’ å‹ç¼©ç»´åº¦: {num_signals * self.compressed_feature_dim}")
        
        # ç‰¹å¾å‹ç¼©å±‚ï¼šæ¯ä¸ªä¿¡å·ç‹¬ç«‹å‹ç¼©ç‰¹å¾ç»´åº¦
        self.feature_compress = nn.Sequential(
            nn.Linear(input_feature_dim, self.compressed_feature_dim),
            nn.BatchNorm1d(num_signals),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        # ç®€å•çš„MLPåˆ†ç±»å™¨
        final_dim = num_signals * self.compressed_feature_dim
        hidden_dim = min(final_dim // 2, 512)  # é€‚ä¸­çš„éšè—å±‚å¤§å°
        
        self.classifier = nn.Sequential(
            nn.Linear(final_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # è®¡ç®—å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   ğŸ”¸ æ€»å‚æ•°é‡: {total_params:,} ({total_params/1000:.1f}K)")
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        ç‰¹å¾å‹ç¼©å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, num_signals * input_feature_dim] - ä»DualGAFNetæ¥çš„æ‰å¹³åŒ–ç‰¹å¾
        Returns:
            output: [batch_size, num_classes]
        """
        batch_size = x.size(0)
        
        # é‡å¡‘è¾“å…¥: [B, num_signals * feature_dim] â†’ [B, num_signals, feature_dim]
        x = x.view(batch_size, self.num_signals, self.input_feature_dim)
        
        # ç‰¹å¾å‹ç¼©: [B, num_signals, input_feature_dim] â†’ [B, num_signals, compressed_feature_dim]
        compressed_features = self.feature_compress(x)  # [B, num_signals, compressed_feature_dim]
        
        # å±•å¹³: [B, num_signals, compressed_feature_dim] â†’ [B, num_signals * compressed_feature_dim]
        flattened = compressed_features.reshape(batch_size, -1)
        
        # åˆ†ç±»
        output = self.classifier(flattened)  # [B, num_classes]
        
        return output
    
    def get_compression_stats(self):
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        original_dim = self.num_signals * self.input_feature_dim
        compressed_dim = self.num_signals * self.compressed_feature_dim
        compression_ratio = compressed_dim / original_dim
        
        return {
            'feature_compression_ratio': self.compressed_feature_dim / self.input_feature_dim,
            'overall_compression': compression_ratio,
            'original_dim': original_dim,
            'compressed_dim': compressed_dim,
            'param_reduction': 1 - compression_ratio  # å‚æ•°å‡å°‘æ¯”ä¾‹çš„è¿‘ä¼¼
        }


class HierarchicalCompressionClassifier(nn.Module):
    """åˆ†å±‚å‹ç¼©åˆ†ç±»å™¨ï¼šå…ˆå‹ç¼©ç‰¹å¾ç»´åº¦ï¼Œå†å‹ç¼©ä¿¡å·ç»´åº¦"""
    
    def __init__(self, input_feature_dim, num_signals, num_classes, 
                 compressed_feature_dim=32, compressed_signals=None, 
                 intermediate_dim=256, compression_ratio=0.6):
        super().__init__()
        self.input_feature_dim = input_feature_dim
        self.num_signals = num_signals
        self.compressed_feature_dim = compressed_feature_dim
        
        # è‡ªåŠ¨è®¡ç®—å‹ç¼©åçš„ä¿¡å·æ•°é‡
        if compressed_signals is None:
            self.compressed_signals = max(16, int(num_signals * compression_ratio))
        else:
            self.compressed_signals = compressed_signals
        
        print(f"ğŸ—ï¸ åˆ†å±‚å‹ç¼©é…ç½®:")
        print(f"   ç‰¹å¾ç»´åº¦å‹ç¼©: {input_feature_dim} â†’ {compressed_feature_dim}")
        print(f"   ä¿¡å·ç»´åº¦å‹ç¼©: {num_signals} â†’ {self.compressed_signals}")
        print(f"   å‹ç¼©æ¯”ä¾‹: ç‰¹å¾{compressed_feature_dim/input_feature_dim:.2f}, ä¿¡å·{self.compressed_signals/num_signals:.2f}")
        
        # é˜¶æ®µ1: å•ä¿¡å·ç‰¹å¾å‹ç¼© (ç‰¹å¾é™ç»´)
        # æ¯ä¸ªä¿¡å·ç‹¬ç«‹è¿›è¡Œç‰¹å¾å‹ç¼©: input_feature_dim â†’ compressed_feature_dim
        self.signal_feature_compress = nn.Sequential(
            nn.Linear(input_feature_dim, compressed_feature_dim * 2),
            nn.BatchNorm1d(num_signals),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            
            nn.Linear(compressed_feature_dim * 2, compressed_feature_dim),
            nn.BatchNorm1d(num_signals),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        # é˜¶æ®µ2: ä¿¡å·é‡è¦æ€§æ³¨æ„åŠ›æœºåˆ¶ (ç±»ä¼¼ChannelAttention)
        # ç›®æ ‡ï¼šä¸ºæ¯ä¸ªä¿¡å·è®¡ç®—ä¸€ä¸ªé‡è¦æ€§åˆ†æ•°
        # è¾“å…¥ï¼š[B, num_signals, compressed_feature_dim] 
        # è¾“å‡ºï¼š[B, num_signals, 1]
        
        # å…¨å±€æ± åŒ–ï¼šè·å–æ¯ä¸ªä¿¡å·çš„å…¨å±€ç‰¹å¾è¡¨ç¤º
        self.signal_avg_pool = nn.AdaptiveAvgPool1d(1)  # å…¨å±€å¹³å‡æ± åŒ–
        self.signal_max_pool = nn.AdaptiveMaxPool1d(1)  # å…¨å±€æœ€å¤§æ± åŒ–
        
        # MLPï¼šå­¦ä¹ ä¿¡å·é—´çš„å…³ç³»ï¼ˆreductionç­–ç•¥ï¼‰
        # è¾“å…¥/è¾“å‡ºéƒ½æ˜¯num_signalsç»´åº¦ï¼Œä¸­é—´å±‚è¿›è¡Œå‹ç¼©
        reduction = max(1, num_signals // 8)  # è‡³å°‘ä¸º1ï¼Œé€šå¸¸å‹ç¼©8å€
        self.signal_fc = nn.Sequential(
            nn.Linear(num_signals, reduction),    # [B, num_signals] â†’ [B, reduction]
            nn.ReLU(inplace=True),
            nn.Linear(reduction, num_signals)     # [B, reduction] â†’ [B, num_signals]
        )
        
        self.signal_sigmoid = nn.Sigmoid()
        
        # é˜¶æ®µ3: ä¿¡å·èåˆ (è·¨ä¿¡å·çš„ä¿¡æ¯èåˆ)
        # num_signals â†’ compressed_signals
        self.signal_fusion = nn.Sequential(
            nn.Linear(num_signals, self.compressed_signals * 2),
            nn.BatchNorm1d(compressed_feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.15),
            
            nn.Linear(self.compressed_signals * 2, self.compressed_signals),
            nn.BatchNorm1d(compressed_feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.15)
        )
        
        # é˜¶æ®µ4: çŸ©é˜µåˆ†è§£çš„åˆ†ç±»å¤´
        final_dim = self.compressed_signals * compressed_feature_dim
        
        # çŸ©é˜µåˆ†è§£: é¿å…ç›´æ¥å¤§çŸ©é˜µä¹˜æ³•
        # åŸå§‹: final_dim â†’ large_hidden â†’ num_classes éœ€è¦å¤§é‡å‚æ•°
        # åˆ†è§£: final_dim â†’ intermediate_dim â†’ intermediate_dim*2 â†’ num_classes å‚æ•°æ›´å°‘
        self.classifier_decomp = nn.Sequential(
            # ç¬¬ä¸€å±‚åˆ†è§£ - ä¸»è¦é™ç»´
            nn.Linear(final_dim, intermediate_dim),
            nn.BatchNorm1d(intermediate_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            # ç¬¬äºŒå±‚åˆ†è§£ - ç‰¹å¾é‡ç»„
            nn.Linear(intermediate_dim, intermediate_dim * 2),
            nn.BatchNorm1d(intermediate_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            # æœ€ç»ˆåˆ†ç±»å±‚
            nn.Linear(intermediate_dim * 2, num_classes)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        åˆ†å±‚å‹ç¼©å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, num_signals * input_feature_dim] - ä»DualGAFNetæ¥çš„æ‰å¹³åŒ–ç‰¹å¾
        Returns:
            output: [batch_size, num_classes]
        """
        batch_size = x.size(0)
        
        # é‡å¡‘è¾“å…¥: [B, num_signals * feature_dim] â†’ [B, num_signals, feature_dim]
        x = x.view(batch_size, self.num_signals, self.input_feature_dim)
        
        # é˜¶æ®µ1: å•ä¿¡å·ç‰¹å¾å‹ç¼©
        # æ¯ä¸ªä¿¡å·ç‹¬ç«‹å‹ç¼©: [B, num_signals, input_feature_dim] â†’ [B, num_signals, compressed_feature_dim]
        signal_compressed = self.signal_feature_compress(x)  # [B, num_signals, compressed_feature_dim]
        
        # é˜¶æ®µ2: ä¿¡å·é‡è¦æ€§æ³¨æ„åŠ› (ç±»ä¼¼ChannelAttentionçš„æ–¹å¼)
        # signal_compressed: [B, num_signals, compressed_feature_dim]
        B, num_signals, compressed_feature_dim = signal_compressed.shape
        
        # å¯¹æ¯ä¸ªä¿¡å·çš„ç‰¹å¾ç»´åº¦è¿›è¡Œå…¨å±€æ± åŒ–ï¼Œè·å–æ¯ä¸ªä¿¡å·çš„å…¨å±€è¡¨ç¤º
        # [B, num_signals, compressed_feature_dim] â†’ [B, num_signals, 1] â†’ [B, num_signals]
        avg_signal_repr = self.signal_avg_pool(signal_compressed).squeeze(-1)  # [B, num_signals]
        max_signal_repr = self.signal_max_pool(signal_compressed).squeeze(-1)  # [B, num_signals]
        
        # é€šè¿‡MLPå­¦ä¹ ä¿¡å·é—´çš„å…³ç³»
        # [B, num_signals] â†’ [B, num_signals]
        avg_attention = self.signal_fc(avg_signal_repr)  # [B, num_signals]
        max_attention = self.signal_fc(max_signal_repr)  # [B, num_signals]
        
        # èåˆå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–çš„æ³¨æ„åŠ›ï¼Œå¾—åˆ°æ¯ä¸ªä¿¡å·çš„é‡è¦æ€§åˆ†æ•°
        signal_importance = self.signal_sigmoid(avg_attention + max_attention)  # [B, num_signals]
        signal_importance = signal_importance.unsqueeze(-1)  # [B, num_signals, 1]
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡ï¼šé€å…ƒç´ ç›¸ä¹˜
        weighted_signals = signal_compressed * signal_importance  # [B, num_signals, compressed_feature_dim]
        
        # é˜¶æ®µ3: ä¿¡å·èåˆ
        # è·¨ä¿¡å·ç»´åº¦çš„ä¿¡æ¯èåˆ: [B, num_signals, compressed_feature_dim] â†’ [B, compressed_signals, compressed_feature_dim]
        # è½¬ç½®åèåˆ: [B, compressed_feature_dim, num_signals] â†’ [B, compressed_feature_dim, compressed_signals] â†’ [B, compressed_signals, compressed_feature_dim]
        signals_transposed = weighted_signals.transpose(1, 2)  # [B, compressed_feature_dim, num_signals]
        fused_signals = self.signal_fusion(signals_transposed)  # [B, compressed_feature_dim, compressed_signals]
        fused_signals = fused_signals.transpose(1, 2)  # [B, compressed_signals, compressed_feature_dim]
        
        # é˜¶æ®µ4: åˆ†ç±»
        # å±•å¹³å¹¶é€šè¿‡åˆ†è§£çš„åˆ†ç±»å™¨ï¼ˆä½¿ç”¨reshapeå¤„ç†éè¿ç»­å†…å­˜ï¼‰
        flattened = fused_signals.reshape(batch_size, -1)  # [B, compressed_signals * compressed_feature_dim]
        output = self.classifier_decomp(flattened)  # [B, num_classes]
        
        return output
    
    def get_signal_importance(self, x):
        """
        è·å–ä¿¡å·é‡è¦æ€§åˆ†æ•°ï¼Œç”¨äºåˆ†æå“ªäº›ä¿¡å·æ›´é‡è¦
        
        Args:
            x: [batch_size, num_signals * input_feature_dim] - åŸå§‹è¾“å…¥
        Returns:
            importance: [batch_size, num_signals] - å„ä¿¡å·çš„é‡è¦æ€§åˆ†æ•°
        """
        batch_size = x.size(0)
        x = x.view(batch_size, self.num_signals, self.input_feature_dim)
        
        with torch.no_grad():
            signal_compressed = self.signal_feature_compress(x)  # [B, num_signals, compressed_feature_dim]
            
            # ä½¿ç”¨æ–°çš„ChannelAttentioné£æ ¼è®¡ç®—ä¿¡å·é‡è¦æ€§
            avg_signal_repr = self.signal_avg_pool(signal_compressed).squeeze(-1)  # [B, num_signals]
            max_signal_repr = self.signal_max_pool(signal_compressed).squeeze(-1)  # [B, num_signals]
            
            avg_attention = self.signal_fc(avg_signal_repr)  # [B, num_signals]
            max_attention = self.signal_fc(max_signal_repr)  # [B, num_signals]
            
            signal_importance = self.signal_sigmoid(avg_attention + max_attention)  # [B, num_signals]
            return signal_importance  # [B, num_signals]
    
    def get_compression_stats(self):
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        original_params = self.num_signals * self.input_feature_dim * 1024  # å‡è®¾åŸå§‹åˆ†ç±»å™¨ç¬¬ä¸€å±‚1024
        current_params = sum(p.numel() for p in self.parameters())
        
        feature_compression_ratio = self.compressed_feature_dim / self.input_feature_dim
        signal_compression_ratio = self.compressed_signals / self.num_signals
        overall_compression = feature_compression_ratio * signal_compression_ratio
        
        return {
            'feature_compression_ratio': feature_compression_ratio,
            'signal_compression_ratio': signal_compression_ratio,
            'overall_compression': overall_compression,
            'param_reduction': 1 - (current_params / original_params),
            'original_dim': self.num_signals * self.input_feature_dim,
            'compressed_dim': self.compressed_signals * self.compressed_feature_dim
        }


# ===== åŸæœ‰ç»„ä»¶ =====

class BasicBlock(nn.Module):
    """ResNetåŸºç¡€å— - ä»MultiImageFeatureNetç§»æ¤å¹¶ä¼˜åŒ–"""

    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels,
                               3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample is not None:
            identity = self.downsample(x)
        # out += identity
        out = self.relu(out)
        return out
class FiLMFusion(nn.Module):
    def __init__(self, feature_dim):
        super().__init__()
        self.feature_dim = feature_dim
        # å°†ç»Ÿè®¡ç‰¹å¾æ˜ å°„ä¸º Î³ å’Œ Î²
        self.gamma_net = nn.Linear(feature_dim, feature_dim)
        self.beta_net = nn.Linear(feature_dim, feature_dim)

        # åˆå§‹åŒ–ï¼šè®©åˆå§‹çŠ¶æ€æ¥è¿‘æ’ç­‰å˜æ¢
        nn.init.zeros_(self.gamma_net.bias)
        nn.init.constant_(self.beta_net.bias, 0.0)

    def forward(self, gaf_features, stat_features):
        B, C, D = gaf_features.shape

        # âœ… å¼ºçƒˆå»ºè®®ï¼šå¯¹ç»Ÿè®¡ç‰¹å¾å½’ä¸€åŒ–
        stat_normalized = F.normalize(stat_features, p=2, dim=1)  # [B, D]

        # ç”Ÿæˆ Î³ å’Œ Î²
        gamma = 1 + self.gamma_net(stat_normalized)  # åˆå§‹ä¸º 1
        beta = self.beta_net(stat_normalized)        # åˆå§‹ä¸º 0

        # æ‰©å±•åˆ°æ¯ä¸ªé€šé“
        gamma = gamma.unsqueeze(1)  # [B, 1, D]
        beta = beta.unsqueeze(1)    # [B, 1, D]

        # âœ… gaf_features æ˜¯å¦å½’ä¸€åŒ–ï¼Ÿå¯é€‰
        # æ–¹æ¡ˆAï¼šä¿ç•™åŸå§‹å¼ºåº¦
        fused_features = gamma * gaf_features + beta

        # æ–¹æ¡ˆBï¼šå…ˆå½’ä¸€åŒ–ï¼ˆæ›´ç¨³å®šï¼‰
        # gaf_norm = F.normalize(gaf_features, p=2, dim=-1)
        # fused_features = gamma * gaf_norm + beta

        return fused_features

class GAFOptimizedResNet(nn.Module):
    """ä¸“é—¨ä¸ºGAFå›¾åƒä¼˜åŒ–çš„ResNet - æ¸è¿›å¼ä¸‹é‡‡æ ·ä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯"""

    def __init__(self, feature_dim=128, depth='resnet18'):
        super().__init__()
        self.depth = depth

        if depth == 'resnet18_gaf':
            # GAFä¼˜åŒ–ç‰ˆæœ¬ï¼šæ¸è¿›å¼ä¸‹é‡‡æ ·ï¼Œæ›´å¥½ä¿ç•™ç©ºé—´ä¿¡æ¯
            # ç¬¬ä¸€å±‚ï¼šè½»å¾®ä¸‹é‡‡æ ·ï¼Œä¿ç•™æ›´å¤šç»†èŠ‚
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 96->96ï¼Œä¿æŒå°ºå¯¸
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True)
            )

            # æ¸è¿›å¼ä¸‹é‡‡æ ·çš„æ®‹å·®å±‚
            self.layer1 = self._make_layer(32, 64, 2, stride=2)    # 96->48
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 2, stride=2)  # 12->6
            final_channels = 512

        elif depth == 'resnet18_gaf_light':
            # è½»é‡çº§GAFç‰ˆæœ¬ï¼šé€‚åˆå¤§æ•°æ®é›†
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 16, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 96->96
                nn.BatchNorm2d(16),
                nn.ReLU(inplace=True)
            )

            self.layer1 = self._make_layer(16, 32, 2, stride=2)    # 96->48
            self.layer2 = self._make_layer(32, 64, 2, stride=2)    # 48->24
            self.layer3 = self._make_layer(64, 128, 2, stride=2)   # 24->12
            self.layer4 = self._make_layer(128, 256, 2, stride=2)  # 12->6
            final_channels = 256

        elif depth == 'resnet_gaf_deep':
            # æ·±åº¦GAFç‰ˆæœ¬ï¼šæ›´å¤šå±‚ä½†ä¿æŒæ¸è¿›ä¸‹é‡‡æ ·
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 96->96
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True),
                nn.Conv2d(32, 32, kernel_size=3, stride=1,
                          padding=1, bias=False),  # é¢å¤–çš„3x3å·ç§¯
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True)
            )

            self.layer1 = self._make_layer(32, 64, 3, stride=2)    # 96->48
            self.layer2 = self._make_layer(64, 128, 4, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 6, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 3, stride=2)  # 12->6
            final_channels = 512

        elif depth == 'resnet_gaf_preserve':
            # é«˜ä¿çœŸç‰ˆæœ¬ï¼šæœ€å¤§ç¨‹åº¦ä¿ç•™ç©ºé—´ä¿¡æ¯
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 96->96
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )

            self.layer1 = self._make_layer(
                64, 64, 2, stride=1)    # 96->96ï¼Œä¸ä¸‹é‡‡æ ·
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 96->48
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 48->24
            self.layer4 = self._make_layer(256, 512, 2, stride=2)  # 24->12
            final_channels = 512

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„GAF ResNetæ·±åº¦: {depth}")

        # è‡ªé€‚åº”å¹³å‡æ± åŒ–å’Œå…¨è¿æ¥å±‚
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(final_channels, feature_dim)

        print(f"GAFä¼˜åŒ–ResNet ({depth}) æ„å»ºå®Œæˆ:")
        print(f"  - ç¬¬ä¸€å±‚ä¿æŒç©ºé—´åˆ†è¾¨ç‡ï¼š96x96")
        print(f"  - æ¸è¿›å¼ä¸‹é‡‡æ ·ç­–ç•¥")
        print(f"  - æœ€ç»ˆé€šé“æ•°ï¼š{final_channels}")

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """æ„å»ºResNetå±‚"""
        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
        layers.append(BasicBlock(in_channels, out_channels, stride))
        # åç»­å—ä¸éœ€è¦ä¸‹é‡‡æ ·
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        # x: [N, 1, 96, 96]
        x = self.conv1(x)       # [N, channels, 96, 96] - ä¿æŒç©ºé—´åˆ†è¾¨ç‡ï¼
        x = self.layer1(x)      # æ ¹æ®é…ç½®è¿›è¡Œä¸‹é‡‡æ ·
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)     # [N, final_channels, 1, 1]
        x = torch.flatten(x, 1)  # [N, final_channels]
        x = self.fc(x)          # [N, feature_dim]
        return x


class ResNetFeatureExtractor(nn.Module):
    """ResNetç‰¹å¾æå–å™¨ - ä¸“é—¨ä¸º96x96 GAFå›¾åƒä¼˜åŒ–"""

    def __init__(self, feature_dim=128, depth='resnet18'):
        super().__init__()
        self.depth = depth

        # æ–°çš„GAFä¼˜åŒ–æ¶æ„
        if depth in ['resnet18_gaf', 'resnet18_gaf_light', 'resnet_gaf_deep', 'resnet_gaf_preserve']:
            self.resnet = GAFOptimizedResNet(feature_dim, depth)

        # ä¿ç•™åŸæœ‰æ¶æ„ä»¥ä¿æŒå…¼å®¹æ€§
        elif depth == 'resnet18':
            # æ”¹è¿›çš„ResNet18ï¼šå‡å°‘åˆå§‹ä¸‹é‡‡æ ·
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2,
                          bias=False),  # 96->48ï¼Œä½¿ç”¨5x5è€Œä¸æ˜¯7x7
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
                # å»æ‰MaxPoolï¼Œå‡å°‘ä¸‹é‡‡æ ·
            )

            self.layer1 = self._make_layer(
                64, 64, 2, stride=1)    # 48->48ï¼Œä¸ä¸‹é‡‡æ ·
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 2, stride=2)  # 12->6

            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512, feature_dim)

        elif depth == 'resnet34':
            # æ”¹è¿›çš„ResNet34
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=5, stride=2,
                          padding=2, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )

            self.layer1 = self._make_layer(64, 64, 3, stride=1)    # 48->48
            self.layer2 = self._make_layer(64, 128, 4, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 6, stride=2)  # 24->12
            self.layer4 = self._make_layer(256, 512, 3, stride=2)  # 12->6

            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512, feature_dim)

        elif depth == 'resnet_light':
            # è½»é‡çº§ç‰ˆæœ¬ï¼Œé€‚åˆDAHUæ•°æ®é›†çš„120ä¸ªä¿¡å·
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 96->96ï¼Œä¸ä¸‹é‡‡æ ·
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True)
            )

            self.layer1 = self._make_layer(32, 64, 2, stride=2)    # 96->48
            self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 48->24
            self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 24->12
            self.layer4 = None  # å»æ‰æœ€åä¸€å±‚å‡å°‘å‚æ•°

            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(256, feature_dim)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ResNetæ·±åº¦: {depth}")

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """æ„å»ºResNetå±‚"""
        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
        layers.append(BasicBlock(in_channels, out_channels, stride))
        # åç»­å—ä¸éœ€è¦ä¸‹é‡‡æ ·
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        # å¦‚æœæ˜¯æ–°çš„GAFä¼˜åŒ–æ¶æ„ï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(self, 'resnet'):
            return self.resnet(x)

        # åŸæœ‰æ¶æ„çš„å‰å‘ä¼ æ’­
        x = self.conv1(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        if self.layer4 is not None:
            x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


class OptimizedLargeKernelFeatureExtractor(nn.Module):
    """é’ˆå¯¹96x96ä¼˜åŒ–çš„å¤§æ ¸ç‰¹å¾æå–å™¨"""

    def __init__(self, feature_dim=128):
        super().__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),  # 96->48
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),  # 48->24
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # 24->12
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # 12->6
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, feature_dim)

    def forward(self, x):
        # x: [N, 1, 96, 96]
        x = self.conv1(x)  # [N, 32, 48, 48]
        x = self.conv2(x)  # [N, 64, 24, 24]
        x = self.conv3(x)  # [N, 128, 12, 12]
        x = self.conv4(x)  # [N, 256, 6, 6]
        x = self.avgpool(x)  # [N, 256, 1, 1]
        x = x.view(x.size(0), -1)  # [N, 256]
        x = self.fc(x)  # [N, feature_dim]
        return x


class OptimizedDilatedFeatureExtractor(nn.Module):
    """é’ˆå¯¹96x96ä¼˜åŒ–çš„è†¨èƒ€å·ç§¯ç‰¹å¾æå–å™¨"""

    def __init__(self, feature_dim=128):
        super().__init__()
        # åˆå§‹å·ç§¯
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),  # 96->48
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )

        # è†¨èƒ€å·ç§¯å±‚ï¼Œä¿æŒç©ºé—´åˆ†è¾¨ç‡åŒæ—¶å¢å¤§æ„Ÿå—é‡
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1,
                      padding=2, dilation=2),  # ä¿æŒ48x48
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1,
                      padding=4, dilation=4),  # ä¿æŒ48x48
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )

        # æœ€ç»ˆä¸‹é‡‡æ ·
        self.conv4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # 48->24
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, feature_dim)

    def forward(self, x):
        # x: [N, 1, 96, 96]
        x = self.conv1(x)  # [N, 32, 48, 48]
        x = self.conv2(x)  # [N, 64, 24, 24] (è†¨èƒ€å·ç§¯)
        x = self.conv3(x)  # [N, 128, 12, 12] (è†¨èƒ€å·ç§¯)
        x = self.conv4(x)  # [N, 256, 6, 6]
        x = self.avgpool(x)  # [N, 256, 1, 1]
        x = x.view(x.size(0), -1)  # [N, 256]
        x = self.fc(x)  # [N, feature_dim]
        return x


class AdaptiveFusion(nn.Module):
    """è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—"""

    def __init__(self, feature_dim=128):
        super().__init__()
        self.weight_net = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )

    def forward(self, sum_feat, diff_feat):
        concat_feat = torch.cat([sum_feat, diff_feat], dim=-1)
        weights = self.weight_net(concat_feat)
        return sum_feat * weights + diff_feat * (1 - weights)


class ConcatFusion(nn.Module):
    """ç®€å•æ‹¼æ¥èåˆæ¨¡å—"""

    def __init__(self, feature_dim=128):
        super().__init__()
        self.fc = nn.Linear(feature_dim * 2, feature_dim)

    def forward(self, sum_feat, diff_feat):
        concat_feat = torch.cat([sum_feat, diff_feat], dim=-1)
        return self.fc(concat_feat)


class ElementwiseFusion(nn.Module):
    """é€å…ƒç´ èåˆæ¨¡å—"""

    def __init__(self, feature_dim=128, fusion_type='add'):
        super().__init__()
        self.fusion_type = fusion_type
        if fusion_type == 'weighted_add':
            self.alpha = nn.Parameter(torch.tensor(0.5))

    def forward(self, sum_feat, diff_feat):
        if self.fusion_type == 'add':
            return sum_feat + diff_feat
        elif self.fusion_type == 'mul':
            return sum_feat * diff_feat
        elif self.fusion_type == 'weighted_add':
            return self.alpha * sum_feat + (1 - self.alpha) * diff_feat
        else:
            raise ValueError(f"Unknown fusion type: {self.fusion_type}")


# class BiDirectionalFusion(nn.Module):
#     """åŒå‘æ³¨æ„åŠ›èåˆæ¨¡å—"""

#     def __init__(self, feature_dim=128, num_heads=4):
#         super().__init__()
#         self.feature_dim = feature_dim
#         self.sum_to_diff = nn.MultiheadAttention(
#             feature_dim, num_heads=num_heads, batch_first=True)
#         self.diff_to_sum = nn.MultiheadAttention(
#             feature_dim, num_heads=num_heads, batch_first=True)

#         # æ·»åŠ ç»´åº¦å˜æ¢å±‚ï¼š2*feature_dim -> feature_dim
#         self.fusion_projection = nn.Sequential(
#             nn.Linear(2 * feature_dim, feature_dim),
#             nn.LayerNorm(feature_dim),
#             nn.ReLU(inplace=True)
#         )

#     def forward(self, sum_feat, diff_feat):
#         # ç¡®ä¿è¾“å…¥æ˜¯3Då¼ é‡ [batch_size, seq_len, feature_dim]
#         if sum_feat.dim() == 2:
#             sum_feat = sum_feat.unsqueeze(1)  # [B, 1, D]
#             diff_feat = diff_feat.unsqueeze(1)  # [B, 1, D]
#             squeeze_output = True
#         else:
#             squeeze_output = False

#         # sumå…³æ³¨diffçš„ä¿¡æ¯
#         sum_enhanced, _ = self.sum_to_diff(sum_feat, diff_feat, diff_feat)
#         # diffå…³æ³¨sumçš„ä¿¡æ¯
#         diff_enhanced, _ = self.diff_to_sum(diff_feat, sum_feat, sum_feat)

#         # æ‹¼æ¥å¢å¼ºåçš„ç‰¹å¾ [B, seq_len, 2*feature_dim]
#         concatenated = torch.cat([sum_enhanced, diff_enhanced], dim=-1)

#         # é€šè¿‡æŠ•å½±å±‚å˜æ¢å›æ ‡å‡†ç»´åº¦ [B, seq_len, feature_dim]
#         result = self.fusion_projection(concatenated)

#         # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œåˆ™å°†è¾“å‡ºä¹Ÿè½¬æ¢ä¸º2D
#         if squeeze_output:
#             result = result.squeeze(1)

#         return result
class BiDirectionalFusion(nn.Module):
    """åŒå‘æ³¨æ„åŠ›èåˆæ¨¡å—ï¼šèåˆ sum å’Œ diff ç±»å‹ GAF å‘é‡"""

    def __init__(self, feature_dim=128, num_heads=4, use_ffn=True, ffn_dim=None):
        super().__init__()
        self.feature_dim = feature_dim
        self.use_ffn = use_ffn

        # åŒå‘äº¤å‰æ³¨æ„åŠ›
        self.sum_to_diff = nn.MultiheadAttention(
            feature_dim, num_heads=num_heads, batch_first=True
        )
        self.diff_to_sum = nn.MultiheadAttention(
            feature_dim, num_heads=num_heads, batch_first=True
        )

        # LayerNorm for each attention output
        self.sum_norm = nn.LayerNorm(feature_dim)
        self.diff_norm = nn.LayerNorm(feature_dim)

        # æ‹¼æ¥åçš„ç‰¹å¾æŠ•å½±å±‚
        self.fusion_projection = nn.Sequential(
            nn.Linear(2 * feature_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(inplace=True)
        )

        # å¯é€‰ï¼šå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å¢å¼ºè¡¨è¾¾èƒ½åŠ›
        if self.use_ffn:
            ffn_inner_dim = ffn_dim if ffn_dim is not None else 4 * feature_dim
            self.ffn = nn.Sequential(
                nn.Linear(feature_dim, ffn_inner_dim),
                nn.GELU(),
                nn.Linear(ffn_inner_dim, feature_dim)
            )
            self.ffn_norm = nn.LayerNorm(feature_dim)

    def forward(self, sum_feat, diff_feat):
        # ç¡®ä¿è¾“å…¥æ˜¯3Då¼ é‡ [batch_size, seq_len, feature_dim]
        if sum_feat.dim() == 2:
            sum_feat = sum_feat.unsqueeze(1)  # [B, 1, D]
            diff_feat = diff_feat.unsqueeze(1)  # [B, 1, D]
            squeeze_output = True
        else:
            squeeze_output = False

        # sum ç‰¹å¾å…³æ³¨ diff ç‰¹å¾
        sum_enhanced, _ = self.sum_to_diff(sum_feat, diff_feat, diff_feat)
        sum_enhanced = self.sum_norm(sum_enhanced + sum_feat)  # æ®‹å·® + LayerNorm

        # diff ç‰¹å¾å…³æ³¨ sum ç‰¹å¾
        diff_enhanced, _ = self.diff_to_sum(diff_feat, sum_feat, sum_feat)
        diff_enhanced = self.diff_norm(
            diff_enhanced + diff_feat)  # æ®‹å·® + LayerNorm

        # æ‹¼æ¥å¢å¼ºåçš„ç‰¹å¾
        concatenated = torch.cat(
            [sum_enhanced, diff_enhanced], dim=-1)  # [B, C, 2D]

        # æŠ•å½±å›åŸå§‹ç»´åº¦
        fused = self.fusion_projection(concatenated)  # [B, C, D]

        # å¯é€‰ï¼šå‰é¦ˆç½‘ç»œè¿›ä¸€æ­¥å¢å¼ºè¡¨è¾¾
        if self.use_ffn:
            fused = fused + self.ffn(fused)  # æ®‹å·®è¿æ¥
            fused = self.ffn_norm(fused)  # LayerNorm

        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œåˆ™å°†è¾“å‡ºä¹Ÿè½¬æ¢ä¸º2D
        if squeeze_output:
            fused = fused.squeeze(1)

        return fused


class GatedFusion(nn.Module):
    """é—¨æ§æœºåˆ¶èåˆæ¨¡å—"""

    def __init__(self, feature_dim=128):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )
        self.transform = nn.Linear(feature_dim * 2, feature_dim)

    def forward(self, sum_feat, diff_feat):
        concat_feat = torch.cat([sum_feat, diff_feat], dim=-1)
        gate_weights = self.gate(concat_feat)
        transformed_feat = self.transform(concat_feat)
        return transformed_feat * gate_weights


class TimeSeriesStatisticalExtractor(nn.Module):
    """æ—¶åºç»Ÿè®¡ç‰¹å¾æå–å™¨
    ä»åŸå§‹æ—¶åºæ•°æ®ä¸­æå–ç»Ÿè®¡ç‰¹å¾å’Œä¿¡å·é—´å…³è”ç‰¹å¾ï¼Œ
    é‡ç‚¹å…³æ³¨ä¿¡å·é—´çš„é™æ€å…³ç³»è€Œéæ—¶é—´ä¾èµ–æ€§
    """

    def __init__(self, num_signals, time_length, feature_dim=128, stat_type='comprehensive'):
        super().__init__()
        self.num_signals = num_signals
        self.time_length = time_length
        self.feature_dim = feature_dim
        self.stat_type = stat_type

        # ç»Ÿè®¡ç‰¹å¾ç»´åº¦
        if stat_type == 'basic':
            # 5(å‡å€¼/æ–¹å·®/æœ€å¤§/æœ€å°/ä¸­ä½æ•°) + 2(ååº¦/å³°åº¦) + 4(10/25/75/90åˆ†ä½æ•°) + 1(iqr) + 2(å˜åŒ–ç‡å‡å€¼/æ–¹å·®) = 14
            self.basic_dim = num_signals * 5
        elif stat_type in ['comprehensive', 'correlation_focused']:
            self.basic_dim = num_signals * 5
        else:
            self.basic_dim = num_signals * 5
        self.corr_dim = num_signals * (num_signals - 1) // 2 if stat_type in ['comprehensive', 'correlation_focused'] else 0
        self.diff_dim = num_signals * (num_signals - 1) if stat_type == 'correlation_focused' else 0

        # é™ç»´ MLP
        if stat_type in ['comprehensive', 'correlation_focused']:
            self.basic_proj = nn.Sequential(
                nn.Linear(self.basic_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, 128),
                nn.LayerNorm(128)
            )
            self.corr_proj = nn.Sequential(
                nn.Linear(self.corr_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, 128),
                nn.LayerNorm(128)
            )
            if stat_type == 'correlation_focused':
                self.diff_proj = nn.Sequential(
                    nn.Linear(self.diff_dim, 512),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(512, 128),
                    nn.LayerNorm(128)
                )
            else:
                self.diff_proj = None
            final_in_dim = 128 + 128 + (128 if self.diff_proj is not None else 0)
            self.final_proj = nn.Sequential(
                nn.Linear(final_in_dim, feature_dim*2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(feature_dim*2, feature_dim),
                nn.LayerNorm(feature_dim)
            )
        else:
            self.feature_projection = nn.Sequential(
                nn.Linear(self.basic_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, feature_dim),
                nn.LayerNorm(feature_dim)
            )

        print(f"æ—¶åºç»Ÿè®¡ç‰¹å¾æå–å™¨åˆå§‹åŒ–:")
        print(f"  - ä¿¡å·æ•°é‡: {num_signals}")
        print(f"  - æ—¶é—´é•¿åº¦: {time_length}")
        print(f"  - ç»Ÿè®¡ç‰¹å¾ç»´åº¦: {self.basic_dim + self.corr_dim + self.diff_dim}")
        print(f"  - è¾“å‡ºç‰¹å¾ç»´åº¦: {feature_dim}")
        print(f"  - ç»Ÿè®¡ç±»å‹: {stat_type}")

    def _calculate_stat_dim(self):
        if self.stat_type == 'basic':
            return self.num_signals * 14
        elif self.stat_type == 'comprehensive':
            basic_dim = self.num_signals * 5
            corr_dim = self.num_signals * (self.num_signals - 1) // 2
            return basic_dim + corr_dim
        elif self.stat_type == 'correlation_focused':
            basic_dim = self.num_signals * 5
            corr_dim = self.num_signals * (self.num_signals - 1) // 2
            cross_dim = self.num_signals * (self.num_signals - 1)
            return basic_dim + corr_dim + cross_dim
        else:
            raise ValueError(f"Unknown stat_type: {self.stat_type}")

    def extract_statistical_features(self, x):
        B, T, C = x.shape
        # åŸºç¡€ç»Ÿè®¡
        if self.stat_type == 'basic':
            mean_vals = torch.mean(x, dim=1)  # [B, C]
            std_vals = torch.std(x, dim=1)    # [B, C]
            max_vals = torch.max(x, dim=1)[0]  # [B, C]
            min_vals = torch.min(x, dim=1)[0]  # [B, C]
            median_vals = torch.median(x, dim=1)[0]  # [B, C]

            # ååº¦ã€å³°åº¦
            centered = x - mean_vals.unsqueeze(1)
            skewness = torch.mean(centered**3, dim=1) / (std_vals**3 + 1e-8)  # [B, C]
            kurtosis = torch.mean(centered**4, dim=1) / (std_vals**4 + 1e-8)  # [B, C]

            # åˆ†ä½æ•°
            q10 = torch.quantile(x, 0.10, dim=1)  # [B, C]
            q25 = torch.quantile(x, 0.25, dim=1)  # [B, C]
            q75 = torch.quantile(x, 0.75, dim=1)  # [B, C]
            q90 = torch.quantile(x, 0.90, dim=1)  # [B, C]
            iqr = q75 - q25  # [B, C]
            # å˜åŒ–ç‡
            diff = x[:, 1:, :] - x[:, :-1, :]  # [B, T-1, C]
            mean_diff_rate = torch.mean(diff, dim=1)  # [B, C]
            std_diff_rate = torch.std(diff, dim=1)    # [B, C]
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
            # feats = [mean_vals, std_vals, max_vals, min_vals, median_vals,
            #          skewness, kurtosis, q10, q25, q75, q90, iqr, mean_diff_rate, std_diff_rate]
            feats = [mean_vals, std_vals, max_vals, min_vals, median_vals]
            basic_feats = torch.cat(feats, dim=1)  # [B, C*14]
            return basic_feats, None, None
        else:
            # å…¶å®ƒæ¨¡å¼ä¿æŒåŸæœ‰é€»è¾‘
            mean_vals = torch.mean(x, dim=1)  # [B, C]
            std_vals = torch.std(x, dim=1)    # [B, C]
            max_vals = torch.max(x, dim=1)[0]  # [B, C]
            min_vals = torch.min(x, dim=1)[0]  # [B, C]
            median_vals = torch.median(x, dim=1)[0]  # [B, C]
            basic_feats = [mean_vals, std_vals, max_vals, min_vals, median_vals]
            basic_feats = torch.cat(basic_feats, dim=1)  # [B, basic_dim]
            # ç›¸å…³æ€§
            if self.stat_type in ['comprehensive', 'correlation_focused']:
                corr_features = []
                for b in range(B):
                    sample = x[b]
                    sample_mean = torch.mean(sample, dim=0, keepdim=True)
                    sample_std = torch.std(sample, dim=0, keepdim=True)
                    sample_normalized = (sample - sample_mean) / (sample_std + 1e-8)
                    corr_matrix = torch.mm(sample_normalized.T, sample_normalized) / (T - 1)
                    triu_indices = torch.triu_indices(C, C, offset=1)
                    corr_values = corr_matrix[triu_indices[0], triu_indices[1]]
                    corr_features.append(corr_values)
                corr_features = torch.stack(corr_features, dim=0)  # [B, corr_dim]
            else:
                corr_features = None
            # å·®å¼‚ç‰¹å¾
            if self.stat_type == 'correlation_focused':
                signal_diffs = []
                for i in range(C):
                    for j in range(i+1, C):
                        diff = x[:, :, i] - x[:, :, j]  # [B, T]
                        max_diff = torch.max(torch.abs(diff), dim=1)[0]  # [B]
                        mean_abs_diff = torch.mean(torch.abs(diff), dim=1)  # [B]
                        signal_diffs.extend([max_diff, mean_abs_diff])
                signal_diffs = torch.stack(signal_diffs, dim=1)  # [B, diff_dim]
            else:
                signal_diffs = None
            return basic_feats, corr_features, signal_diffs

    def forward(self, x):
        """å‰å‘ä¼ æ’­
        Args:
            x: [B, T, C] æˆ– [B, C, T] åŸå§‹æ—¶åºæ•°æ®
        Returns:
            features: [B, feature_dim] æŠ•å½±åçš„ç‰¹å¾
        """
        if x.dim() == 3:
            if x.shape[1] == self.time_length and x.shape[2] == self.num_signals:
                pass
            elif x.shape[1] == self.num_signals and x.shape[2] == self.time_length:
                x = x.transpose(1, 2)
            else:
                raise ValueError(f"è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¸åŒ¹é…ï¼šæœŸæœ›å½¢çŠ¶ä¸º[B, {self.time_length}, {self.num_signals}]æˆ–[B, {self.num_signals}, {self.time_length}]ï¼Œä½†å¾—åˆ°å½¢çŠ¶{x.shape}")
        else:
            raise ValueError(f"è¾“å…¥ç»´åº¦åº”ä¸º3ï¼Œå½“å‰ä¸º{x.dim()}")
        if self.stat_type in ['comprehensive', 'correlation_focused']:
            basic_feats, corr_features, signal_diffs = self.extract_statistical_features(x)
            basic_emb = self.basic_proj(basic_feats)
            if corr_features is not None:
                corr_emb = self.corr_proj(corr_features)
            else:
                corr_emb = torch.zeros(basic_emb.shape[0], 128, device=basic_emb.device, dtype=basic_emb.dtype)
            if self.stat_type == 'correlation_focused':
                if signal_diffs is not None:
                    diff_emb = self.diff_proj(signal_diffs)
                else:
                    diff_emb = torch.zeros(basic_emb.shape[0], 128, device=basic_emb.device, dtype=basic_emb.dtype)
                all_emb = torch.cat([basic_emb, corr_emb, diff_emb], dim=1)
            else:
                all_emb = torch.cat([basic_emb, corr_emb], dim=1)
            projected_features = self.final_proj(all_emb)
        else:
            stat_features = self.extract_statistical_features(x)[0]
            projected_features = self.feature_projection(stat_features)
        return projected_features


class MultiModalFusion(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å—

    èåˆGAFå›¾åƒç‰¹å¾å’Œæ—¶åºç»Ÿè®¡ç‰¹å¾
    """

    def __init__(self, feature_dim=128, fusion_strategy='concat'):
        super().__init__()
        self.feature_dim = feature_dim
        self.fusion_strategy = fusion_strategy

        if fusion_strategy == 'concat':
            # ç®€å•æ‹¼æ¥åæŠ•å½±
            self.fusion_layer = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        elif fusion_strategy == 'attention':
            # æ³¨æ„åŠ›èåˆ
            self.attention = nn.MultiheadAttention(
                feature_dim, num_heads=4, batch_first=True)
            self.norm = nn.LayerNorm(feature_dim)
        elif fusion_strategy == 'gated':
            # é—¨æ§èåˆ
            self.gate = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.Sigmoid()
            )
            self.transform = nn.Linear(feature_dim * 2, feature_dim)
        elif fusion_strategy == 'adaptive':
            # è‡ªé€‚åº”èåˆ
            self.weight_net = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.Sigmoid()
            )
        elif fusion_strategy == 'film':
            # å°†ç»Ÿè®¡ç‰¹å¾æ˜ å°„ä¸º Î³ å’Œ Î²
            self.gamma_net = nn.Linear(feature_dim, feature_dim)
            self.beta_net = nn.Linear(feature_dim, feature_dim)

            # åˆå§‹åŒ–ï¼šè®©åˆå§‹çŠ¶æ€æ¥è¿‘æ’ç­‰å˜æ¢
            nn.init.zeros_(self.gamma_net.bias)
            nn.init.constant_(self.beta_net.bias, 0.0)
            # self.film = FiLMFusion(feature_dim)
        else:
            raise ValueError(f"Unknown fusion strategy: {fusion_strategy}")

    def forward(self, gaf_features, stat_features):
        """
        Args:
            gaf_features: [B, C, feature_dim] GAFå›¾åƒç‰¹å¾
            stat_features: [B, feature_dim] ç»Ÿè®¡ç‰¹å¾

        Returns:
            fused_features: [B, C, feature_dim] èåˆåçš„ç‰¹å¾
        """
        B, C, D = gaf_features.shape
        gaf_normalized = F.normalize(gaf_features, p=2, dim=2)  # [B, C, D]
        stat_normalized = F.normalize(stat_features, p=2, dim=1)  # [B, D]
        # å°†ç»Ÿè®¡ç‰¹å¾æ‰©å±•åˆ°æ¯ä¸ªä¿¡å·
        stat_features_expanded = stat_features.unsqueeze(
            1).expand(B, C, D)  # [B, C, feature_dim]
        stat_expanded = stat_normalized.unsqueeze(1).expand_as(gaf_normalized)  # [B, C, D]
        if self.fusion_strategy == 'concat':
            # æ‹¼æ¥èåˆ
            concat_features = torch.cat(
                [gaf_normalized, stat_expanded], dim=-1)  # [B, C, 2*feature_dim]
            fused_features = self.fusion_layer(
                concat_features)  # [B, C, feature_dim]

        elif self.fusion_strategy == 'attention':
            # æ³¨æ„åŠ›èåˆ
            # å°†GAFç‰¹å¾ä½œä¸ºqueryï¼Œç»Ÿè®¡ç‰¹å¾ä½œä¸ºkeyå’Œvalue
            gaf_flat = gaf_features.reshape(B*C, 1, D)  # [B*C, 1, D]
            stat_flat = stat_features_expanded.contiguous().view(B*C, 1,
                                                                 D)  # [B*C, 1, D]

            attended, _ = self.attention(
                gaf_flat, stat_flat, stat_flat)  # [B*C, 1, D]
            attended = attended.view(B, C, D)  # [B, C, D]

            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            fused_features = self.norm(gaf_features + attended)

        elif self.fusion_strategy == 'gated':
            # é—¨æ§èåˆ
            concat_features = torch.cat(
                [gaf_normalized, stat_expanded], dim=-1)
            gate_weights = self.gate(concat_features)  # [B, C, feature_dim]
            # transformed_features = self.transform(
            #     concat_features)  # [B, C, feature_dim]
            fused_features = gate_weights * gaf_normalized + (1 - gate_weights) * stat_expanded
            # fused_features = transformed_features * gate_weights

        elif self.fusion_strategy == 'adaptive':
            # è‡ªé€‚åº”èåˆ
            concat_features = torch.cat(
                [gaf_normalized, stat_expanded], dim=-1)
            weights = self.weight_net(concat_features)  # [B, C, feature_dim]
            fused_features = gaf_features * weights + \
                stat_features_expanded * (1 - weights)
        elif self.fusion_strategy == 'film':
            # ç”Ÿæˆ Î³ å’Œ Î²
            gamma = 1 + self.gamma_net(stat_normalized)  # åˆå§‹ä¸º 1
            beta = self.beta_net(stat_normalized)        # åˆå§‹ä¸º 0

            # æ‰©å±•åˆ°æ¯ä¸ªé€šé“
            gamma = gamma.unsqueeze(1)  # [B, 1, D]
            beta = beta.unsqueeze(1)    # [B, 1, D]

            # âœ… gaf_features æ˜¯å¦å½’ä¸€åŒ–ï¼Ÿå¯é€‰
            # æ–¹æ¡ˆAï¼šä¿ç•™åŸå§‹å¼ºåº¦
            fused_features = gamma * gaf_features + beta
        return fused_features


class SignalLevelStatisticalExtractor(nn.Module):
    """ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾æå–å™¨

    ä¸ºæ¯ä¸ªä¿¡å·å•ç‹¬æå–ç»Ÿè®¡ç‰¹å¾ï¼Œæä¾›æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œä¿¡å·ç‰¹å®šæ€§
    """

    def __init__(self, num_signals, time_length, stat_feature_dim=32, stat_type='comprehensive'):
        super().__init__()
        self.num_signals = num_signals
        self.time_length = time_length
        self.stat_feature_dim = stat_feature_dim
        self.stat_type = stat_type

        # è®¡ç®—æ¯ä¸ªä¿¡å·çš„ç»Ÿè®¡ç‰¹å¾æ•°é‡
        self.single_signal_stat_dim = self._calculate_single_signal_stat_dim()

        # ä¸ºæ¯ä¸ªä¿¡å·çš„ç»Ÿè®¡ç‰¹å¾åˆ›å»ºæŠ•å½±å±‚
        self.signal_stat_projections = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.single_signal_stat_dim, stat_feature_dim * 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(stat_feature_dim * 2, stat_feature_dim),
                nn.LayerNorm(stat_feature_dim)
            ) for _ in range(num_signals)
        ])

        print(f"ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾æå–å™¨åˆå§‹åŒ–:")
        print(f"  - ä¿¡å·æ•°é‡: {num_signals}")
        print(f"  - æ—¶é—´é•¿åº¦: {time_length}")
        print(f"  - å•ä¿¡å·ç»Ÿè®¡ç‰¹å¾ç»´åº¦: {self.single_signal_stat_dim}")
        print(f"  - è¾“å‡ºç‰¹å¾ç»´åº¦: {stat_feature_dim}")
        print(f"  - ç»Ÿè®¡ç±»å‹: {stat_type}")

    def _calculate_single_signal_stat_dim(self):
        """è®¡ç®—å•ä¸ªä¿¡å·çš„ç»Ÿè®¡ç‰¹å¾ç»´åº¦"""
        if self.stat_type == 'basic':
            # åŸºç¡€ç»Ÿè®¡ï¼šå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€ä¸­ä½æ•°
            return 5
        elif self.stat_type == 'comprehensive':
            # ç»¼åˆç»Ÿè®¡ï¼šå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€ä¸­ä½æ•°ã€ååº¦ã€å³°åº¦ã€å˜å¼‚ç³»æ•°
            return 8
        elif self.stat_type == 'extended':
            # æ‰©å±•ç»Ÿè®¡ï¼šåŸºç¡€ç»Ÿè®¡(5) + é«˜é˜¶ç»Ÿè®¡(3) + æ‰©å±•ç»Ÿè®¡(4) = 12
            # åŸºç¡€ï¼šmean, std, max, min, median
            # é«˜é˜¶ï¼šskewness, kurtosis, cv
            # æ‰©å±•ï¼špercentile_25, percentile_75, range_val, mad
            return 12
        else:
            raise ValueError(f"Unknown stat_type: {self.stat_type}")

    def extract_signal_level_features(self, x):
        """æå–ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾

        Args:
            x: [B, T, C] åŸå§‹æ—¶åºæ•°æ®

        Returns:
            features: [B, C, single_signal_stat_dim] æ¯ä¸ªä¿¡å·çš„ç»Ÿè®¡ç‰¹å¾
        """
        B, T, C = x.shape
        signal_features = []

        for c in range(C):
            signal_data = x[:, :, c]  # [B, T] å•ä¸ªä¿¡å·çš„æ•°æ®

            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            mean_val = torch.mean(signal_data, dim=1)  # [B]
            std_val = torch.std(signal_data, dim=1)    # [B]
            max_val = torch.max(signal_data, dim=1)[0]  # [B]
            min_val = torch.min(signal_data, dim=1)[0]  # [B]
            median_val = torch.median(signal_data, dim=1)[0]  # [B]

            features = [mean_val, std_val, max_val, min_val, median_val]

            if self.stat_type in ['comprehensive', 'extended']:
                # é«˜é˜¶ç»Ÿè®¡ç‰¹å¾
                centered = signal_data - mean_val.unsqueeze(1)  # [B, T]
                skewness = torch.mean(centered**3, dim=1) / \
                    (std_val**3 + 1e-8)  # [B]
                kurtosis = torch.mean(centered**4, dim=1) / \
                    (std_val**4 + 1e-8)  # [B]
                cv = std_val / (torch.abs(mean_val) + 1e-8)  # [B] å˜å¼‚ç³»æ•°

                features.extend([skewness, kurtosis, cv])

            if self.stat_type == 'extended':
                # æ‰©å±•ç»Ÿè®¡ç‰¹å¾
                # 25% å’Œ 75% ç™¾åˆ†ä½æ•°
                percentile_25 = torch.quantile(signal_data, 0.25, dim=1)  # [B]
                percentile_75 = torch.quantile(signal_data, 0.75, dim=1)  # [B]

                # èŒƒå›´å’Œå››åˆ†ä½è·
                range_val = max_val - min_val  # [B]

                # å‡å€¼ç»å¯¹åå·®
                mad = torch.mean(
                    torch.abs(signal_data - mean_val.unsqueeze(1)), dim=1)  # [B]

                features.extend([percentile_25, percentile_75, range_val, mad])

            # å †å å•ä¸ªä¿¡å·çš„æ‰€æœ‰ç‰¹å¾
            # [B, single_signal_stat_dim]
            signal_feat = torch.stack(features, dim=1)
            signal_features.append(signal_feat)

        # å †å æ‰€æœ‰ä¿¡å·çš„ç‰¹å¾
        # [B, C, single_signal_stat_dim]
        all_signal_features = torch.stack(signal_features, dim=1)
        return all_signal_features

    def forward(self, x):
        """å‰å‘ä¼ æ’­

        Args:
            x: [B, T, C] æˆ– [B, C, T] åŸå§‹æ—¶åºæ•°æ®

        Returns:
            features: [B, C, stat_feature_dim] æ¯ä¸ªä¿¡å·çš„æŠ•å½±ç»Ÿè®¡ç‰¹å¾
        """
        # ç¡®ä¿è¾“å…¥æ ¼å¼ä¸º [B, T, C]
        if x.dim() == 3:
            if x.shape[1] == self.time_length and x.shape[2] == self.num_signals:
                # [B, T, C] - æ­£ç¡®æ ¼å¼
                pass
            elif x.shape[1] == self.num_signals and x.shape[2] == self.time_length:
                # [B, C, T] - éœ€è¦è½¬ç½®
                x = x.transpose(1, 2)  # [B, T, C]
            else:
                raise ValueError(
                    f"è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¸åŒ¹é…ï¼šæœŸæœ›å½¢çŠ¶ä¸º[B, {self.time_length}, {self.num_signals}]æˆ–[B, {self.num_signals}, {self.time_length}]ï¼Œä½†å¾—åˆ°å½¢çŠ¶{x.shape}")
        else:
            raise ValueError(f"è¾“å…¥ç»´åº¦åº”ä¸º3ï¼Œå½“å‰ä¸º{x.dim()}")

        # æå–ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾
        signal_stat_features = self.extract_signal_level_features(
            x)  # [B, C, single_signal_stat_dim]

        # ä¸ºæ¯ä¸ªä¿¡å·åº”ç”¨å¯¹åº”çš„æŠ•å½±å±‚
        projected_features = []
        for c in range(self.num_signals):
            # [B, single_signal_stat_dim]
            signal_feat = signal_stat_features[:, c, :]
            projected_feat = self.signal_stat_projections[c](
                signal_feat)  # [B, stat_feature_dim]
            projected_features.append(projected_feat)

        # å †å æ‰€æœ‰ä¿¡å·çš„æŠ•å½±ç‰¹å¾
        all_projected_features = torch.stack(
            projected_features, dim=1)  # [B, C, stat_feature_dim]

        return all_projected_features


class SignalLevelStatisticalFusion(nn.Module):
    """ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾èåˆæ¨¡å—

    å°†GAFç‰¹å¾å’Œä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾è¿›è¡Œèåˆï¼Œæ”¯æŒå¤šç§èåˆç­–ç•¥
    """

    def __init__(self, gaf_feature_dim, stat_feature_dim, output_feature_dim=None,
                 fusion_strategy='concat_project'):
        super().__init__()
        self.gaf_feature_dim = gaf_feature_dim
        self.stat_feature_dim = stat_feature_dim
        self.output_feature_dim = output_feature_dim or gaf_feature_dim
        self.fusion_strategy = fusion_strategy

        self._build_fusion_module()

        print(f"ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾èåˆæ¨¡å—åˆå§‹åŒ–:")
        print(f"  - GAFç‰¹å¾ç»´åº¦: {gaf_feature_dim}")
        print(f"  - ç»Ÿè®¡ç‰¹å¾ç»´åº¦: {stat_feature_dim}")
        print(f"  - è¾“å‡ºç‰¹å¾ç»´åº¦: {self.output_feature_dim}")
        print(f"  - èåˆç­–ç•¥: {fusion_strategy}")

    def _build_fusion_module(self):
        """æ„å»ºèåˆæ¨¡å—"""
        if self.fusion_strategy == 'concat_project':
            # ç­–ç•¥1ï¼šç›´æ¥æ‹¼æ¥åæŠ•å½±
            self.fusion_layer = nn.Sequential(
                nn.Linear(self.gaf_feature_dim + self.stat_feature_dim,
                          self.output_feature_dim * 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(self.output_feature_dim *
                          2, self.output_feature_dim),
                nn.LayerNorm(self.output_feature_dim)
            )

        elif self.fusion_strategy == 'attention_fusion':
            # ç­–ç•¥2ï¼šæ³¨æ„åŠ›èåˆ
            self.gaf_attention = nn.Sequential(
                nn.Linear(self.gaf_feature_dim +
                          self.stat_feature_dim, self.gaf_feature_dim),
                nn.Sigmoid()
            )
            self.stat_attention = nn.Sequential(
                nn.Linear(self.gaf_feature_dim +
                          self.stat_feature_dim, self.stat_feature_dim),
                nn.Sigmoid()
            )
            self.output_projection = nn.Linear(
                self.gaf_feature_dim + self.stat_feature_dim, self.output_feature_dim)

        elif self.fusion_strategy == 'gated_fusion':
            # ç­–ç•¥3ï¼šé—¨æ§èåˆ
            self.gate_network = nn.Sequential(
                nn.Linear(self.gaf_feature_dim +
                          self.stat_feature_dim, self.output_feature_dim),
                nn.Sigmoid()
            )
            self.transform_network = nn.Sequential(
                nn.Linear(self.gaf_feature_dim +
                          self.stat_feature_dim, self.output_feature_dim),
                nn.Tanh()
            )

        elif self.fusion_strategy == 'residual_fusion':
            # ç­–ç•¥4ï¼šæ®‹å·®èåˆ
            self.stat_transform = nn.Sequential(
                nn.Linear(self.stat_feature_dim, self.gaf_feature_dim),
                nn.ReLU(),
                nn.Linear(self.gaf_feature_dim, self.gaf_feature_dim)
            )
            self.output_projection = nn.Linear(
                self.gaf_feature_dim, self.output_feature_dim) if self.output_feature_dim != self.gaf_feature_dim else nn.Identity()

        elif self.fusion_strategy == 'cross_attention':
            # ç­–ç•¥5ï¼šäº¤å‰æ³¨æ„åŠ›èåˆ
            self.gaf_to_stat_attention = nn.MultiheadAttention(
                self.gaf_feature_dim, num_heads=4, batch_first=True
            )
            self.stat_to_gaf_attention = nn.MultiheadAttention(
                self.stat_feature_dim, num_heads=4, batch_first=True
            )
            # æ·»åŠ  LayerNormï¼ˆæ³¨æ„ï¼šéœ€è¦ä¸å¯¹åº”ç‰¹å¾ç»´åº¦ä¸€è‡´ï¼‰
            self.gaf_norm = nn.LayerNorm(self.gaf_feature_dim)
            self.stat_norm = nn.LayerNorm(self.stat_feature_dim)

            self.output_projection = nn.Sequential(
                nn.Linear(self.gaf_feature_dim +
                          self.stat_feature_dim, self.output_feature_dim),
                nn.LayerNorm(self.output_feature_dim)
            )
            # æ–°å¢ï¼šFFN æ¨¡å—
            self.ffn = nn.Sequential(
                nn.Linear(self.output_feature_dim,
                          self.output_feature_dim * 2),
                nn.GELU(),
                nn.Linear(self.output_feature_dim * 2, self.output_feature_dim)
            )
            self.ffn_norm = nn.LayerNorm(self.output_feature_dim)

        elif self.fusion_strategy == 'adaptive_fusion':
            # ç­–ç•¥6ï¼šè‡ªé€‚åº”èåˆ
            self.adaptation_network = nn.Sequential(
                nn.Linear(self.gaf_feature_dim +
                          self.stat_feature_dim, self.output_feature_dim),
                nn.ReLU(),
                nn.Linear(self.output_feature_dim, 2),  # è¾“å‡ºä¸¤ä¸ªæƒé‡
                nn.Softmax(dim=-1)
            )
            self.gaf_projection = nn.Linear(
                self.gaf_feature_dim, self.output_feature_dim)
            self.stat_projection = nn.Linear(
                self.stat_feature_dim, self.output_feature_dim)

        else:
            raise ValueError(
                f"Unknown fusion strategy: {self.fusion_strategy}")

    def forward(self, gaf_features, stat_features):
        """å‰å‘ä¼ æ’­

        Args:
            gaf_features: [B, C, gaf_feature_dim] GAFç‰¹å¾
            stat_features: [B, C, stat_feature_dim] ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾

        Returns:
            fused_features: [B, C, output_feature_dim] èåˆåçš„ç‰¹å¾
        """
        B, C, _ = gaf_features.shape

        if self.fusion_strategy == 'concat_project':
            # ç›´æ¥æ‹¼æ¥åæŠ•å½±
            # [B, C, gaf_dim + stat_dim]
            concatenated = torch.cat([gaf_features, stat_features], dim=-1)
            fused_features = self.fusion_layer(
                concatenated)  # [B, C, output_dim]

        elif self.fusion_strategy == 'attention_fusion':
            # æ³¨æ„åŠ›èåˆ
            # [B, C, gaf_dim + stat_dim]
            concatenated = torch.cat([gaf_features, stat_features], dim=-1)

            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            gaf_attention = self.gaf_attention(concatenated)  # [B, C, gaf_dim]
            stat_attention = self.stat_attention(
                concatenated)  # [B, C, stat_dim]

            # åº”ç”¨æ³¨æ„åŠ›æƒé‡
            attended_gaf = gaf_features * gaf_attention  # [B, C, gaf_dim]
            attended_stat = stat_features * stat_attention  # [B, C, stat_dim]

            # æ‹¼æ¥å¹¶æŠ•å½±
            # [B, C, gaf_dim + stat_dim]
            attended_concat = torch.cat([attended_gaf, attended_stat], dim=-1)
            fused_features = self.output_projection(
                attended_concat)  # [B, C, output_dim]

        elif self.fusion_strategy == 'gated_fusion':
            # é—¨æ§èåˆ
            # [B, C, gaf_dim + stat_dim]
            concatenated = torch.cat([gaf_features, stat_features], dim=-1)

            gate = self.gate_network(concatenated)  # [B, C, output_dim]
            transform = self.transform_network(
                concatenated)  # [B, C, output_dim]

            fused_features = gate * transform  # [B, C, output_dim]

        elif self.fusion_strategy == 'residual_fusion':
            # æ®‹å·®èåˆ
            transformed_stat = self.stat_transform(
                stat_features)  # [B, C, gaf_dim]
            residual_features = gaf_features + \
                transformed_stat  # [B, C, gaf_dim]
            fused_features = self.output_projection(
                residual_features)  # [B, C, output_dim]

        elif self.fusion_strategy == 'cross_attention':
            # äº¤å‰æ³¨æ„åŠ›èåˆ
            # GAFç‰¹å¾å…³æ³¨ç»Ÿè®¡ç‰¹å¾
            gaf_attended, _ = self.gaf_to_stat_attention(
                gaf_features, stat_features, stat_features
            )  # [B, C, gaf_dim]
            # æ®‹å·®è¿æ¥ + LayerNorm
            gaf_attended = self.gaf_norm(
                gaf_attended + gaf_features)  # [B, C, gaf_dim]

            # ç»Ÿè®¡ç‰¹å¾å…³æ³¨GAFç‰¹å¾
            stat_attended, _ = self.stat_to_gaf_attention(
                stat_features, gaf_features, gaf_features
            )  # [B, C, stat_dim]
            # æ®‹å·®è¿æ¥ + LayerNorm
            stat_attended = self.stat_norm(
                stat_attended + stat_features)  # [B, C, stat_dim]

            # æ‹¼æ¥å¹¶æŠ•å½±
            # [B, C, gaf_dim + stat_dim]
            cross_attended = torch.cat([gaf_attended, stat_attended], dim=-1)
            fused_features = self.output_projection(
                cross_attended)  # [B, C, output_dim]

            # æ–°å¢ï¼šFFN å¤„ç†
            ffn_output = self.ffn(fused_features)
            fused_features = self.ffn_norm(
                ffn_output + fused_features)  # æ®‹å·® + å½’ä¸€åŒ–

        elif self.fusion_strategy == 'adaptive_fusion':
            # è‡ªé€‚åº”èåˆ
            # [B, C, gaf_dim + stat_dim]
            concatenated = torch.cat([gaf_features, stat_features], dim=-1)

            # è®¡ç®—è‡ªé€‚åº”æƒé‡
            fusion_weights = self.adaptation_network(concatenated)  # [B, C, 2]

            # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
            projected_gaf = self.gaf_projection(
                gaf_features)  # [B, C, output_dim]
            projected_stat = self.stat_projection(
                stat_features)  # [B, C, output_dim]

            # è‡ªé€‚åº”åŠ æƒ
            fused_features = (fusion_weights[:, :, 0:1] * projected_gaf +
                              fusion_weights[:, :, 1:2] * projected_stat)  # [B, C, output_dim]

        return fused_features


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å— - å¯¹ä¿¡å·é€šé“ç»´åº¦è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—"""

    def __init__(self, num_channels, reduction=8):
        super().__init__()
        # å¯¹ç‰¹å¾ç»´åº¦è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)

        # ç¡®ä¿reductionåçš„é€šé“æ•°è‡³å°‘ä¸º1
        reduced_channels = max(1, num_channels // reduction)

        # MLPç”¨äºå­¦ä¹ é€šé“é—´çš„å…³ç³»
        self.fc = nn.Sequential(
            nn.Linear(num_channels, reduced_channels),
            nn.ReLU(),
            nn.Linear(reduced_channels, num_channels)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: [B, C, feature_dim] å…¶ä¸­Cæ˜¯ä¿¡å·æ•°é‡
        B, C, D = x.shape

        # å¯¹æ¯ä¸ªä¿¡å·çš„ç‰¹å¾ç»´åº¦è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        # [B, C, D] -> [B, C, 1] -> [B, C]
        avg_out = self.avg_pool(x).squeeze(-1)  # [B, C]
        max_out = self.max_pool(x).squeeze(-1)  # [B, C]

        # é€šè¿‡MLPå­¦ä¹ é€šé“æ³¨æ„åŠ›æƒé‡
        avg_attention = self.fc(avg_out)  # [B, C]
        max_attention = self.fc(max_out)  # [B, C]

        # èåˆå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–çš„æ³¨æ„åŠ›
        attention = self.sigmoid(avg_attention + max_attention)  # [B, C]

        # æ‰©å±•ç»´åº¦å¹¶åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attention = attention.unsqueeze(-1)  # [B, C, 1]
        return x * attention  # [B, C, D]


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆå¯¹ä¿¡å·ç»´åº¦ï¼‰"""

    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv1 = nn.Conv1d(
            2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: [B, C, feature_dim]
        # å¯¹ç‰¹å¾ç»´åº¦è¿›è¡Œå¹³å‡å’Œæœ€å¤§æ“ä½œï¼Œå¾—åˆ°æ¯ä¸ªä¿¡å·çš„ä»£è¡¨æ€§ç‰¹å¾
        avg_out = torch.mean(x, dim=2, keepdim=True)  # [B, C, 1]
        max_out, _ = torch.max(x, dim=2, keepdim=True)  # [B, C, 1]

        # æ‹¼æ¥å¹³å‡å’Œæœ€å¤§ç‰¹å¾
        x_cat = torch.cat([avg_out, max_out], dim=2)  # [B, C, 2]
        x_cat = x_cat.transpose(1, 2)  # [B, 2, C]

        # é€šè¿‡1Då·ç§¯å­¦ä¹ ä¿¡å·é—´çš„ç©ºé—´å…³ç³»
        attention = self.conv1(x_cat)  # [B, 1, C]
        attention = attention.transpose(1, 2)  # [B, C, 1]
        attention = self.sigmoid(attention)

        return x * attention  # [B, C, feature_dim]


class SignalAttention(nn.Module):
    """ä¿¡å·æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, feature_dim, num_signals, attention_type='channel'):
        super().__init__()
        self.attention_type = attention_type

        if attention_type == 'channel':
            self.attention = ChannelAttention(num_signals)  # ä¼ å…¥ä¿¡å·æ•°é‡è€Œä¸æ˜¯ç‰¹å¾ç»´åº¦
        elif attention_type == 'spatial':
            self.attention = SpatialAttention()
        elif attention_type == 'cbam':
            self.channel_attention = ChannelAttention(num_signals)  # ä¼ å…¥ä¿¡å·æ•°é‡
            self.spatial_attention = SpatialAttention()
        elif attention_type == 'self':
            self.self_attention = nn.MultiheadAttention(
                feature_dim, num_heads=8, batch_first=True)
            self.norm1 = nn.LayerNorm(feature_dim)  # è‡ªæ³¨æ„åŠ›åçš„å½’ä¸€åŒ–
            self.ffn = nn.Sequential(
                nn.Linear(feature_dim, feature_dim * 2),
                nn.GELU(),
                nn.Linear(feature_dim * 2, feature_dim)
            )
            self.norm2 = nn.LayerNorm(feature_dim)

        else:
            raise ValueError(f"Unknown attention type: {attention_type}")

    def forward(self, x):
        # x: [B, C, feature_dim]
        if self.attention_type == 'channel':
            return self.attention(x)
        elif self.attention_type == 'spatial':
            return self.attention(x)
        elif self.attention_type == 'cbam':
            x = self.channel_attention(x)
            x = self.spatial_attention(x)
            return x
        elif self.attention_type == 'self':
            # x: [B, C, feature_dim] -> [B, C, feature_dim]
            x_reshaped = x.reshape(x.size(0), x.size(1), -1)
            attn_out, _ = self.self_attention(
                x_reshaped, x_reshaped, x_reshaped)
            # æ®‹å·®è¿æ¥ + LayerNorm
            out = self.norm1(attn_out + x)  # [B, C, feature_dim]
            out = self.ffn(out)
            out = self.norm2(out + out)
            return out


class ResidualBlock(nn.Module):
    """
    æ®‹å·®å—ï¼Œç”¨äºåˆ†ç±»å™¨ä¸­çš„ç‰¹å¾å­¦ä¹ 

    æ”¯æŒä¸åŒç±»å‹çš„æ®‹å·®è¿æ¥ï¼š
    - 'basic': åŸºç¡€æ®‹å·®å—
    - 'bottleneck': ç“¶é¢ˆæ®‹å·®å—ï¼ˆé€‚ç”¨äºé«˜ç»´ç‰¹å¾ï¼‰
    - 'dense': å¯†é›†è¿æ¥æ®‹å·®å—
    """

    def __init__(self, in_dim, out_dim, hidden_dim=None, block_type='basic', dropout=0.1):
        super().__init__()
        self.block_type = block_type
        self.in_dim = in_dim
        self.out_dim = out_dim

        if hidden_dim is None:
            hidden_dim = max(in_dim, out_dim) // 2

        if block_type == 'basic':
            # åŸºç¡€æ®‹å·®å—ï¼šè¾“å…¥ -> çº¿æ€§ -> æ¿€æ´» -> Dropout -> çº¿æ€§ -> è¾“å‡º
            self.main_path = nn.Sequential(
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, out_dim)
            )
        elif block_type == 'bottleneck':
            # ç“¶é¢ˆæ®‹å·®å—ï¼šé™ç»´ -> å¤„ç† -> å‡ç»´
            bottleneck_dim = hidden_dim // 2
            self.main_path = nn.Sequential(
                nn.Linear(in_dim, bottleneck_dim),
                nn.ReLU(inplace=True),
                nn.Linear(bottleneck_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, out_dim)
            )
        elif block_type == 'dense':
            # å¯†é›†è¿æ¥æ®‹å·®å—ï¼šæ›´å¤æ‚çš„ç‰¹å¾ç»„åˆ
            self.main_path = nn.Sequential(
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, out_dim)
            )

        # å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒï¼Œéœ€è¦æŠ•å½±å±‚
        if in_dim != out_dim:
            self.shortcut = nn.Linear(in_dim, out_dim)
        else:
            self.shortcut = nn.Identity()

        # æœ€ç»ˆæ¿€æ´»å‡½æ•°
        self.final_activation = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = self.shortcut(x)
        out = self.main_path(x)
        out = out + residual
        out = self.final_activation(out)
        return out


class ResidualClassifier(nn.Module):
    """
    åŸºäºæ®‹å·®å—çš„åˆ†ç±»å™¨

    Args:
        input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        num_classes (int): åˆ†ç±»ç±»åˆ«æ•°
        hidden_dims (list): éšè—å±‚ç»´åº¦åˆ—è¡¨
        block_type (str): æ®‹å·®å—ç±»å‹
        dropout (float): Dropoutæ¯”ç‡
        use_batch_norm (bool): æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
    """

    def __init__(self, input_dim, num_classes, hidden_dims=None,
                 block_type='basic', dropout=0.1, use_batch_norm=False):
        super().__init__()

        if hidden_dims is None:
            # æ ¹æ®è¾“å…¥ç»´åº¦è‡ªåŠ¨è®¾è®¡ç½‘ç»œç»“æ„
            if input_dim > 2048:
                hidden_dims = [2048, 1024, 512, 256]
            elif input_dim > 1024:
                hidden_dims = [1024, 512, 256]
            else:
                hidden_dims = [512, 256]

        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList() if use_batch_norm else None

        # æ„å»ºæ®‹å·®å±‚åºåˆ—
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            self.layers.append(ResidualBlock(
                in_dim=prev_dim,
                out_dim=hidden_dim,
                block_type=block_type,
                dropout=dropout
            ))

            if use_batch_norm:
                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

            prev_dim = hidden_dim

        # æœ€ç»ˆåˆ†ç±»å±‚
        self.classifier = nn.Linear(prev_dim, num_classes)

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(
                    m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if self.batch_norms is not None:
                # ã€ä¿®å¤ã€‘ç§»é™¤å±é™©çš„åŠ¨æ€æ¨¡å¼åˆ‡æ¢ï¼Œä½¿ç”¨å®‰å…¨çš„æ‰¹å½’ä¸€åŒ–ç­–ç•¥
                if x.size(0) == 1:
                    # å¯¹äºbatch sizeä¸º1çš„æƒ…å†µï¼Œä½¿ç”¨å®ä¾‹å½’ä¸€åŒ–æˆ–ç¦ç”¨BatchNorm
                    # è¿™æ ·å¯ä»¥é¿å…BatchNormåœ¨è®­ç»ƒæ¨¡å¼ä¸‹çš„æ•°å€¼ä¸ç¨³å®šé—®é¢˜
                    if self.training:
                        # è®­ç»ƒæ¨¡å¼ä¸‹ï¼šè·³è¿‡BatchNormä»¥é¿å…å†…å­˜é”™è¯¯
                        pass  # è·³è¿‡BatchNorm
                    else:
                        # è¯„ä¼°æ¨¡å¼ä¸‹ï¼šæ­£å¸¸ä½¿ç”¨BatchNorm
                        x = self.batch_norms[i](x)
                else:
                    # æ­£å¸¸batch sizeï¼šç›´æ¥ä½¿ç”¨BatchNorm
                    x = self.batch_norms[i](x)

        x = self.classifier(x)
        return x


class DualGAFNet(nn.Module):
    """åŒè·¯GAFç½‘ç»œ"""

    def __init__(
        self,
        feature_dim=32,
        num_classes=4,
        num_images=30,
        time_length=96,  # æ–°å¢ï¼šæ—¶é—´åºåˆ—é•¿åº¦
        # ç‰¹å¾æå–å™¨é…ç½®
        # 'large_kernel', 'inception', 'dilated', 'multiscale'
        extractor_type='large_kernel',
        # èåˆæ¨¡å—é…ç½®
        fusion_type='adaptive',  # 'adaptive', 'concat', 'add', 'mul', 'weighted_add'
        # æ³¨æ„åŠ›æ¨¡å—é…ç½®
        attention_type='channel',  # 'channel', 'spatial', 'cbam', 'self', 'none'
        # åˆ†ç±»å™¨é…ç½®
        classifier_type='mlp',  # 'mlp', 'simple'
        # ç»Ÿè®¡ç‰¹å¾é…ç½®
        use_statistical_features=True,  # æ˜¯å¦ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾
        stat_type='comprehensive',  # 'basic', 'comprehensive', 'correlation_focused'
        # 'concat', 'attention', 'gated', 'adaptive'
        multimodal_fusion_strategy='concat',

        # ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾é…ç½®ï¼ˆæ–°å¢ï¼‰
        use_signal_level_stats=False,  # æ˜¯å¦ä½¿ç”¨ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ï¼ˆå¯¹æ¯”å®éªŒï¼‰
        signal_stat_type='comprehensive',  # 'basic', 'comprehensive', 'extended'
        # 'concat_project', 'attention_fusion', 'gated_fusion', 'residual_fusion', 'cross_attention', 'adaptive_fusion'
        signal_stat_fusion_strategy='concat_project',
        signal_stat_feature_dim=32,  # ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾çš„ç»´åº¦

        # æ¶ˆèå®éªŒå¼€å…³
        use_diff_branch=True,  # æ˜¯å¦ä½¿ç”¨diffåˆ†æ”¯è¿›è¡Œèåˆï¼ˆæ¶ˆèå®éªŒï¼‰
        
        # é€šé“å‹ç¼©é…ç½®ï¼ˆæ–°å¢ï¼‰
        use_channel_compression=False,  # æ˜¯å¦ä½¿ç”¨é€šé“å‹ç¼©
        compression_strategy='conv1d',  # 'conv1d', 'grouped', 'separable', 'multiscale', 'attention_guided', 'adaptive', 'hvac_grouped'
        compression_ratio=0.7,  # å‹ç¼©æ¯”ä¾‹
        adaptive_compression_ratios=[0.5, 0.7, 0.8],  # è‡ªé€‚åº”å‹ç¼©å¯é€‰æ¯”ä¾‹
        hvac_group_compression_ratios=None,  # HVACåˆ†ç»„å‹ç¼©æ¯”ä¾‹
        compression_channels=None,  # å‹ç¼©é€šé“æ•°
        
        # å…¶ä»–é…ç½®
        hvac_groups=None,
        feature_columns=None,
        nhead=4,
        num_layers=2
    ):
        super().__init__()
        self.num_images = num_images
        self.time_length = time_length
        self.feature_dim = feature_dim
        self.extractor_type = extractor_type
        self.fusion_type = fusion_type
        self.attention_type = attention_type
        self.classifier_type = classifier_type
        self.use_statistical_features = use_statistical_features
        self.stat_type = stat_type
        self.multimodal_fusion_strategy = multimodal_fusion_strategy
        # ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾é…ç½®
        self.use_signal_level_stats = use_signal_level_stats
        self.signal_stat_type = signal_stat_type
        self.signal_stat_fusion_strategy = signal_stat_fusion_strategy
        self.signal_stat_feature_dim = signal_stat_feature_dim

        # æ¶ˆèå®éªŒå¼€å…³
        self.use_diff_branch = use_diff_branch

        # é€šé“å‹ç¼©é…ç½®
        self.use_channel_compression = use_channel_compression
        self.compression_strategy = compression_strategy
        self.compression_ratio = compression_ratio
        self.compression_channels = compression_channels
        self.adaptive_compression_ratios = adaptive_compression_ratios
        self.hvac_group_compression_ratios = hvac_group_compression_ratios

        # HVACåˆ†ç»„é…ç½®
        self.hvac_groups = hvac_groups
        self.feature_columns = feature_columns if feature_columns else []
        self.use_grouping = hvac_groups is not None

        # æ™ºèƒ½æ¨èç‰¹å¾æå–å™¨ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼‰
        if extractor_type == 'auto':
            recommended_extractor = self._get_recommended_extractor_for_signal_count(
                num_images)
            print(f"æ ¹æ®ä¿¡å·æ•°é‡ {num_images} è‡ªåŠ¨æ¨èç‰¹å¾æå–å™¨: {recommended_extractor}")
            self.extractor_type = recommended_extractor

        # åˆ›å»ºç‰¹å¾æå–å™¨
        self._build_extractors()

        # åˆ›å»ºèåˆæ¨¡å—ï¼ˆä»…åœ¨ä½¿ç”¨diffåˆ†æ”¯æ—¶ï¼‰
        if self.use_diff_branch:
            self._build_fusion_module()

        # åˆ›å»ºç»Ÿè®¡ç‰¹å¾æå–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_statistical_features:
            self._build_statistical_extractor()
            self._build_multimodal_fusion()

        # åˆ›å»ºä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾æå–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_signal_level_stats:
            self._build_signal_level_statistical_extractor()
            self._build_signal_level_statistical_fusion()
            
        # åˆ›å»ºé€šé“å‹ç¼©æ¨¡å—ï¼ˆå¦‚æœå¯ç”¨ï¼‰- å¿…é¡»åœ¨æ³¨æ„åŠ›æ¨¡å—ä¹‹å‰
        if self.use_channel_compression:
            self._build_channel_compression_module()
            
        # åˆ›å»ºä¿¡å·æ³¨æ„åŠ›æ¨¡å—
        self._build_attention_module()

        # åˆ›å»ºåˆ†ç±»å™¨
        self._build_classifier(num_classes)

        # æ‰“å°é…ç½®ä¿¡æ¯
        print(f"å¢å¼ºåŒè·¯GAFç½‘ç»œæ„å»ºå®Œæˆ:")
        print(f"  - ç‰¹å¾æå–å™¨: {self.extractor_type}")
        if self.use_diff_branch:
            print(f"  - èåˆæ¨¡å—: {fusion_type}")
        else:
            print(f"  - èåˆæ¨¡å—: å·²ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒï¼šä»…ä½¿ç”¨sumåˆ†æ”¯ï¼‰")
        print(f"  - æ³¨æ„åŠ›æ¨¡å—: {attention_type}")
        if attention_type == 'none':
            print(f"    ğŸ’¡ æ³¨æ„åŠ›å·²ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒï¼‰")
        print(f"  - åˆ†ç±»å™¨: {classifier_type}")
        print(f"  - ä¿¡å·æ•°é‡: {num_images}")
        print(f"  - GAFå›¾åƒå°ºå¯¸: {time_length}x{time_length}")
        print(f"  - ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾: {use_statistical_features}")
        if not use_statistical_features:
            print(f"    ğŸ’¡ ç»Ÿè®¡ç‰¹å¾å·²ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒï¼‰")
        if use_statistical_features:
            print(f"  - ç»Ÿè®¡ç‰¹å¾ç±»å‹: {stat_type}")
            print(f"  - å¤šæ¨¡æ€èåˆç­–ç•¥: {multimodal_fusion_strategy}")
        print(f"  - ä½¿ç”¨ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾: {use_signal_level_stats}")
        if use_signal_level_stats:
            print(f"  - ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ç±»å‹: {signal_stat_type}")
            print(f"  - ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ç»´åº¦: {signal_stat_feature_dim}")
            print(f"  - ä¿¡å·çº§ç»Ÿè®¡èåˆç­–ç•¥: {signal_stat_fusion_strategy}")
        print(f"  - ä½¿ç”¨åˆ†ç»„: {self.use_grouping}")

        # æ¶ˆèå®éªŒçŠ¶æ€æç¤º
        ablation_info = []
        if not self.use_diff_branch:
            ablation_info.append("GAFå·®åˆ†åˆ†æ”¯æ¶ˆè")
        if not self.use_statistical_features:
            ablation_info.append("ç»Ÿè®¡ç‰¹å¾æ¶ˆè")
        if self.use_signal_level_stats:
            ablation_info.append("ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾å¯ç”¨")
        if self.attention_type == 'none':
            ablation_info.append("æ³¨æ„åŠ›æœºåˆ¶æ¶ˆè")

        if ablation_info:
            print(f"\nğŸ”¬ æ¶ˆèå®éªŒçŠ¶æ€: {' + '.join(ablation_info)}")
        else:
            print(f"\nğŸ”¬ æ¶ˆèå®éªŒçŠ¶æ€: å®Œæ•´æ¨¡å‹ï¼ˆæœªå¯ç”¨æ¶ˆèï¼‰")

        # æ‰“å°å¯ç”¨çš„ç‰¹å¾æå–å™¨ç±»å‹
        print(f"\næ”¯æŒçš„ç‰¹å¾æå–å™¨ç±»å‹:")
        print(f"ğŸ“Š GAFä¼˜åŒ–ResNetç³»åˆ— (æ¨è):")
        print(f"  - resnet18_gaf: GAFä¼˜åŒ–ResNet18 (é€‚åˆâ‰¤30ä¸ªä¿¡å·) â­")
        print(f"  - resnet18_gaf_light: è½»é‡çº§GAF ResNet (é€‚åˆ30-60ä¸ªä¿¡å·) â­")
        print(f"  - resnet_gaf_deep: æ·±å±‚GAF ResNet (æ›´å¥½è¡¨è¾¾èƒ½åŠ›)")
        print(f"  - resnet_gaf_preserve: é«˜ä¿çœŸGAF ResNet (æœ€å¤§ç¨‹åº¦ä¿ç•™ç©ºé—´ä¿¡æ¯)")
        print(f"ğŸ”§ ä¼ ç»ŸResNetç³»åˆ— (å…¼å®¹æ€§):")
        print(f"  - resnet18: æ ‡å‡†ResNet18 (è¿‡æ—©ä¸‹é‡‡æ ·)")
        print(f"  - resnet34: æ ‡å‡†ResNet34 (è¿‡æ—©ä¸‹é‡‡æ ·)")
        print(f"  - resnet_light: è½»é‡çº§ResNet")
        print(f"âš¡ å…¶ä»–ä¼˜åŒ–æ¶æ„:")
        print(f"  - optimized_large_kernel: ä¼˜åŒ–å¤§æ ¸å·ç§¯ (é€‚åˆ60-120ä¸ªä¿¡å·)")
        print(f"  - optimized_dilated: ä¼˜åŒ–è†¨èƒ€å·ç§¯ (é€‚åˆ>120ä¸ªä¿¡å·)")
        print(f"  - large_kernel: åŸå§‹å¤§æ ¸ç‰¹å¾æå–å™¨")
        print(f"  - inception: Inceptionç»“æ„ç‰¹å¾æå–å™¨")
        print(f"  - dilated: è†¨èƒ€å·ç§¯ç‰¹å¾æå–å™¨")
        print(f"  - multiscale: å¤šå°ºåº¦ç‰¹å¾æå–å™¨")
        print(f"ğŸ¤– æ™ºèƒ½é€‰æ‹©:")
        print(f"  - auto: æ ¹æ®ä¿¡å·æ•°é‡è‡ªåŠ¨é€‰æ‹© (æ¨èGAFä¼˜åŒ–ç‰ˆæœ¬)")

    def _build_extractors(self):
        """æ„å»ºç‰¹å¾æå–å™¨"""
        if self.use_grouping:
            # ä½¿ç”¨åˆ†ç»„ç‰¹å¾æå–
            self.channel_to_group = self._create_channel_mapping()
            self.num_groups = len(self.hvac_groups)

            # ä¸ºæ¯ä¸ªåˆ†æ”¯çš„æ¯ä¸ªç»„åˆ›å»ºç‰¹å¾æå–å™¨
            self.sum_extractors = nn.ModuleDict()
            if self.use_diff_branch:
                self.diff_extractors = nn.ModuleDict()

            for group_idx in range(self.num_groups):
                self.sum_extractors[f'group_{group_idx}'] = self._create_extractor(
                )
                if self.use_diff_branch:
                    self.diff_extractors[f'group_{group_idx}'] = self._create_extractor(
                    )

            # å¦‚æœæœ‰æœªåˆ†ç»„çš„é€šé“ï¼Œåˆ›å»ºé»˜è®¤ç‰¹å¾æå–å™¨
            if -1 in self.channel_to_group.values():
                self.sum_extractors['default'] = self._create_extractor()
                if self.use_diff_branch:
                    self.diff_extractors['default'] = self._create_extractor()

            if self.use_diff_branch:
                print(f"åˆ›å»ºäº† {len(self.sum_extractors)} ä¸ªåˆ†ç»„ç‰¹å¾æå–å™¨ï¼ˆsum + diffåˆ†æ”¯ï¼‰")
            else:
                print(f"åˆ›å»ºäº† {len(self.sum_extractors)} ä¸ªåˆ†ç»„ç‰¹å¾æå–å™¨ï¼ˆä»…sumåˆ†æ”¯ï¼‰")
        else:
            # ä¸ä½¿ç”¨åˆ†ç»„ï¼Œæ‰€æœ‰é€šé“å…±ç”¨ç‰¹å¾æå–å™¨
            self.sum_extractor = self._create_extractor()
            # if self.use_diff_branch:
            #     self.diff_extractor = self._create_extractor()
            self.diff_extractor = self._create_extractor()
            if self.use_diff_branch:
                print("åˆ›å»ºäº†ç»Ÿä¸€çš„ç‰¹å¾æå–å™¨ï¼ˆsum + diffåˆ†æ”¯ï¼‰")
            else:
                print("åˆ›å»ºäº†ç»Ÿä¸€çš„ç‰¹å¾æå–å™¨ï¼ˆä»…sumåˆ†æ”¯ï¼‰")

    def _create_extractor(self):
        """æ ¹æ®é…ç½®åˆ›å»ºç‰¹å¾æå–å™¨"""
        if self.extractor_type == 'large_kernel':
            return NoPaddingLargeKernelFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'inception':
            return InceptionFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'dilated':
            return LargeKernelDilatedFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'multiscale':
            return MultiScaleStackedFeatureExtractor(self.feature_dim)
        # åŸå§‹ResNetæ¶æ„ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰
        elif self.extractor_type == 'resnet18':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet18')
        elif self.extractor_type == 'resnet34':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet34')
        elif self.extractor_type == 'resnet_light':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet_light')
        # æ–°çš„GAFä¼˜åŒ–ResNetæ¶æ„
        elif self.extractor_type == 'resnet18_gaf':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet18_gaf')
        elif self.extractor_type == 'resnet18_gaf_light':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet18_gaf_light')
        elif self.extractor_type == 'resnet_gaf_deep':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet_gaf_deep')
        elif self.extractor_type == 'resnet_gaf_preserve':
            return ResNetFeatureExtractor(self.feature_dim, depth='resnet_gaf_preserve')
        # å…¶ä»–ä¼˜åŒ–çš„ç‰¹å¾æå–å™¨
        elif self.extractor_type == 'optimized_large_kernel':
            return OptimizedLargeKernelFeatureExtractor(self.feature_dim)
        elif self.extractor_type == 'optimized_dilated':
            return OptimizedDilatedFeatureExtractor(self.feature_dim)
        else:
            raise ValueError(f"Unknown extractor type: {self.extractor_type}")

    def _get_recommended_extractor_for_signal_count(self, signal_count):
        """æ ¹æ®ä¿¡å·æ•°é‡æ¨èç‰¹å¾æå–å™¨"""
        if signal_count <= 30:
            return 'resnet18_gaf'  # å°æ•°æ®é›†ä½¿ç”¨GAFä¼˜åŒ–ResNet18
        elif signal_count <= 60:
            return 'resnet18_gaf_light'  # ä¸­ç­‰æ•°æ®é›†ä½¿ç”¨è½»é‡çº§GAF ResNet
        elif signal_count <= 120:
            return 'optimized_large_kernel'  # å¤§æ•°æ®é›†ä½¿ç”¨ä¼˜åŒ–çš„è½»é‡çº§æå–å™¨
        else:
            return 'optimized_dilated'  # è¶…å¤§æ•°æ®é›†ä½¿ç”¨è†¨èƒ€å·ç§¯å‡å°‘å‚æ•°

    def _create_channel_mapping(self):
        """åˆ›å»ºé€šé“ç´¢å¼•åˆ°ç»„çš„æ˜ å°„"""
        channel_to_group = {}

        if not self.feature_columns:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾åˆ—ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
            for i in range(self.num_images):
                channel_to_group[i] = 0
            return channel_to_group

        # ä¸ºæ¯ä¸ªé€šé“æ‰¾åˆ°å¯¹åº”çš„ç»„
        for channel_idx, column_name in enumerate(self.feature_columns):
            group_found = False

            for group_idx, group_signals in enumerate(self.hvac_groups):
                if any(signal in column_name.upper() for signal in [s.upper() for s in group_signals]):
                    channel_to_group[channel_idx] = group_idx
                    group_found = True
                    break

            if not group_found:
                channel_to_group[channel_idx] = -1

        return channel_to_group

    def _build_fusion_module(self):
        """æ„å»ºç‰¹å¾èåˆæ¨¡å—"""
        if self.fusion_type == 'adaptive':
            self.fusion = AdaptiveFusion(self.feature_dim)
        elif self.fusion_type == 'concat':
            self.fusion = ConcatFusion(self.feature_dim)
        elif self.fusion_type == 'bidirectional':
            self.fusion = BiDirectionalFusion(self.feature_dim, num_heads=4)
        elif self.fusion_type == 'gated':
            self.fusion = GatedFusion(self.feature_dim)
        elif self.fusion_type in ['add', 'mul', 'weighted_add']:
            self.fusion = ElementwiseFusion(self.feature_dim, self.fusion_type)
        else:
            raise ValueError(f"Unknown fusion type: {self.fusion_type}")

    def _build_statistical_extractor(self):
        """æ„å»ºç»Ÿè®¡ç‰¹å¾æå–å™¨"""
        self.statistical_extractor = TimeSeriesStatisticalExtractor(
            num_signals=self.num_images,
            time_length=self.time_length,
            feature_dim=self.feature_dim,
            stat_type=self.stat_type
        )

    def _build_multimodal_fusion(self):
        """æ„å»ºå¤šæ¨¡æ€èåˆæ¨¡å—"""
        self.multimodal_fusion = MultiModalFusion(
            feature_dim=self.feature_dim,
            fusion_strategy=self.multimodal_fusion_strategy
        )

    def _build_signal_level_statistical_extractor(self):
        """æ„å»ºä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾æå–å™¨"""
        self.signal_level_statistical_extractor = SignalLevelStatisticalExtractor(
            num_signals=self.num_images,
            time_length=self.time_length,
            stat_feature_dim=self.signal_stat_feature_dim,
            stat_type=self.signal_stat_type
        )

    def _build_signal_level_statistical_fusion(self):
        """æ„å»ºä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾èåˆæ¨¡å—"""
        self.signal_level_statistical_fusion = SignalLevelStatisticalFusion(
            gaf_feature_dim=self.feature_dim,
            stat_feature_dim=self.signal_stat_feature_dim,
            output_feature_dim=self.feature_dim,
            fusion_strategy=self.signal_stat_fusion_strategy
        )

    def _build_attention_module(self):
        """æ„å»ºæ³¨æ„åŠ›æ¨¡å—"""
        if self.attention_type == 'none':
            self.attention = nn.Identity()
        else:
            # ç¡®å®šæ³¨æ„åŠ›æ¨¡å—åº”è¯¥ä½¿ç”¨çš„ä¿¡å·æ•°é‡
            if self.use_channel_compression:
                if hasattr(self, 'compressed_num_images'):
                    # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„å‹ç¼©åé€šé“æ•°
                    num_signals = self.compressed_num_images
                elif self.compression_channels is not None:
                    # ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„å‹ç¼©é€šé“æ•°
                    num_signals = self.compression_channels
                else:
                    # ä½¿ç”¨å‹ç¼©æ¯”ä¾‹è®¡ç®—
                    num_signals = max(1, int(self.num_images * self.compression_ratio))
            else:
                # æ²¡æœ‰å‹ç¼©ï¼Œä½¿ç”¨åŸå§‹é€šé“æ•°
                num_signals = self.num_images
                
            self.attention = SignalAttention(
                self.feature_dim,
                num_signals,
                self.attention_type
            )

    def _build_channel_compression_module(self):
        """æ„å»ºé€šé“å‹ç¼©æ¨¡å—"""
        if self.compression_strategy == 'adaptive':
            # è‡ªé€‚åº”å‹ç¼©
            min_output_channels = max(1, int(self.num_images * min(self.adaptive_compression_ratios)))
            self.channel_compressor = AdaptiveChannelCompressionModule(
                input_channels=self.num_images,
                min_output_channels=min_output_channels,
                feature_dim=self.feature_dim,
                compression_ratios=self.adaptive_compression_ratios
            )
            # æ›´æ–°å‹ç¼©åçš„é€šé“æ•°ï¼ˆå–æœ€å°å€¼ï¼‰
            self.compressed_num_images = min_output_channels
            
        elif self.compression_strategy == 'hvac_grouped':
            # HVACåˆ†ç»„å‹ç¼©
            self.channel_compressor = HVACSignalGroupCompressionModule(
                input_channels=self.num_images,
                feature_dim=self.feature_dim,
                hvac_groups=self.hvac_groups,
                feature_columns=self.feature_columns,
                group_compression_ratios=self.hvac_group_compression_ratios
            )
            # æ›´æ–°å‹ç¼©åçš„é€šé“æ•°
            self.compressed_num_images = self.channel_compressor.total_output_channels
        elif self.compression_strategy == 'signal_compression':
            output_channels = max(1, int(self.num_images * self.compression_ratio))
            self.channel_compressor = SignalCompressionModule(
                input_channels=self.num_images,
                output_channels=output_channels,
                feature_dim=self.feature_dim
            )
            self.compressed_num_images = output_channels
        else:
            # æ ‡å‡†å‹ç¼©ç­–ç•¥
            output_channels = max(1, int(self.num_images * self.compression_ratio))
            self.channel_compressor = ChannelCompressionModule(
                input_channels=self.num_images,
                output_channels=output_channels,
                feature_dim=self.feature_dim,
                compression_strategy=self.compression_strategy
            )
            # æ›´æ–°å‹ç¼©åçš„é€šé“æ•°
            self.compressed_num_images = output_channels
            
        print(f"é€šé“å‹ç¼©é…ç½®:")
        print(f"  - å‹ç¼©å‰é€šé“æ•°: {self.num_images}")
        print(f"  - å‹ç¼©åé€šé“æ•°: {self.compressed_num_images}")
        print(f"  - å®é™…å‹ç¼©æ¯”ä¾‹: {self.compressed_num_images / self.num_images:.2f}")
        print(f"  - å‚æ•°é‡å‡å°‘ä¼°è®¡: {1 - (self.compressed_num_images / self.num_images):.1%}")

    def _build_classifier(self, num_classes):
        """æ„å»ºåˆ†ç±»å™¨"""
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨é€šé“å‹ç¼©å†³å®šæœ€ç»ˆç‰¹å¾ç»´åº¦
        if self.use_channel_compression:
            final_feature_dim = self.feature_dim * self.compressed_num_images
        else:
            final_feature_dim = self.feature_dim * self.num_images

        # ğŸ”¥ æ£€æµ‹é«˜ç»´ç‰¹å¾å¹¶è‡ªåŠ¨é€‰æ‹©å‚æ•°é«˜æ•ˆçš„åˆ†ç±»å™¨
        # if final_feature_dim > 2048:
        #     print(f"âš ï¸ æ£€æµ‹åˆ°é«˜ç»´ç‰¹å¾ ({final_feature_dim}ç»´)ï¼Œè‡ªåŠ¨å¯ç”¨å‚æ•°é«˜æ•ˆåˆ†ç±»å™¨")
            
        #     # æ ¹æ®ä¿¡å·æ•°é‡å’Œç‰¹å¾ç»´åº¦æ™ºèƒ½é€‰æ‹©æœ€ä¼˜åˆ†ç±»å™¨
        #     num_signals = self.compressed_num_images if self.use_channel_compression else self.num_images
            
        #     if num_signals >= 100 and self.feature_dim >= 128:
        #         # è¶…å¤§é‡ä¿¡å· + è¶…é«˜ç»´ç‰¹å¾ -> åˆ†å±‚å‹ç¼©ï¼ˆæœ€æ¿€è¿›ï¼‰
        #         if self.classifier_type in ['mlp', 'simple']:
        #             print(f"   è‡ªåŠ¨åˆ‡æ¢: {self.classifier_type} -> hierarchical (è¶…å¤§è§„æ¨¡æ•°æ®)")
        #             self.classifier_type = 'hierarchical'
        #     elif num_signals >= 30 and self.feature_dim >= 64:
        #         # ä¸­å¤§é‡ä¿¡å· + é«˜ç»´ç‰¹å¾ -> ç‰¹å¾å‹ç¼©ï¼ˆæ¸©å’Œæ–¹æ¡ˆï¼‰
        #         if self.classifier_type in ['mlp', 'simple']:
        #             print(f"   è‡ªåŠ¨åˆ‡æ¢: {self.classifier_type} -> feature_compression (å¹³è¡¡æ–¹æ¡ˆ)")
        #             self.classifier_type = 'feature_compression'
        #     elif self.classifier_type == 'mlp':
        #         print(f"   è‡ªåŠ¨åˆ‡æ¢: mlp -> efficient_mlp")
        #         self.classifier_type = 'efficient_mlp'
        #     elif self.classifier_type == 'simple':
        #         print(f"   è‡ªåŠ¨åˆ‡æ¢: simple -> efficient_simple")
        #         self.classifier_type = 'efficient_simple'

        if self.classifier_type == 'mlp':
            self.classifier = nn.Sequential(
                nn.Linear(final_feature_dim, 2048),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(2048, 512),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(512, 128),
                nn.ReLU(),
                nn.Linear(128, num_classes),
            )
        elif self.classifier_type == 'simple':
            self.classifier = nn.Sequential(
                nn.Linear(final_feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(512, num_classes),
            )
        elif self.classifier_type == 'efficient_mlp':
            # ğŸš€ å‚æ•°é«˜æ•ˆçš„MLPåˆ†ç±»å™¨
            self.classifier = EfficientClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                reduction_method='pooling_projection',  # å…ˆæ± åŒ–å†æŠ•å½±
                intermediate_dim=512,
                classifier_type='mlp'
            )
            print(f"ğŸš€ æ„å»ºå‚æ•°é«˜æ•ˆMLPåˆ†ç±»å™¨: {final_feature_dim} -> æ± åŒ–+æŠ•å½± -> 512 -> {num_classes}")
            
        elif self.classifier_type == 'efficient_simple':
            # ğŸš€ å‚æ•°é«˜æ•ˆçš„ç®€å•åˆ†ç±»å™¨
            self.classifier = EfficientClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                reduction_method='attention_pooling',  # æ³¨æ„åŠ›æ± åŒ–
                intermediate_dim=256,
                classifier_type='simple'
            )
            print(f"ğŸš€ æ„å»ºå‚æ•°é«˜æ•ˆç®€å•åˆ†ç±»å™¨: {final_feature_dim} -> æ³¨æ„åŠ›æ± åŒ– -> 256 -> {num_classes}")
            
        elif self.classifier_type == 'global_pooling':
            # ğŸ¯ å…¨å±€æ± åŒ–åˆ†ç±»å™¨ï¼ˆæœ€å‚æ•°é«˜æ•ˆï¼‰
            self.classifier = GlobalPoolingClassifier(
                input_feature_dim=self.feature_dim,
                num_signals=self.compressed_num_images if self.use_channel_compression else self.num_images,
                num_classes=num_classes,
                pooling_type='adaptive'  # 'mean', 'max', 'attention', 'adaptive'
            )
            print(f"ğŸ¯ æ„å»ºå…¨å±€æ± åŒ–åˆ†ç±»å™¨: ({self.feature_dim}, {self.num_images}) -> è‡ªé€‚åº”æ± åŒ– -> {num_classes}")
            
        elif self.classifier_type == 'conv1d':
            # ğŸ”¥ 1Då·ç§¯åˆ†ç±»å™¨ï¼ˆå¤„ç†åºåˆ—ç‰¹å¾ï¼‰
            self.classifier = Conv1DClassifier(
                input_feature_dim=self.feature_dim,
                num_signals=self.compressed_num_images if self.use_channel_compression else self.num_images,
                num_classes=num_classes,
                hidden_channels=[256, 128, 64]
            )
            print(f"ğŸ”¥ æ„å»º1Då·ç§¯åˆ†ç±»å™¨: ({self.feature_dim}, {self.num_images}) -> Conv1D -> {num_classes}")
            
        elif self.classifier_type == 'separable':
            # âš¡ åˆ†ç¦»å¼åˆ†ç±»å™¨ï¼ˆå…ˆåˆ†åˆ«å¤„ç†å†èåˆï¼‰
            self.classifier = SeparableClassifier(
                input_feature_dim=self.feature_dim,
                num_signals=self.compressed_num_images if self.use_channel_compression else self.num_images,
                num_classes=num_classes,
                fusion_method='attention'  # 'mean', 'max', 'attention', 'weighted'
            )
            print(f"âš¡ æ„å»ºåˆ†ç¦»å¼åˆ†ç±»å™¨: æ¯ä¿¡å·ç‹¬ç«‹åˆ†ç±» -> æ³¨æ„åŠ›èåˆ -> {num_classes}")
            
        elif self.classifier_type == 'feature_compression':
            # ğŸ¯ ç‰¹å¾å‹ç¼©åˆ†ç±»å™¨ï¼ˆä¸­é—´ç‰ˆæœ¬ï¼šåªå‹ç¼©ç‰¹å¾ç»´åº¦ï¼‰
            num_signals_for_compression = self.compressed_num_images if self.use_channel_compression else self.num_images
            
            # è‡ªé€‚åº”å‹ç¼©ç‰¹å¾ç»´åº¦
            if self.feature_dim >= 128:
                compressed_feature_dim = 32
            elif self.feature_dim >= 64:
                compressed_feature_dim = 16
            else:
                compressed_feature_dim = max(8, self.feature_dim // 4)
            
            self.classifier = FeatureCompressionClassifier(
                input_feature_dim=self.feature_dim,
                num_signals=num_signals_for_compression,
                num_classes=num_classes,
                compressed_feature_dim=compressed_feature_dim
            )
            print(f"ğŸ¯ æ„å»ºç‰¹å¾å‹ç¼©åˆ†ç±»å™¨: ç‰¹å¾{self.feature_dim}â†’{compressed_feature_dim}, ä¿¡å·{num_signals_for_compression}(ä¿æŒä¸å˜) -> {num_classes}")
            
        elif self.classifier_type == 'hierarchical':
            # ğŸ¯ åˆ†å±‚å‹ç¼©åˆ†ç±»å™¨ï¼ˆtest.pyæ–¹æ¡ˆé›†æˆç‰ˆï¼‰
            num_signals_for_compression = self.compressed_num_images if self.use_channel_compression else self.num_images
            
            # è‡ªé€‚åº”å‹ç¼©å‚æ•°
            if self.feature_dim >= 128:
                compressed_feature_dim = 32
            elif self.feature_dim >= 64:
                compressed_feature_dim = 16
            else:
                compressed_feature_dim = max(8, self.feature_dim // 4)
            
            # ä¿¡å·å‹ç¼©æ¯”ä¾‹ï¼šä¿¡å·è¶Šå¤šï¼Œå‹ç¼©è¶Šå‰å®³
            if num_signals_for_compression >= 100:
                compression_ratio = 0.4  # å‹ç¼©åˆ°40%
            elif num_signals_for_compression >= 50:
                compression_ratio = 0.5  # å‹ç¼©åˆ°50%
            elif num_signals_for_compression >= 20:
                compression_ratio = 0.6  # å‹ç¼©åˆ°60%
            else:
                compression_ratio = 0.8  # å‹ç¼©åˆ°80%
            
            self.classifier = HierarchicalCompressionClassifier(
                input_feature_dim=self.feature_dim,
                num_signals=num_signals_for_compression,
                num_classes=num_classes,
                compressed_feature_dim=compressed_feature_dim,
                compressed_signals=None,  # è‡ªåŠ¨è®¡ç®—
                intermediate_dim=256,
                compression_ratio=compression_ratio
            )
            print(f"ğŸ¯ æ„å»ºåˆ†å±‚å‹ç¼©åˆ†ç±»å™¨: ç‰¹å¾{self.feature_dim}â†’{compressed_feature_dim}, ä¿¡å·{num_signals_for_compression}â†’{int(num_signals_for_compression*compression_ratio)} -> {num_classes}")
            
            # æ˜¾ç¤ºå‹ç¼©ç»Ÿè®¡
            if hasattr(self.classifier, 'get_compression_stats'):
                stats = self.classifier.get_compression_stats()
                print(f"   ğŸ”¸ ç‰¹å¾å‹ç¼©æ¯”: {stats['feature_compression_ratio']:.3f}")
                print(f"   ğŸ”¸ ä¿¡å·å‹ç¼©æ¯”: {stats['signal_compression_ratio']:.3f}")
                print(f"   ğŸ”¸ æ•´ä½“å‹ç¼©æ¯”: {stats['overall_compression']:.3f}")
                print(f"   ğŸ”¸ åŸå§‹ç»´åº¦: {stats['original_dim']} â†’ å‹ç¼©ç»´åº¦: {stats['compressed_dim']}")
            
        elif self.classifier_type == 'residual':
            # åŸºç¡€æ®‹å·®åˆ†ç±»å™¨
            self.classifier = ResidualClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                hidden_dims=None,
                block_type='basic',
                dropout=0.15,
                use_batch_norm=False
            )
            print(
                f"ğŸ—ï¸ æ„å»ºæ®‹å·®åˆ†ç±»å™¨ (åŸºç¡€æ®‹å·®å—): {final_feature_dim} -> [1024, 512, 256] -> {num_classes}")
        elif self.classifier_type == 'residual_bottleneck':
            # ç“¶é¢ˆæ®‹å·®åˆ†ç±»å™¨ï¼ˆé€‚ç”¨äºé«˜ç»´ç‰¹å¾ï¼‰
            self.classifier = ResidualClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                hidden_dims=[2048, 1024, 512] if final_feature_dim > 1000 else [
                    1024, 512, 256],
                block_type='bottleneck',
                dropout=0.15,
                use_batch_norm=True
            )
            hidden_info = "[2048, 1024, 512]" if final_feature_dim > 1000 else "[1024, 512, 256]"
            print(
                f"ğŸ—ï¸ æ„å»ºç“¶é¢ˆæ®‹å·®åˆ†ç±»å™¨ (å¸¦BatchNorm): {final_feature_dim} -> {hidden_info} -> {num_classes}")
        elif self.classifier_type == 'residual_dense':
            # å¯†é›†æ®‹å·®åˆ†ç±»å™¨ï¼ˆæœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼‰
            self.classifier = ResidualClassifier(
                input_dim=final_feature_dim,
                num_classes=num_classes,
                hidden_dims=[1024, 512, 256, 128],
                block_type='dense',
                dropout=0.2,
                use_batch_norm=True
            )
            print(
                f"ğŸ—ï¸ æ„å»ºå¯†é›†æ®‹å·®åˆ†ç±»å™¨ (æœ€å¼ºè¡¨è¾¾èƒ½åŠ›): {final_feature_dim} -> [1024, 512, 256, 128] -> {num_classes}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {self.classifier_type}")

        # è®¡ç®—å¹¶æ˜¾ç¤ºåˆ†ç±»å™¨å‚æ•°é‡
        if hasattr(self.classifier, 'parameters'):
            classifier_params = sum(p.numel() for p in self.classifier.parameters())
            print(f"âœ… åˆ†ç±»å™¨æ„å»ºå®Œæˆ: ç±»å‹={self.classifier_type}, è¾“å…¥ç»´åº¦={final_feature_dim}, è¾“å‡ºç±»åˆ«={num_classes}")
            print(f"   åˆ†ç±»å™¨å‚æ•°é‡: {classifier_params:,} ({classifier_params/1000:.1f}K)")
        else:
            print(f"âœ… åˆ†ç±»å™¨æ„å»ºå®Œæˆ: ç±»å‹={self.classifier_type}, è¾“å…¥ç»´åº¦={final_feature_dim}, è¾“å‡ºç±»åˆ«={num_classes}")
    
    def print_model_structure(self, input_shape=None, detailed=True):
        """
        æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
        
        Args:
            input_shape: tuple, è¾“å…¥æ•°æ®å½¢çŠ¶ (B, C, H, W)ï¼Œç”¨äºè®¡ç®—è¯¦ç»†ä¿¡æ¯
            detailed: bool, æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„å‚æ•°ä¿¡æ¯
        """
        print("\n" + "="*80)
        print("ğŸ—ï¸ DualGAFNet æ¨¡å‹ç»“æ„")
        print("="*80)
        
        # 1. æ¨¡å‹åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“Š æ¨¡å‹é…ç½®:")
        print(f"   ä¿¡å·æ•°é‡: {self.num_images}")
        print(f"   ç‰¹å¾ç»´åº¦: {self.feature_dim}")
        print(f"   ç‰¹å¾æå–å™¨: {self.extractor_type}")
        print(f"   èåˆç±»å‹: {self.fusion_type}")
        print(f"   æ³¨æ„åŠ›ç±»å‹: {self.attention_type}")
        print(f"   åˆ†ç±»å™¨ç±»å‹: {self.classifier_type}")
        print(f"   ç»Ÿè®¡ç‰¹å¾: {self.use_statistical_features}")
        print(f"   ä¿¡å·çº§ç»Ÿè®¡: {self.use_signal_level_stats}")
        if hasattr(self, 'use_channel_compression'):
            print(f"   é€šé“å‹ç¼©: {self.use_channel_compression}")
            if self.use_channel_compression:
                print(f"     å‹ç¼©ç­–ç•¥: {self.compression_strategy}")
                print(f"     å‹ç¼©æ¯”ä¾‹: {self.compression_ratio}")
                if hasattr(self, 'compressed_num_images'):
                    print(f"     å‹ç¼©åé€šé“: {self.compressed_num_images}")
        
        # 2. è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\nğŸ“ˆ å‚æ•°ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°é‡: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.2f} MB")
        
        # 3. å„æ¨¡å—å‚æ•°åˆ†å¸ƒ
        if detailed:
            print(f"\nğŸ” å„æ¨¡å—å‚æ•°åˆ†å¸ƒ:")
            
            # ç‰¹å¾æå–å™¨
            if hasattr(self, 'sum_extractor'):
                sum_params = sum(p.numel() for p in self.sum_extractor.parameters())
                print(f"   Sumç‰¹å¾æå–å™¨: {sum_params:,} ({sum_params/total_params*100:.1f}%)")
            
            if hasattr(self, 'diff_extractor'):
                diff_params = sum(p.numel() for p in self.diff_extractor.parameters())
                print(f"   Diffç‰¹å¾æå–å™¨: {diff_params:,} ({diff_params/total_params*100:.1f}%)")
            
            # èåˆæ¨¡å—
            if hasattr(self, 'fusion'):
                fusion_params = sum(p.numel() for p in self.fusion.parameters())
                print(f"   èåˆæ¨¡å—: {fusion_params:,} ({fusion_params/total_params*100:.1f}%)")
            
            # ç»Ÿè®¡ç‰¹å¾
            if hasattr(self, 'statistical_extractor'):
                stat_params = sum(p.numel() for p in self.statistical_extractor.parameters())
                print(f"   ç»Ÿè®¡ç‰¹å¾æå–å™¨: {stat_params:,} ({stat_params/total_params*100:.1f}%)")
            
            if hasattr(self, 'multimodal_fusion'):
                mm_params = sum(p.numel() for p in self.multimodal_fusion.parameters())
                print(f"   å¤šæ¨¡æ€èåˆ: {mm_params:,} ({mm_params/total_params*100:.1f}%)")
            
            # é€šé“å‹ç¼©
            if hasattr(self, 'channel_compressor') and self.channel_compressor is not None:
                comp_params = sum(p.numel() for p in self.channel_compressor.parameters())
                print(f"   é€šé“å‹ç¼©: {comp_params:,} ({comp_params/total_params*100:.1f}%)")
            
            # æ³¨æ„åŠ›æ¨¡å—
            if hasattr(self, 'attention'):
                att_params = sum(p.numel() for p in self.attention.parameters())
                print(f"   æ³¨æ„åŠ›æ¨¡å—: {att_params:,} ({att_params/total_params*100:.1f}%)")
            
            # åˆ†ç±»å™¨
            if hasattr(self, 'classifier'):
                cls_params = sum(p.numel() for p in self.classifier.parameters())
                print(f"   ä¸»åˆ†ç±»å™¨: {cls_params:,} ({cls_params/total_params*100:.1f}%)")
        
        # 4. æ¨¡å—å±‚æ¬¡ç»“æ„
        print(f"\nğŸ›ï¸ æ¨¡å‹å±‚æ¬¡ç»“æ„:")
        for name, module in self.named_children():
            module_params = sum(p.numel() for p in module.parameters())
            print(f"   {name}: {type(module).__name__} ({module_params:,} å‚æ•°)")
        
        # 5. å¦‚æœæä¾›äº†è¾“å…¥å½¢çŠ¶ï¼Œè®¡ç®—å„å±‚è¾“å‡ºå½¢çŠ¶
        if input_shape is not None:
            self._print_layer_shapes(input_shape)
        
        print("="*80)
    
    def _print_layer_shapes(self, input_shape):
        """
        æ‰“å°å„å±‚çš„è¾“å‡ºå½¢çŠ¶ï¼ˆéœ€è¦å®é™…å‰å‘ä¼ æ’­ï¼‰
        
        Args:
            input_shape: tuple, (B, C, H, W)
        """
        print(f"\nğŸ“ å±‚è¾“å‡ºå½¢çŠ¶åˆ†æ (è¾“å…¥: {input_shape}):")
        
        try:
            device = next(self.parameters()).device
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            B, C, H, W = input_shape
            sum_x = torch.randn(B, C, H, W).to(device)
            diff_x = torch.randn(B, C, H, W).to(device)
            
            # å¦‚æœä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ï¼Œåˆ›å»ºæ—¶åºæ•°æ®
            if self.use_statistical_features:
                time_series_x = torch.randn(B, H, C).to(device)  # å‡è®¾æ—¶åºé•¿åº¦ç­‰äºH
            else:
                time_series_x = None
            
            self.eval()
            with torch.no_grad():
                # è¿™é‡Œå¯ä»¥æ·»åŠ hookæ¥æ•è·ä¸­é—´å±‚è¾“å‡º
                print(f"   è¾“å…¥ - Sum GAF: {sum_x.shape}")
                print(f"   è¾“å…¥ - Diff GAF: {diff_x.shape}")
                if time_series_x is not None:
                    print(f"   è¾“å…¥ - æ—¶åºæ•°æ®: {time_series_x.shape}")
                
                # å‰å‘ä¼ æ’­å¹¶è·å–è¾“å‡ºå½¢çŠ¶
                if time_series_x is not None:
                    output = self(sum_x, diff_x, time_series_x)
                else:
                    output = self(sum_x, diff_x)
                
                if isinstance(output, tuple):
                    print(f"   è¾“å‡º - ä¸»è¾“å‡º: {output[0].shape}")
                    print(f"   è¾“å‡º - è¾…åŠ©è¾“å‡º: {output[1].shape}")
                else:
                    print(f"   è¾“å‡º: {output.shape}")
                    
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•è®¡ç®—å±‚å½¢çŠ¶: {str(e)}")
    
    def get_model_summary(self):
        """
        è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        
        Returns:
            dict: åŒ…å«æ¨¡å‹æ‘˜è¦ä¿¡æ¯çš„å­—å…¸
        """
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        summary = {
            'model_name': 'DualGAFNet',
            'signal_count': self.num_images,
            'feature_dim': self.feature_dim,
            'extractor_type': self.extractor_type,
            'fusion_type': self.fusion_type,
            'attention_type': self.attention_type,
            'classifier_type': self.classifier_type,
            'use_statistical_features': self.use_statistical_features,
            'use_signal_level_stats': self.use_signal_level_stats,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024**2),
            'compression_enabled': getattr(self, 'use_channel_compression', False)
        }
        
        if hasattr(self, 'use_channel_compression') and self.use_channel_compression:
            summary.update({
                'compression_strategy': self.compression_strategy,
                'compression_ratio': self.compression_ratio,
                'compressed_channels': getattr(self, 'compressed_num_images', 'unknown')
            })
        
        return summary

    def _extract_features(self, x, extractors_dict, default_extractor):
        """æå–ç‰¹å¾çš„é€šç”¨æ–¹æ³•"""
        B, C, H, W = x.shape

        if self.use_grouping:
            # ä½¿ç”¨åˆ†ç»„ç‰¹å¾æå–
            feats_list = []

            # æŒ‰ç»„æ‰¹é‡å¤„ç†
            for group_idx in range(self.num_groups):
                group_channels = [
                    ch for ch, g in self.channel_to_group.items() if g == group_idx]

                if group_channels:
                    group_x = x[:, group_channels, :, :]
                    group_x = group_x.view(B * len(group_channels), 1, H, W)

                    extractor = extractors_dict[f'group_{group_idx}']
                    group_feats = extractor(group_x)
                    group_feats = group_feats.view(B, len(group_channels), -1)

                    for i, ch in enumerate(group_channels):
                        feats_list.append((ch, group_feats[:, i, :]))

            # å¤„ç†æœªåˆ†ç»„çš„é€šé“
            ungrouped_channels = [
                ch for ch, g in self.channel_to_group.items() if g == -1]
            if ungrouped_channels and 'default' in extractors_dict:
                for ch in ungrouped_channels:
                    channel_x = x[:, ch:ch+1, :, :]
                    channel_feat = extractors_dict['default'](channel_x)
                    feats_list.append((ch, channel_feat))

            # æŒ‰é€šé“ç´¢å¼•æ’åºå¹¶å †å 
            feats_list.sort(key=lambda x: x[0])
            feats = torch.stack([feat for _, feat in feats_list], dim=1)
        else:
            # ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾æå–å™¨
            x = x.view(B * C, 1, H, W)
            feats = default_extractor(x)
            feats = feats.view(B, C, -1)

        return feats

    def forward(self, sum_x, diff_x, time_series_x=None):
        """å‰å‘ä¼ æ’­

        Args:
            sum_x: [B, C, H, W] Summation GAFå›¾åƒ
            diff_x: [B, C, H, W] Difference GAFå›¾åƒï¼ˆæ¶ˆèå®éªŒæ—¶å¯å¿½ç•¥ï¼‰
            time_series_x: [B, C, T] æˆ– [B, T, C] åŸå§‹æ—¶åºæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        B, C, H, W = sum_x.shape
        if self.use_diff_branch:
            assert sum_x.shape == diff_x.shape, "ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å…¥å½¢çŠ¶å¿…é¡»ç›¸åŒ"
        assert C == self.num_images, f"è¾“å…¥é€šé“æ•°åº”ä¸º{self.num_images}ï¼Œå®é™…ä¸º{C}"

        # å¦‚æœä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ä½†æ²¡æœ‰æä¾›æ—¶åºæ•°æ®ï¼ŒæŠ›å‡ºé”™è¯¯
        if self.use_statistical_features and time_series_x is None:
            raise ValueError("å¯ç”¨ç»Ÿè®¡ç‰¹å¾æ—¶å¿…é¡»æä¾›åŸå§‹æ—¶åºæ•°æ® time_series_x")

        # æå–sumåˆ†æ”¯ç‰¹å¾
        if self.use_grouping:
            sum_feats = self._extract_features(
                sum_x, self.sum_extractors, None)
        else:
            sum_feats = self._extract_features(sum_x, None, self.sum_extractor)

        # æ ‡å‡†åŒè·¯GAFï¼šæå–diffåˆ†æ”¯ç‰¹å¾å¹¶èåˆ
        if self.use_grouping:
            diff_feats = self._extract_features(
                diff_x, self.diff_extractors, None)
        else:
            diff_feats = self._extract_features(
                diff_x, None, self.diff_extractor)
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨diffåˆ†æ”¯å†³å®šç‰¹å¾èåˆç­–ç•¥
        if self.use_diff_branch:

            # ç‰¹å¾èåˆ
            fused_feats = []
            for i in range(C):
                fused_feat = self.fusion(
                    sum_feats[:, i, :], diff_feats[:, i, :])
                fused_feats.append(fused_feat)

            # æ‰€æœ‰èåˆç±»å‹ç°åœ¨éƒ½è¾“å‡ºæ ‡å‡†çš„feature_dimç»´åº¦
            fused_feats = torch.stack(
                fused_feats, dim=1)  # [B, C, feature_dim]
        else:
            # æ¶ˆèå®éªŒï¼šä»…ä½¿ç”¨sumåˆ†æ”¯ï¼Œä¸è¿›è¡Œèåˆ
            fused_feats = sum_feats  # [B, C, feature_dim]

        # é€šé“å‹ç¼©ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_channel_compression:
            if self.compression_strategy == 'signal_compression':
                # SignalCompressionModule è¿”å› (compressed, attention_weights)
                fused_feats, _ = self.channel_compressor(fused_feats)  # [B, compressed_C, feature_dim]
            else:
                # å…¶ä»–å‹ç¼©æ¨¡å—è¿”å›å•ä¸ªtensor
                fused_feats = self.channel_compressor(fused_feats)  # [B, compressed_C, feature_dim]

        # å¦‚æœä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ï¼Œè¿›è¡Œå¤šæ¨¡æ€èåˆ
        if self.use_statistical_features:
            # æå–ç»Ÿè®¡ç‰¹å¾
            stat_features = self.statistical_extractor(
                time_series_x)  # [B, feature_dim]

            # å¤šæ¨¡æ€èåˆ
            fused_feats = self.multimodal_fusion(
                fused_feats, stat_features)  # [B, C, feature_dim]

        # å¦‚æœä½¿ç”¨ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ï¼Œè¿›è¡Œä¿¡å·çº§èåˆ
        if self.use_signal_level_stats:
            # æå–ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾
            signal_stat_features = self.signal_level_statistical_extractor(
                time_series_x)  # [B, C, signal_stat_feature_dim]

            # ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾èåˆ
            fused_feats = self.signal_level_statistical_fusion(
                fused_feats, signal_stat_features)  # [B, C, feature_dim]
                
        # ä¿¡å·æ³¨æ„åŠ›ï¼ˆå¦‚æœattention_type='none'ï¼Œåˆ™ç›¸å½“äºIdentityï¼‰
        attended_feats = self.attention(fused_feats)  # [B, C, feature_dim]

        # å±•å¹³ç”¨äºåˆ†ç±»
        merged = attended_feats.reshape(B, -1)  # [B, (compressed_)C * feature_dim]

        # åˆ†ç±»
        out = self.classifier(merged)
        return out


class Model(nn.Module):
    """åŒè·¯GAFç½‘ç»œçš„åŒ…è£…ç±»"""

    def __init__(self, configs):
        super().__init__()
        self.configs = configs

        # ä»é…ç½®ä¸­è·å–å‚æ•°
        feature_dim = configs.feature_dim
        num_classes = configs.num_class
        num_images = configs.enc_in
        time_length = getattr(configs, 'seq_len', 96)  # æ—¶é—´åºåˆ—é•¿åº¦

        # è·å–æ¨¡å—é…ç½®
        extractor_type = getattr(configs, 'extractor_type', 'large_kernel')
        fusion_type = getattr(configs, 'fusion_type', 'adaptive')
        attention_type = getattr(configs, 'attention_type', 'channel')
        classifier_type = getattr(configs, 'classifier_type', 'mlp')

        # è·å–ç»Ÿè®¡ç‰¹å¾é…ç½®
        use_statistical_features = getattr(
            configs, 'use_statistical_features', True)
        stat_type = getattr(configs, 'stat_type', 'comprehensive')
        multimodal_fusion_strategy = getattr(
            configs, 'multimodal_fusion_strategy', 'concat')

        # è·å–ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾é…ç½®
        use_signal_level_stats = getattr(
            configs, 'use_signal_level_stats', False)
        signal_stat_type = getattr(
            configs, 'signal_stat_type', 'comprehensive')
        signal_stat_fusion_strategy = getattr(
            configs, 'signal_stat_fusion_strategy', 'concat_project')
        signal_stat_feature_dim = getattr(
            configs, 'signal_stat_feature_dim', 32)

        # è·å–æ¶ˆèå®éªŒé…ç½®
        use_diff_branch = getattr(configs, 'use_diff_branch', True)

        # è·å–é€šé“å‹ç¼©é…ç½®
        use_channel_compression = getattr(configs, 'use_channel_compression', False)
        compression_strategy = getattr(configs, 'compression_strategy', 'conv1d')
        compression_ratio = getattr(configs, 'compression_ratio', 0.7)
        adaptive_compression_ratios = getattr(configs, 'adaptive_compression_ratios', [0.5, 0.7, 0.8])
        hvac_group_compression_ratios = getattr(configs, 'hvac_group_compression_ratios', None)
        compression_channels = getattr(configs, 'compression_channels', None)
        # è·å–HVACä¿¡å·åˆ†ç»„é…ç½®
        hvac_groups = getattr(configs, 'hvac_groups', None)
        feature_columns = getattr(configs, 'feature_columns', None)

        self.model = DualGAFNet(
            feature_dim=feature_dim,
            num_classes=num_classes,
            num_images=num_images,
            time_length=time_length,
            extractor_type=extractor_type,
            fusion_type=fusion_type,
            attention_type=attention_type,
            classifier_type=classifier_type,
            use_statistical_features=use_statistical_features,
            stat_type=stat_type,
            multimodal_fusion_strategy=multimodal_fusion_strategy,
            use_signal_level_stats=use_signal_level_stats,
            signal_stat_type=signal_stat_type,
            signal_stat_fusion_strategy=signal_stat_fusion_strategy,
            signal_stat_feature_dim=signal_stat_feature_dim,
            use_diff_branch=use_diff_branch,
            use_channel_compression=use_channel_compression,
            compression_strategy=compression_strategy,
            compression_ratio=compression_ratio,
            compression_channels=compression_channels,
            adaptive_compression_ratios=adaptive_compression_ratios,
            hvac_group_compression_ratios=hvac_group_compression_ratios,
            hvac_groups=hvac_groups,
            feature_columns=feature_columns
        )

    def forward(self, sum_x, diff_x, time_series_x=None):
        return self.model(sum_x, diff_x, time_series_x)
    
    def print_model_structure(self, input_shape=None, detailed=True):
        """
        æ‰“å°æ¨¡å‹ç»“æ„çš„ä¾¿æ·æ–¹æ³•
        
        Args:
            input_shape: tuple, è¾“å…¥æ•°æ®å½¢çŠ¶ (B, C, H, W)
            detailed: bool, æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†å‚æ•°ä¿¡æ¯
        """
        return self.model.print_model_structure(input_shape, detailed)
    
    def get_model_summary(self):
        """
        è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯çš„ä¾¿æ·æ–¹æ³•
        
        Returns:
            dict: åŒ…å«æ¨¡å‹æ‘˜è¦ä¿¡æ¯çš„å­—å…¸
        """
        return self.model.get_model_summary()


if __name__ == "__main__":
    print("="*100)
    print("æµ‹è¯•åŒè·¯GAFç½‘ç»œ - é’ˆå¯¹96x96 GAFå›¾åƒä¼˜åŒ–")
    print("="*100)

    # æµ‹è¯•ä¸åŒä¿¡å·æ•°é‡å’Œç‰¹å¾æå–å™¨
    test_configs = [
        (26, 96, "resnet18", "SAHUæ•°æ®é›†"),
        (120, 96, "auto", "DAHUæ•°æ®é›†ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰"),
        (120, 96, "resnet_light", "DAHUæ•°æ®é›†ï¼ˆæ‰‹åŠ¨è½»é‡çº§ï¼‰"),
        (120, 96, "optimized_large_kernel", "DAHUæ•°æ®é›†ï¼ˆä¼˜åŒ–å¤§æ ¸ï¼‰"),
    ]

    for signal_count, gaf_size, extractor_type, dataset_name in test_configs:
        print(f"\n{'-'*60}")
        print(f"æµ‹è¯•é…ç½®: {dataset_name}")
        print(f"ä¿¡å·æ•°é‡: {signal_count}, GAFå°ºå¯¸: {gaf_size}x{gaf_size}")
        print(f"ç‰¹å¾æå–å™¨: {extractor_type}")
        print(f"{'-'*60}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        B = 2  # å°æ‰¹æ¬¡æµ‹è¯•
        C, H, W = signal_count, gaf_size, gaf_size
        sum_x = torch.randn(B, C, H, W)
        diff_x = torch.randn(B, C, H, W)
        time_series_x = torch.randn(B, gaf_size, C)  # æ—¶åºæ•°æ®

        # åˆ›å»ºé…ç½®
        configs = type("cfg", (), {
            "feature_dim": 64,  # ä½¿ç”¨è¾ƒå°çš„ç‰¹å¾ç»´åº¦è¿›è¡Œæµ‹è¯•
            "num_class": 6,     # DAHUæ•°æ®é›†æœ‰æ›´å¤šç±»åˆ«
            "enc_in": C,
            "seq_len": gaf_size,
            "extractor_type": extractor_type,
            "fusion_type": "adaptive",
            "attention_type": "channel",
            "classifier_type": "mlp",
            "use_statistical_features": True,
            "stat_type": "basic",  # ä½¿ç”¨åŸºç¡€ç»Ÿè®¡ç‰¹å¾å‡å°‘è®¡ç®—
            "multimodal_fusion_strategy": "concat",
            "hvac_groups": None,
            "feature_columns": None
        })()

        try:
            model = Model(configs)

            print(f"\nè¾“å…¥æ•°æ®:")
            print(f"  - Summation GAF: {sum_x.shape}")
            print(f"  - Difference GAF: {diff_x.shape}")
            print(f"  - Time Series: {time_series_x.shape}")

            # æµ‹è¯•forwardè¿‡ç¨‹
            out = model(sum_x, diff_x, time_series_x)
            print(f"\næ¨¡å‹è¾“å‡º:")
            print(f"  - è¾“å‡º shape: {out.shape}")
            print(
                f"  - è¾“å‡ºèŒƒå›´: [{out.min().item():.4f}, {out.max().item():.4f}]")

            # è®¡ç®—å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel()
                                   for p in model.parameters() if p.requires_grad)
            print(f"\næ¨¡å‹å¤æ‚åº¦:")
            print(f"  - æ€»å‚æ•°æ•°é‡: {total_params:,}")
            print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"  - æ¨¡å‹å¤§å°ä¼°è®¡: {total_params * 4 / (1024**2):.2f} MB")

            # è®¡ç®—å†…å­˜ä¼°è®¡
            input_memory = (sum_x.numel() + diff_x.numel() +
                            time_series_x.numel()) * 4 / (1024**2)
            print(f"  - å•batchè¾“å…¥å†…å­˜: {input_memory:.2f} MB")

            print(f"âœ… æµ‹è¯•é€šè¿‡")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")

    print(f"\n{'='*100}")
    print("æµ‹è¯•å®Œæˆ")
    print("="*100)
