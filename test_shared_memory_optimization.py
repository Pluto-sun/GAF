#!/usr/bin/env python3
"""
å…±äº«å†…å­˜ä¼˜åŒ–æ€§èƒ½æµ‹è¯•è„šæœ¬
å¯¹æ¯”æ ‡å‡†å¤šè¿›ç¨‹å’Œå…±äº«å†…å­˜æ–¹å¼çš„æ€§èƒ½å·®å¼‚
"""

import os
import sys
import time
import numpy as np
import psutil
import multiprocessing as mp
from contextlib import contextmanager

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_provider.data_loader.DualGAFDataLoader import DualGAFDataManager


@contextmanager
def performance_monitor():
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    process = psutil.Process()
    start_time = time.time()
    start_memory = process.memory_info().rss / 1024**3  # GB
    
    yield
    
    end_time = time.time()
    end_memory = process.memory_info().rss / 1024**3  # GB
    
    elapsed_time = end_time - start_time
    memory_diff = end_memory - start_memory
    
    print(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f}ç§’")
    print(f"  ğŸ’¾ å†…å­˜å˜åŒ–: {memory_diff:+.2f}GB")
    print(f"  ğŸ“Š å³°å€¼å†…å­˜: {end_memory:.2f}GB")


class TestArgs:
    """æµ‹è¯•ç”¨å‚æ•°é…ç½®"""
    def __init__(self, n_jobs=8, chunk_size=200, use_multiprocessing=True, use_shared_memory=True):
        self.root_path = "./dataset/SAHU/direct_5_working"
        self.seq_len = 96
        self.step = 96
        self.test_size = 0.2
        self.data_type_method = 'uint8'
        
        # å¹¶è¡Œé…ç½®
        self.n_jobs = n_jobs
        self.use_multiprocessing = use_multiprocessing
        self.chunk_size = chunk_size
        self.use_shared_memory = use_shared_memory


def create_test_data(n_samples=3000, n_features=20, seq_len=96):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    print(f"ğŸ“¦ åˆ›å»ºæµ‹è¯•æ•°æ®: {n_samples} æ ·æœ¬ Ã— {n_features} ç‰¹å¾ Ã— {seq_len} æ—¶é—´ç‚¹")
    
    # ç”Ÿæˆå¤šæ ·åŒ–çš„æ—¶åºæ•°æ®
    data = []
    for i in range(n_samples):
        if i % 4 == 0:  # è¶‹åŠ¿æ¨¡å¼
            trend = np.linspace(-1, 1, seq_len)
            noise = np.random.normal(0, 0.1, (seq_len, n_features))
            sample = trend[:, np.newaxis] + noise
        elif i % 4 == 1:  # å‘¨æœŸæ¨¡å¼
            period = np.sin(2 * np.pi * np.arange(seq_len) / 24)[:, np.newaxis]
            noise = np.random.normal(0, 0.1, (seq_len, n_features))
            sample = period + noise
        elif i % 4 == 2:  # ç™½å™ªå£°
            sample = np.random.normal(0, 0.3, (seq_len, n_features))
        else:  # å¤åˆæ¨¡å¼
            trend = 0.5 * np.linspace(-1, 1, seq_len)[:, np.newaxis]
            period = 0.3 * np.sin(4 * np.pi * np.arange(seq_len) / seq_len)[:, np.newaxis]
            noise = np.random.normal(0, 0.05, (seq_len, n_features))
            sample = trend + period + noise
        
        data.append(sample)
    
    result = np.array(data)
    print(f"  ğŸ“Š æ•°æ®å¤§å°: {result.nbytes / 1024**3:.2f}GB")
    print(f"  ğŸ“ æ•°æ®å½¢çŠ¶: {result.shape}")
    return result


def test_gaf_generation_comparison():
    """å¯¹æ¯”GAFç”Ÿæˆçš„æ€§èƒ½"""
    print("\n" + "="*80)
    print("ğŸ”¥ GAFç”Ÿæˆæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(3000, 20, 96)
    
    results = {}
    
    # æµ‹è¯•1: æ ‡å‡†å¤šè¿›ç¨‹
    print(f"\nğŸ”§ æµ‹è¯•1: æ ‡å‡†å¤šè¿›ç¨‹å¤„ç†")
    args1 = TestArgs(n_jobs=8, chunk_size=200, use_shared_memory=False)
    manager1 = DualGAFDataManager.__new__(DualGAFDataManager, args1)
    manager1.args = args1
    manager1.n_jobs = args1.n_jobs
    manager1.use_multiprocessing = args1.use_multiprocessing
    manager1.chunk_size = args1.chunk_size
    manager1.use_shared_memory = args1.use_shared_memory
    
    with performance_monitor():
        result1 = manager1.generate_gaf_matrix_parallel(test_data, "summation", False)
    print(f"  âœ… ç»“æœå½¢çŠ¶: {result1.shape}")
    
    # æµ‹è¯•2: å…±äº«å†…å­˜
    print(f"\nğŸš€ æµ‹è¯•2: å…±äº«å†…å­˜ä¼˜åŒ–å¤„ç†")
    args2 = TestArgs(n_jobs=8, chunk_size=200, use_shared_memory=True)
    manager2 = DualGAFDataManager.__new__(DualGAFDataManager, args2)
    manager2.args = args2
    manager2.n_jobs = args2.n_jobs
    manager2.use_multiprocessing = args2.use_multiprocessing
    manager2.chunk_size = args2.chunk_size
    manager2.use_shared_memory = args2.use_shared_memory
    
    with performance_monitor():
        result2 = manager2.generate_gaf_matrix_shared_memory(test_data, "summation", False)
    print(f"  âœ… ç»“æœå½¢çŠ¶: {result2.shape}")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    print(f"\nğŸ” ç»“æœéªŒè¯:")
    if np.allclose(result1, result2, rtol=1e-5):
        print("  âœ… æ ‡å‡†å¤šè¿›ç¨‹å’Œå…±äº«å†…å­˜ç»“æœä¸€è‡´")
    else:
        print("  âŒ ç»“æœä¸ä¸€è‡´ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        diff = np.abs(result1 - result2).mean()
        print(f"  ğŸ“Š å¹³å‡å·®å¼‚: {diff}")


def test_data_conversion_comparison():
    """å¯¹æ¯”æ•°æ®è½¬æ¢çš„æ€§èƒ½"""
    print("\n" + "="*80)
    print("ğŸ”„ æ•°æ®è½¬æ¢æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # æ¨¡æ‹ŸGAFæ•°æ®
    n_samples, n_channels, height, width = 2000, 20, 96, 96
    test_gaf_data = np.random.uniform(-1, 1, (n_samples, n_channels, height, width)).astype(np.float32)
    print(f"ğŸ“¦ GAFæµ‹è¯•æ•°æ®å½¢çŠ¶: {test_gaf_data.shape}")
    print(f"ğŸ“Š æ•°æ®å¤§å°: {test_gaf_data.nbytes / 1024**3:.2f}GB")
    
    # æµ‹è¯•1: æ ‡å‡†å¤šè¿›ç¨‹è½¬æ¢
    print(f"\nğŸ”§ æµ‹è¯•1: æ ‡å‡†å¤šçº¿ç¨‹è½¬æ¢")
    args1 = TestArgs(n_jobs=6, chunk_size=250, use_shared_memory=False)
    manager1 = DualGAFDataManager.__new__(DualGAFDataManager, args1)
    manager1.args = args1
    manager1.data_type_method = 'uint8'
    manager1.n_jobs = args1.n_jobs
    manager1.use_multiprocessing = args1.use_multiprocessing
    manager1.chunk_size = args1.chunk_size
    manager1.use_shared_memory = args1.use_shared_memory
    
    with performance_monitor():
        result1 = manager1._gaf_to_int_parallel(test_gaf_data, dtype=np.uint8)
    print(f"  âœ… è½¬æ¢åç±»å‹: {result1.dtype}")
    print(f"  ğŸ“Š å‹ç¼©æ¯”: {test_gaf_data.nbytes / result1.nbytes:.1f}x")
    
    # æµ‹è¯•2: å…±äº«å†…å­˜è½¬æ¢
    print(f"\nğŸš€ æµ‹è¯•2: å…±äº«å†…å­˜ä¼˜åŒ–è½¬æ¢")
    args2 = TestArgs(n_jobs=6, chunk_size=250, use_shared_memory=True)
    manager2 = DualGAFDataManager.__new__(DualGAFDataManager, args2)
    manager2.args = args2
    manager2.data_type_method = 'uint8'
    manager2.n_jobs = args2.n_jobs
    manager2.use_multiprocessing = args2.use_multiprocessing
    manager2.chunk_size = args2.chunk_size
    manager2.use_shared_memory = args2.use_shared_memory
    
    with performance_monitor():
        result2 = manager2.convert_gaf_data_type_shared_memory(test_gaf_data)
    print(f"  âœ… è½¬æ¢åç±»å‹: {result2.dtype}")
    print(f"  ğŸ“Š å‹ç¼©æ¯”: {test_gaf_data.nbytes / result2.nbytes:.1f}x")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    print(f"\nğŸ” ç»“æœéªŒè¯:")
    if np.array_equal(result1, result2):
        print("  âœ… æ ‡å‡†å¤šçº¿ç¨‹å’Œå…±äº«å†…å­˜è½¬æ¢ç»“æœä¸€è‡´")
    else:
        print("  âŒ è½¬æ¢ç»“æœä¸ä¸€è‡´")
        diff_count = np.sum(result1 != result2)
        print(f"  ğŸ“Š ä¸åŒå…ƒç´ æ•°é‡: {diff_count} / {result1.size}")


def test_scalability():
    """æµ‹è¯•ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„å¯æ‰©å±•æ€§"""
    print("\n" + "="*80)
    print("ğŸ“ˆ å¯æ‰©å±•æ€§æµ‹è¯•")
    print("="*80)
    
    data_sizes = [
        (1000, "å°è§„æ¨¡"),
        (3000, "ä¸­è§„æ¨¡"),
        (5000, "å¤§è§„æ¨¡"),
    ]
    
    for n_samples, size_desc in data_sizes:
        print(f"\nğŸ“Š {size_desc}æ•°æ®æµ‹è¯• ({n_samples} æ ·æœ¬)")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(n_samples, 15, 72)
        
        # å…±äº«å†…å­˜æµ‹è¯•
        print(f"  ğŸš€ å…±äº«å†…å­˜å¤„ç†:")
        args = TestArgs(n_jobs=8, chunk_size=max(200, n_samples//20), use_shared_memory=True)
        manager = DualGAFDataManager.__new__(DualGAFDataManager, args)
        manager.args = args
        manager.n_jobs = args.n_jobs
        manager.use_multiprocessing = args.use_multiprocessing
        manager.chunk_size = args.chunk_size
        manager.use_shared_memory = args.use_shared_memory
        
        with performance_monitor():
            try:
                result = manager.generate_gaf_matrix_shared_memory(test_data, "summation", False)
                print(f"    âœ… æˆåŠŸå¤„ç†ï¼Œè¾“å‡ºå½¢çŠ¶: {result.shape}")
            except Exception as e:
                print(f"    âŒ å¤„ç†å¤±è´¥: {e}")


def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("ğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}")
    
    memory = psutil.virtual_memory()
    print(f"  æ€»å†…å­˜: {memory.total / 1024**3:.1f}GB")
    print(f"  å¯ç”¨å†…å­˜: {memory.available / 1024**3:.1f}GB")
    print(f"  å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    
    print(f"  Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥å…±äº«å†…å­˜æ”¯æŒ
    try:
        from multiprocessing import shared_memory
        print(f"  å…±äº«å†…å­˜æ”¯æŒ: âœ… å¯ç”¨")
    except ImportError:
        print(f"  å…±äº«å†…å­˜æ”¯æŒ: âŒ ä¸å¯ç”¨ (éœ€è¦Python 3.8+)")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å…±äº«å†…å­˜ä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    print("="*80)
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("\nâŒ é”™è¯¯: å…±äº«å†…å­˜éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        print(f"å½“å‰ç‰ˆæœ¬: {sys.version}")
        return 1
    
    try:
        # GAFç”Ÿæˆæ€§èƒ½å¯¹æ¯”
        test_gaf_generation_comparison()
        
        # æ•°æ®è½¬æ¢æ€§èƒ½å¯¹æ¯”
        test_data_conversion_comparison()
        
        # å¯æ‰©å±•æ€§æµ‹è¯•
        test_scalability()
        
        print(f"\nğŸ‰ å…±äº«å†…å­˜ä¼˜åŒ–æµ‹è¯•å®Œæˆ")
        print("="*80)
        print("ğŸ’¡ ä¸»è¦å‘ç°:")
        print("  1. å…±äº«å†…å­˜æ˜¾è‘—å‡å°‘äº†è¿›ç¨‹é—´æ•°æ®ä¼ è¾“å¼€é”€")
        print("  2. å¯¹äºå¤§æ•°æ®é›†ï¼Œæ€§èƒ½æå‡æ›´åŠ æ˜æ˜¾")
        print("  3. å†…å­˜ä½¿ç”¨æ›´åŠ é«˜æ•ˆï¼Œå‡å°‘äº†æ•°æ®å¤åˆ¶")
        print("  4. å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨å…±äº«å†…å­˜ä¼˜åŒ–")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main()) 