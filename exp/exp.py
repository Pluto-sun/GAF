import os
import torch
import torch.nn as nn
from torch import optim
import numpy as np
from data_provider.data_factory import data_provider
from utils.tools import EarlyStopping, cal_accuracy
from models import RestNet, ClusteredResNet, VGGNet, ClusteredVGGNet, ClusteredInception, GNN, MultiImageFeatureNet, DualGAFNet, SimpleGAFNet, OneDCNN
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import matplotlib
import matplotlib.font_manager as fm
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score
import torch.nn.functional as F
import torch.optim as optim
import time
import warnings
from torch.optim import lr_scheduler
import logging
import sys
from datetime import datetime
# å·²ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥ï¼švisual_long, save_metrics_to_csv, create_exp_folder è¿™äº›å‡½æ•°åœ¨utils/tools.pyä¸­ä¸å­˜åœ¨
# from utils.tools import adjust_learning_rate, visual  # è¿™äº›å‡½æ•°å­˜åœ¨ä½†æœªä½¿ç”¨
torch.cuda.empty_cache()
# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒçš„å‡½æ•°
def setup_chinese_font():
    """è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ - ä¼˜åŒ–ç‰ˆæœ¬"""
    # æ¸…é™¤matplotlibå­—ä½“ç¼“å­˜
    import shutil
    import os
    try:
        mpl_cache = matplotlib.get_cachedir()
        if os.path.exists(mpl_cache):
            shutil.rmtree(mpl_cache)
        print("âœ“ matplotlibå­—ä½“ç¼“å­˜å·²æ¸…é™¤")
    except Exception as e:
        print(f"âš ï¸ æ¸…é™¤å­—ä½“ç¼“å­˜å¤±è´¥: {e}")
    
    # é‡æ–°åŠ è½½å­—ä½“ç®¡ç†å™¨
    fm.fontManager = fm.FontManager()
    
    # æ‰©å±•çš„ä¸­æ–‡å­—ä½“åˆ—è¡¨ï¼ˆåŸºäºå®é™…ç³»ç»Ÿå®‰è£…ï¼‰
    chinese_fonts = [
        # æ–°å®‰è£…çš„Noto CJKå­—ä½“ï¼ˆä¼˜å…ˆï¼‰
        'Noto Sans CJK SC Regular',
        'Noto Sans CJK SC',
        'Noto Sans CJK TC', 
        'Noto Serif CJK SC',
        # æ–‡æ³‰é©¿å­—ä½“ï¼ˆLinuxå¸¸ç”¨ï¼‰
        'WenQuanYi Micro Hei',
        'WenQuanYi Zen Hei',
        'WenQuanYi Micro Hei Light',
        # æ€æºå­—ä½“
        'Source Han Sans SC',
        'Source Han Sans CN',
        # Windowså­—ä½“ï¼ˆå¦‚æœåœ¨Wineç¯å¢ƒä¸‹ï¼‰
        'SimHei',
        'Microsoft YaHei',
        'SimSun',
        # macOSå­—ä½“
        'PingFang SC',
        'Hiragino Sans GB',
        # AR PLå­—ä½“
        'AR PL UMing CN',
        'AR PL UKai CN',
        # å¤‡ç”¨å­—ä½“
        'DejaVu Sans',
    ]
    
    # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
    available_fonts = []
    for font in fm.fontManager.ttflist:
        available_fonts.append(font.name)
    
    print("ğŸ” æ­£åœ¨æ£€æµ‹ä¸­æ–‡å­—ä½“...")
    print(f"ç³»ç»Ÿæ€»å­—ä½“æ•°: {len(available_fonts)}")
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    selected_font = None
    for font in chinese_fonts:
        if font in available_fonts:
            selected_font = font
            print(f"âœ… æ‰¾åˆ°é¢„è®¾å­—ä½“: {font}")
            break
        else:
            print(f"âŒ æœªæ‰¾åˆ°: {font}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢„è®¾å­—ä½“ï¼Œæ™ºèƒ½æœç´¢CJKå­—ä½“
    if not selected_font:
        print("ğŸ” åœ¨é¢„è®¾å­—ä½“ä¸­æœªæ‰¾åˆ°ï¼Œæ™ºèƒ½æœç´¢CJKå­—ä½“...")
        cjk_keywords = ['Noto', 'CJK', 'WenQuanYi', 'Source Han', 'AR PL', 'SimHei', 'YaHei']
        cjk_fonts = []
        
        for font in fm.fontManager.ttflist:
            font_name = font.name
            font_path = font.fname
            
            # æ£€æŸ¥å­—ä½“åç§°æ˜¯å¦åŒ…å«CJKå…³é”®è¯
            if any(keyword in font_name for keyword in cjk_keywords):
                cjk_fonts.append(font_name)
                print(f"ğŸ” å‘ç°CJKå­—ä½“: {font_name} ({font_path})")
        
        # å»é‡å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
        cjk_fonts = list(set(cjk_fonts))
        if cjk_fonts:
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šNoto > WenQuanYi > Source Han > AR PL > å…¶ä»–
            priority_order = ['Noto', 'WenQuanYi', 'Source Han', 'AR PL']
            
            for priority in priority_order:
                matching_fonts = [f for f in cjk_fonts if priority in f]
                if matching_fonts:
                    selected_font = matching_fonts[0]
                    print(f"âœ… é€‰æ‹©ä¼˜å…ˆçº§å­—ä½“: {selected_font}")
                    break
            
            if not selected_font:
                selected_font = cjk_fonts[0]
                print(f"âœ… é€‰æ‹©é¦–ä¸ªCJKå­—ä½“: {selected_font}")
            
            print(f"ğŸ“‹ å…¶ä»–å¯ç”¨CJKå­—ä½“: {cjk_fonts[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ª
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•CJKå­—ä½“")
    
    # é…ç½®matplotlib
    if selected_font:
        print(f"ğŸ¨ åº”ç”¨å­—ä½“é…ç½®: {selected_font}")
        # è®¾ç½®å­—ä½“æ—ï¼ŒåŒ…å«åå¤‡å­—ä½“
        font_list = [selected_font] + chinese_fonts + ['DejaVu Sans', 'Arial Unicode MS']
        matplotlib.rcParams['font.sans-serif'] = font_list
        matplotlib.rcParams['font.family'] = 'sans-serif'
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨å¤‡ç”¨é…ç½®")
        matplotlib.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
        matplotlib.rcParams['font.family'] = 'sans-serif'
    
    # é€šç”¨é…ç½®
    matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    matplotlib.rcParams['figure.max_open_warning'] = 0  # ç¦ç”¨å›¾å½¢æ•°é‡è­¦å‘Š
    plt.rcParams['font.size'] = 10  # è®¾ç½®é»˜è®¤å­—ä½“å¤§å°
    
    # éªŒè¯å­—ä½“æ˜¯å¦æ­£å¸¸å·¥ä½œ
    try:
        fig, ax = plt.subplots(figsize=(1, 1))
        ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡å­—ä½“', ha='center', va='center')
        plt.close(fig)
        print("âœ… ä¸­æ–‡å­—ä½“éªŒè¯æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å­—ä½“éªŒè¯å¤±è´¥: {e}")
    
    print(f"ğŸ“ æœ€ç»ˆå­—ä½“é…ç½®: {matplotlib.rcParams['font.sans-serif'][:3]}...")

# ä½¿ç”¨ä¼˜åŒ–çš„å­—ä½“é…ç½®æ¨¡å—
try:
    from utils.font_config import quick_setup
    quick_setup()
except ImportError:
    # å¦‚æœå­—ä½“é…ç½®æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸæ¥çš„å‡½æ•°
    setup_chinese_font()


import time
class Exp(object):
    def __init__(self, args, setting):
        self.args = args
        self.setting = setting
        self.model_dict = {
            'RestNet': RestNet,
            'ClusteredResNet': ClusteredResNet,
            'VGG': VGGNet,
            'ClusteredVGGNet': ClusteredVGGNet,
            'ClusteredInception': ClusteredInception,
            'GNN': GNN,
            'MultiImageFeatureNet': MultiImageFeatureNet,
            'DualGAFNet': DualGAFNet,
            'SimpleGAFNet': SimpleGAFNet,
            'OneDCNN': OneDCNN
        }
        # if args.model == 'Mamba':
        #     print('Please make sure you have successfully installed mamba_ssm')
        #     from models import Mamba
        #     self.model_dict['Mamba'] = Mamba
        self.device = self._acquire_device()
        self.train_data, self.train_loader = self._get_data(flag='train')
        self.vali_data, self.vali_loader = self._get_data(flag='val')
        # è·å–æ ‡ç­¾æ˜ å°„ä¿¡æ¯ï¼ˆå¦‚æœæ•°æ®é›†æ”¯æŒï¼‰
        self.class_names = self._get_class_names()
        self.model = self._build_model().to(self.device)
        self.time_stamp = time.strftime('%m%d_%H%M')
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger, self.log_file = self._setup_logging()
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶"""
        # åˆ›å»ºä¸plot_resultsä¸€è‡´çš„ç›®å½•ç»“æ„
        log_dir = os.path.join(self.args.result_path, f"{self.time_stamp}_{self.setting}")
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
        log_file = os.path.join(log_dir, "training.log")
        
        # åˆ›å»ºæ—¥å¿—å™¨
        logger = logging.getLogger(f'exp_{self.setting}')
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        
        # è®¾ç½®æ ¼å¼
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        console_formatter = logging.Formatter('%(message)s')
        
        file_handler.setFormatter(file_formatter)
        console_handler.setFormatter(console_formatter)
        
        # æ·»åŠ å¤„ç†å™¨
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­
        logger.propagate = False
        
        # è®°å½•æ—¥å¿—å¼€å§‹ä¿¡æ¯
        logger.info("=" * 80)
        logger.info(f"å®éªŒæ—¥å¿—å¼€å§‹: {self.setting}")
        logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)
        
        return logger, log_file
    
    def log_info(self, message):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        if hasattr(self, 'logger') and self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def log_warning(self, message):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        if hasattr(self, 'logger') and self.logger:
            self.logger.warning(message)
        else:
            print(f"WARNING: {message}")
    
    def log_error(self, message):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        if hasattr(self, 'logger') and self.logger:
            self.logger.error(message)
        else:
            print(f"ERROR: {message}")
    
    def log_config(self):
        """è®°å½•å®éªŒé…ç½®"""
        self.log_info("âš™ï¸ å®éªŒé…ç½®:")
        for key, value in self.args.__dict__.items():
            if not key.startswith('_'):
                self.log_info(f"   {key}: {value}")
    
    def log_model_info(self):
        """è®°å½•æ¨¡å‹ä¿¡æ¯"""
        self.log_info("ğŸ—ï¸ æ¨¡å‹ä¿¡æ¯:")
        self.log_info(f"   æ¨¡å‹ç±»å‹: {type(self.model).__name__}")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        self.log_info(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        self.log_info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.log_info(f"   æ¨¡å‹å¤§å°ä¼°è®¡: {total_params * 4 / (1024**2):.2f} MB")
    
    def log_data_info(self):
        """è®°å½•æ•°æ®é›†ä¿¡æ¯"""
        self.log_info("ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        self.log_info(f"   è®­ç»ƒé›†å¤§å°: {len(self.train_data)}")
        self.log_info(f"   éªŒè¯é›†å¤§å°: {len(self.vali_data)}")
    
    def log_epoch_metrics(self, epoch, train_loss, train_acc, val_metrics, learning_rate, scheduler_type=None):
        """è®°å½•æ¯è½®è®­ç»ƒçš„è¯¦ç»†æŒ‡æ ‡"""
        self.log_info(f"Epoch {epoch+1}:")
        self.log_info(f"  è®­ç»ƒæŒ‡æ ‡:")
        self.log_info(f"    æŸå¤±({getattr(self.args, 'loss_type', 'ce')}): {train_loss:.6f}")
        self.log_info(f"    å‡†ç¡®ç‡: {train_acc:.6f}")
        self.log_info(f"    å­¦ä¹ ç‡: {learning_rate:.8f}")
        
        self.log_info(f"  éªŒè¯æŒ‡æ ‡(æ ‡å‡†CE):")
        self.log_info(f"    æŸå¤±: {val_metrics['loss']:.6f}")
        self.log_info(f"    å‡†ç¡®ç‡: {val_metrics['accuracy']:.6f}")
        self.log_info(f"    F1(macro): {val_metrics['f1_macro']:.6f}")
        self.log_info(f"    F1(weighted): {val_metrics['f1_weighted']:.6f}")
        self.log_info(f"    ç²¾ç¡®ç‡: {val_metrics['precision']:.6f}")
        self.log_info(f"    å¬å›ç‡: {val_metrics['recall']:.6f}")
        
        # å¦‚æœæœ‰è°ƒåº¦å™¨ä¿¡æ¯ï¼Œè®°å½•è°ƒåº¦å™¨ç±»å‹
        if scheduler_type:
            self.log_info(f"  å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")
        
        self.log_info("-" * 60)
    
    def log_learning_rate_change(self, old_lr, new_lr, scheduler_type, val_metrics):
        """è®°å½•å­¦ä¹ ç‡å˜åŒ–"""
        self.log_info(f"ğŸ¯ å­¦ä¹ ç‡è°ƒæ•´äº‹ä»¶:")
        self.log_info(f"  è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
        self.log_info(f"  å­¦ä¹ ç‡å˜åŒ–: {old_lr:.8f} â†’ {new_lr:.8f}")
        
        if scheduler_type == 'f1_based':
            self.log_info(f"  è§¦å‘æ¡ä»¶: F1åˆ†æ•° = {val_metrics['f1_weighted']:.6f}")
        elif scheduler_type.startswith('composite'):
            self.log_info(f"  è§¦å‘æ¡ä»¶: F1åˆ†æ•° = {val_metrics['f1_weighted']:.6f}, éªŒè¯æŸå¤± = {val_metrics['loss']:.6f}")
        else:
            self.log_info(f"  è§¦å‘æ¡ä»¶: éªŒè¯æŸå¤± = {val_metrics['loss']:.6f}")
        
        self.log_info(f"  å˜åŒ–å¹…åº¦: {((new_lr - old_lr) / old_lr * 100):+.2f}%")
        self.log_info("-" * 60)
    
    def log_epoch_summary(self, epoch, train_loss, train_acc, val_metrics, learning_rate, is_best_model=False):
        """è®°å½•æ¯è½®è®­ç»ƒçš„ç®€è¦æ€»ç»“"""
        status = "ğŸ† [æœ€ä½³æ¨¡å‹]" if is_best_model else ""
        self.log_info(f"Epoch {epoch+1:3d} | Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | "
                      f"Val: Loss={val_metrics['loss']:.4f}, Acc={val_metrics['accuracy']:.4f}, "
                      f"F1={val_metrics['f1_weighted']:.4f} | LR={learning_rate:.6f} {status}")
    
    def log_training_history(self, history):
        """è®°å½•å®Œæ•´è®­ç»ƒå†å²åˆ°CSVæ–‡ä»¶"""
        import csv
        
        # åˆ›å»ºCSVæ–‡ä»¶è·¯å¾„
        log_dir = os.path.join(self.args.result_path, f"{self.time_stamp}_{self.setting}")
        csv_file = os.path.join(log_dir, "training_history.csv")
        
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        epochs = len(history['train_losses'])
        
        for epoch in range(epochs):
            row = {
                'Epoch': epoch + 1,
                'Train_Loss': history['train_losses'][epoch],
                'Train_Acc': history['train_accs'][epoch],
                'Val_Loss': history['val_metrics']['loss'][epoch],
                'Val_Acc': history['val_metrics']['accuracy'][epoch],
                'Val_F1_Macro': history['val_metrics']['f1_macro'][epoch],
                'Val_F1_Weighted': history['val_metrics']['f1_weighted'][epoch],
                'Val_Precision': history['val_metrics']['precision'][epoch],
                'Val_Recall': history['val_metrics']['recall'][epoch],
            }
            
            # æ·»åŠ å­¦ä¹ ç‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'learning_rates' in history:
                row['Learning_Rate'] = history['learning_rates'][epoch]
            
            csv_data.append(row)
        
        # å†™å…¥CSVæ–‡ä»¶
        if csv_data:
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                writer.writeheader()
                writer.writerows(csv_data)
            
            self.log_info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {csv_file}")
    
    def close_logger(self):
        """å…³é—­æ—¥å¿—å™¨"""
        if hasattr(self, 'logger') and self.logger:
            self.logger.info("=" * 80)
            self.logger.info(f"å®éªŒæ—¥å¿—ç»“æŸ: {self.setting}")
            self.logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.info("=" * 80)
            
            # å…³é—­æ‰€æœ‰å¤„ç†å™¨
            for handler in self.logger.handlers:
                handler.close()
            self.logger.handlers.clear()
        

    def _build_model(self):
        sample = next(iter(self.train_loader))
        # åˆ¤æ–­æ˜¯PyGçš„Batchè¿˜æ˜¯æ™®é€štuple
        if hasattr(sample, 'x') and hasattr(sample, 'edge_index'):
            # GNNæ•°æ®
            print(f"Sample GNN data shape: x={sample.x.shape}")
            self.args.seq_len = sample.x.shape[0]  # èŠ‚ç‚¹æ•°ï¼ˆå¯æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ï¼‰
            self.args.enc_in = sample.x.shape[1]   # èŠ‚ç‚¹ç‰¹å¾æ•°
        elif len(sample) == 3:
            # åŒè·¯GAFæ•°æ®ï¼š(summation_data, difference_data, label) - ç¦ç”¨ç»Ÿè®¡ç‰¹å¾æ ¼å¼
            sum_data, diff_data, label = sample
            print(f"Sample dual GAF data shape: sum={sum_data.shape}, diff={diff_data.shape}")
            self.args.seq_len = sum_data.shape[3]
            self.args.enc_in = sum_data.shape[1]
            self.is_dual_gaf = True
            self.has_time_series = False
            
            # å¯¹äºDualGAFNetæ¨¡å‹ï¼Œç¡®ä¿use_statistical_featuresè®¾ç½®ä¸ºFalse
            if self.args.model == 'DualGAFNet':
                self.args.use_statistical_features = False
                print("æ£€æµ‹åˆ°ä¸‰å…ƒç»„æ ¼å¼ï¼Œç¦ç”¨ç»Ÿè®¡ç‰¹å¾")
        elif len(sample) == 4:
            # å¢å¼ºåŒè·¯GAFæ•°æ®ï¼š(summation_data, difference_data, time_series_data, label) - å¯ç”¨ç»Ÿè®¡ç‰¹å¾æ ¼å¼
            sum_data, diff_data, time_series_data, label = sample
            print(f"Sample enhanced dual GAF data shape: sum={sum_data.shape}, diff={diff_data.shape}, time_series={time_series_data.shape}")
            self.args.seq_len = sum_data.shape[3]
            self.args.enc_in = sum_data.shape[1]
            self.is_dual_gaf = True
            self.has_time_series = True
            
            # å¯¹äºDualGAFNetæ¨¡å‹ï¼Œç¡®ä¿use_statistical_featuresè®¾ç½®ä¸ºTrue
            if self.args.model == 'DualGAFNet':
                if not hasattr(self.args, 'use_statistical_features'):
                    self.args.use_statistical_features = True
                print(f"æ£€æµ‹åˆ°å››å…ƒç»„æ ¼å¼ï¼Œç»Ÿè®¡ç‰¹å¾è®¾ç½®: {self.args.use_statistical_features}")
            
            # å°è¯•ä»æ•°æ®é›†è·å–ç‰¹å¾åˆ—ä¿¡æ¯
            if hasattr(self.train_data, 'data_manager') and hasattr(self.train_data.data_manager, 'scalers') and self.train_data.data_manager.scalers:
                feature_columns = list(self.train_data.data_manager.scalers.keys())
                self.args.feature_columns = feature_columns
                print(f"ä»åŒè·¯GAFæ•°æ®é›†è·å–ç‰¹å¾åˆ—: {feature_columns}")
            else:
                self.args.feature_columns = None
                print("æœªæ‰¾åˆ°ç‰¹å¾åˆ—ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆ†ç»„")
            
            # è®¾ç½®HVACä¿¡å·ç»„é…ç½®
            if hasattr(self.args, 'hvac_groups') and self.args.hvac_groups:
                print(f"ä½¿ç”¨è‡ªå®šä¹‰HVACä¿¡å·ç»„: {len(self.args.hvac_groups)} ç»„")
            else:
                # ä¸ä½¿ç”¨åˆ†ç»„ï¼Œè®¾ç½®ä¸ºNone
                self.args.hvac_groups = None
                print("æœªæä¾›HVACä¿¡å·ç»„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾æå–å™¨ï¼ˆä¸åˆ†ç»„ï¼‰")
        else:
            # æ™®é€šåˆ†ç±»æ•°æ®
            sample_data, label = sample
            print(f"Sample data shape: {sample_data.shape}")
            self.args.seq_len = sample_data.shape[3]
            self.args.enc_in = sample_data.shape[1]
            self.is_dual_gaf = False
            
            # å°è¯•ä»æ•°æ®é›†è·å–ç‰¹å¾åˆ—ä¿¡æ¯
            if hasattr(self.train_data, 'scalers') and self.train_data.scalers:
                feature_columns = list(self.train_data.scalers.keys())
                self.args.feature_columns = feature_columns
                print(f"ä»æ•°æ®é›†è·å–ç‰¹å¾åˆ—: {feature_columns}")
            else:
                self.args.feature_columns = None
                print("æœªæ‰¾åˆ°ç‰¹å¾åˆ—ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆ†ç»„")
            
            # è®¾ç½®HVACä¿¡å·ç»„é…ç½®
            if hasattr(self.args, 'hvac_groups') and self.args.hvac_groups:
                print(f"ä½¿ç”¨è‡ªå®šä¹‰HVACä¿¡å·ç»„: {len(self.args.hvac_groups)} ç»„")
            else:
                # ä¸ä½¿ç”¨åˆ†ç»„ï¼Œè®¾ç½®ä¸ºNone
                self.args.hvac_groups = None
                print("æœªæä¾›HVACä¿¡å·ç»„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾æå–å™¨ï¼ˆä¸åˆ†ç»„ï¼‰")
                
        model = self.model_dict[self.args.model].Model(self.args).float()
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"="*100)
        print(f"æ¨¡å‹æ„å»ºå®Œæˆ:")
        print(f"  - è¾“å…¥é€šé“æ•°: {self.args.enc_in}")
        print(f"  - åºåˆ—é•¿åº¦: {self.args.seq_len}")
        print(f"  - ç±»åˆ«æ•°: {self.args.num_class}")
        print(f"  - æ¨¡å‹ç±»å‹: {self.args.model}")
        print(f"="*100)
        if self.args.use_multi_gpu and self.args.use_gpu:
            model = nn.DataParallel(model, device_ids=self.args.device_ids)
        return model

    def _acquire_device(self):
        if self.args.use_gpu and self.args.gpu_type == 'cuda':
            os.environ["CUDA_VISIBLE_DEVICES"] = str(self.args.gpu) if not self.args.use_multi_gpu else self.args.devices
            device = torch.device('cuda:{}'.format(self.args.gpu))
            print('Use GPU: cuda:{}'.format(self.args.gpu))
        elif self.args.use_gpu and self.args.gpu_type == 'mps':
            device = torch.device('mps')
            print('Use GPU: mps')
        else:
            device = torch.device('cpu')
            print('Use CPU')
        return device

    def _get_data(self, flag):
        data_set, data_loader = data_provider(self.args, flag)
        return data_set, data_loader

    def _get_class_names(self):
        """è·å–ç±»åˆ«åç§°ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®é›†ä¸­çš„æ ‡ç­¾æ˜ å°„"""
        try:
            # å°è¯•ä»æ•°æ®é›†è·å–æ ‡ç­¾æ˜ å°„
            if hasattr(self.train_data, 'idx_to_label'):
                class_names = [self.train_data.idx_to_label[i] for i in range(self.args.num_class)]
                print(f"ä½¿ç”¨æ•°æ®é›†ä¸­çš„æ ‡ç­¾æ˜ å°„: {class_names}")
                return class_names
        except Exception as e:
            print(f"æ— æ³•è·å–æ•°æ®é›†æ ‡ç­¾æ˜ å°„: {e}")
        
        # é»˜è®¤ä½¿ç”¨é€šç”¨åç§°
        default_names = [f'å¼‚å¸¸ç±»å‹_{i+1}' for i in range(self.args.num_class)]
        print(f"ä½¿ç”¨é»˜è®¤æ ‡ç­¾åç§°: {default_names}")
        return default_names

    def _select_optimizer(self):
        """
        é€‰æ‹©RAdamä¼˜åŒ–å™¨ï¼Œé…åˆReduceLROnPlateauå­¦ä¹ ç‡è°ƒåº¦å™¨
        
        RAdamä¼˜åŒ–å™¨çš„ä¼˜åŠ¿ï¼š
        - è‡ªé€‚åº”ä¿®æ­£Adamçš„æ–¹å·®é—®é¢˜
        - è®­ç»ƒå‰æœŸæ›´ç¨³å®š
        - å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
        - æ”¶æ•›æ€§æ›´å¥½
        """
        try:
            # å°è¯•ä½¿ç”¨torch.optim.RAdamï¼ˆPyTorch 1.5+ï¼‰
            model_optim = optim.RAdam(
                self.model.parameters(), 
                lr=self.args.learning_rate,
                betas=(0.9, 0.999),  # RAdamæ¨èå‚æ•°
                eps=1e-8,
                weight_decay=1e-4    # è½»å¾®æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            )
            print(f"ğŸš€ ä½¿ç”¨RAdamä¼˜åŒ–å™¨ (lr={self.args.learning_rate}, weight_decay=1e-4)")
        except AttributeError:
            # é™çº§åˆ°Adamï¼ˆå…¼å®¹æ€§ï¼‰
            model_optim = optim.Adam(
                self.model.parameters(), 
                lr=self.args.learning_rate,
                weight_decay=1e-4
            )
            print(f"âš¡ RAdamä¸å¯ç”¨ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨ (lr={self.args.learning_rate}, weight_decay=1e-4)")
        
        return model_optim
    
    def _select_lr_scheduler(self, optimizer):
        """
        é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        
        æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥ï¼š
        - åŸºäºF1åˆ†æ•°ï¼šç›´æ¥å…³æ³¨åˆ†ç±»æ€§èƒ½ï¼Œé€‚åˆåˆ†ç±»ä»»åŠ¡ï¼ˆæ¨èï¼‰
        - åŸºäºéªŒè¯æŸå¤±ï¼šä¼ ç»Ÿæ–¹æ³•ï¼Œé€‚åˆæŸå¤±ä¸æ€§èƒ½ä¸€è‡´çš„åœºæ™¯
        - å¤åˆç­–ç•¥ï¼šç»“åˆä¸¤ç§æŒ‡æ ‡ï¼Œæ›´ç¨³å¥çš„è°ƒåº¦
        """
        # è·å–è°ƒåº¦å™¨ç±»å‹é…ç½®
        scheduler_type = getattr(self.args, 'lr_scheduler_type', 'f1_based')
        
        # åŸºç¡€å‚æ•°é…ç½®
        base_params = {
            'factor': 0.5,           # å­¦ä¹ ç‡ç¼©å‡ä¸ºåŸæ¥çš„50%
            'patience': 7,           # 5ä¸ªepochæ²¡æœ‰æ”¹å–„æ‰è°ƒæ•´
            'min_lr': 1e-6,         # æœ€å°å­¦ä¹ ç‡
            'cooldown': 2,          # è°ƒæ•´åç­‰å¾…2ä¸ªepochå†æ¬¡æ£€æŸ¥
            'threshold': 0.001,      # F1åˆ†æ•°æ”¹å–„é˜ˆå€¼ï¼š1%çš„ç›¸å¯¹æ”¹å–„æ‰ç®—æœ‰æ•ˆ
            'threshold_mode': 'rel', # ç›¸å¯¹é˜ˆå€¼æ¨¡å¼
            'eps': 1e-8,            # æ•°å€¼ç¨³å®šæ€§å‚æ•°
            'verbose': True         # æ‰“å°è°ƒæ•´ä¿¡æ¯
        }
        
        try:
            # å°è¯•å¯¼å…¥F1è°ƒåº¦å™¨
            from utils.f1_based_scheduler import create_lr_scheduler
            
            if scheduler_type == 'f1_based':
                scheduler = create_lr_scheduler(optimizer, 'f1_based', **base_params)
                print(f"ğŸ¯ é…ç½®åŸºäºF1åˆ†æ•°çš„å­¦ä¹ ç‡è°ƒåº¦å™¨:")
                print(f"   â†’ ç›‘æ§æŒ‡æ ‡: F1åˆ†æ•°ï¼ˆæ›´é€‚åˆåˆ†ç±»ä»»åŠ¡ï¼‰")
                print(f"   â†’ ä¼˜åŠ¿: ç›´æ¥å…³æ³¨åˆ†ç±»æ€§èƒ½ï¼Œé¿å…æŸå¤±éœ‡è¡å½±å“")
                print(f"   â†’ è§¦å‘æ¡ä»¶: F1åˆ†æ•°è¿ç»­{base_params['patience']}è½®æ— æ”¹å–„")
                
            elif scheduler_type == 'composite_f1_priority':
                scheduler = create_lr_scheduler(optimizer, 'composite_f1_priority', **base_params)
                print(f"ğŸ”„ é…ç½®å¤åˆå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆF1ä¼˜å…ˆï¼‰:")
                print(f"   â†’ ä¸»è¦ç›‘æ§: F1åˆ†æ•°")
                print(f"   â†’ è¾…åŠ©ç›‘æ§: éªŒè¯æŸå¤±")
                print(f"   â†’ ç­–ç•¥: F1åœæ»æ—¶ä¼˜å…ˆè°ƒæ•´ï¼ŒæŸå¤±å¼‚å¸¸æ—¶è¾…åŠ©è°ƒæ•´")
                print(f"   â†’ è§¦å‘æ¡ä»¶: F1åˆ†æ•°è¿ç»­{base_params['patience']}è½®æ— æ”¹å–„")
                
            elif scheduler_type == 'composite_weighted':
                # å¯é…ç½®æƒé‡
                loss_weight = getattr(self.args, 'lr_loss_weight', 0.3)
                f1_weight = getattr(self.args, 'lr_f1_weight', 0.7)
                scheduler = create_lr_scheduler(
                    optimizer, 'composite_weighted',
                    loss_weight=loss_weight, f1_weight=f1_weight,
                    **base_params
                )
                print(f"âš–ï¸ é…ç½®åŠ æƒå¤åˆå­¦ä¹ ç‡è°ƒåº¦å™¨:")
                print(f"   â†’ æŸå¤±æƒé‡: {loss_weight}")
                print(f"   â†’ F1æƒé‡: {f1_weight}")
                print(f"   â†’ ç­–ç•¥: ç»¼åˆè€ƒè™‘ä¸¤ä¸ªæŒ‡æ ‡çš„åŠ æƒç»„åˆ")
                print(f"   â†’ è§¦å‘æ¡ä»¶: å¤åˆæŒ‡æ ‡è¿ç»­{base_params['patience']}è½®æ— æ”¹å–„")
                
            elif scheduler_type == 'composite_loss_priority':
                scheduler = create_lr_scheduler(optimizer, 'composite_loss_priority', **base_params)
                print(f"ğŸ”„ é…ç½®å¤åˆå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæŸå¤±ä¼˜å…ˆï¼‰:")
                print(f"   â†’ ä¸»è¦ç›‘æ§: éªŒè¯æŸå¤±")
                print(f"   â†’ è¾…åŠ©ç›‘æ§: F1åˆ†æ•°")
                print(f"   â†’ ç­–ç•¥: æŸå¤±åœæ»æ—¶ä¼˜å…ˆè°ƒæ•´ï¼ŒF1å¼‚å¸¸æ—¶è¾…åŠ©è°ƒæ•´")
                print(f"   â†’ è§¦å‘æ¡ä»¶: éªŒè¯æŸå¤±è¿ç»­{base_params['patience']}è½®æ— æ”¹å–„")
                
            else:
                # é»˜è®¤ä½¿ç”¨æŸå¤±è°ƒåº¦å™¨ï¼ˆå‘åå…¼å®¹ï¼‰
                scheduler = create_lr_scheduler(optimizer, 'loss_based', **base_params)
                print(f"ğŸ“‰ é…ç½®åŸºäºéªŒè¯æŸå¤±çš„å­¦ä¹ ç‡è°ƒåº¦å™¨:")
                print(f"   â†’ ç›‘æ§æŒ‡æ ‡: éªŒè¯æŸå¤±ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰")
                print(f"   â†’ è§¦å‘æ¡ä»¶: éªŒè¯æŸå¤±è¿ç»­{base_params['patience']}è½®æ— æ”¹å–„")
                
        except ImportError as e:
            # å¦‚æœF1è°ƒåº¦å™¨ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†è°ƒåº¦å™¨
            print(f"âš ï¸ F1è°ƒåº¦å™¨å¯¼å…¥å¤±è´¥: {e}")
            print("å›é€€åˆ°æ ‡å‡†æŸå¤±è°ƒåº¦å™¨")
            scheduler_type = 'loss_based'  # æ›´æ–°è°ƒåº¦å™¨ç±»å‹ä»¥ç¡®ä¿æ­£ç¡®è°ƒç”¨
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', **base_params
            )
            print(f"ğŸ“‰ é…ç½®ReduceLROnPlateauå­¦ä¹ ç‡è°ƒåº¦å™¨:")
            print(f"   â†’ ç›‘æ§æŒ‡æ ‡: éªŒè¯æŸå¤±")
            print(f"   â†’ è§¦å‘æ¡ä»¶: éªŒè¯æŸå¤±è¿ç»­{base_params['patience']}è½®æ— æ”¹å–„")
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸å¤„ç†
            print(f"âš ï¸ å­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–å¼‚å¸¸: {e}")
            print("å›é€€åˆ°æ ‡å‡†æŸå¤±è°ƒåº¦å™¨")
            scheduler_type = 'loss_based'  # æ›´æ–°è°ƒåº¦å™¨ç±»å‹ä»¥ç¡®ä¿æ­£ç¡®è°ƒç”¨
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', **base_params
            )
            print(f"ğŸ“‰ é…ç½®ReduceLROnPlateauå­¦ä¹ ç‡è°ƒåº¦å™¨:")
            print(f"   â†’ ç›‘æ§æŒ‡æ ‡: éªŒè¯æŸå¤±")
        
        # é€šç”¨é…ç½®ä¿¡æ¯
        print(f"   â†’ ç¼©å‡å› å­: 0.5 (å­¦ä¹ ç‡å‡åŠ)")
        print(f"   â†’ è€å¿ƒåº¦: 5ä¸ªepoch")
        print(f"   â†’ æœ€å°å­¦ä¹ ç‡: 1e-6")
        print(f"   â†’ å†·å´æœŸ: 2ä¸ªepoch")
        
        return scheduler

    def _apply_loss_preset(self, preset):
        """
        åº”ç”¨æŸå¤±å‡½æ•°é¢„è®¾é…ç½®
        
        Args:
            preset (str): é¢„è®¾é…ç½®åç§°
        """
        print(f"ğŸ¯ åº”ç”¨æŸå¤±å‡½æ•°é¢„è®¾é…ç½®: {preset}")
        
        if preset == 'hvac_similar':
            # HVACç›¸ä¼¼ç±»åˆ«é…ç½®
            self.args.loss_type = 'label_smoothing'
            self.args.label_smoothing = 0.15  # è¾ƒé«˜çš„å¹³æ»‘å› å­
            print("   â†’ é…ç½®ï¼šæ ‡ç­¾å¹³æ»‘ (smoothing=0.15) - é€‚ç”¨äºHVACå¼‚å¸¸æ£€æµ‹ä¸­çš„ç›¸ä¼¼æ•…éšœæ¨¡å¼")
            
        elif preset == 'imbalanced_focus':
            # ç±»åˆ«ä¸å¹³è¡¡é…ç½®
            self.args.loss_type = 'focal'
            self.args.focal_alpha = 1.0
            self.args.focal_gamma = 2.0
            print("   â†’ é…ç½®ï¼šFocal Loss (alpha=1.0, gamma=2.0) - é€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡é—®é¢˜")
            
        elif preset == 'hard_samples':
            # éš¾åˆ†ç±»æ ·æœ¬é…ç½®
            self.args.loss_type = 'focal'
            self.args.focal_alpha = 0.25  # é™ä½æ˜“åˆ†ç±»æ ·æœ¬æƒé‡
            self.args.focal_gamma = 3.0   # å¢å¼ºéš¾æ ·æœ¬èšç„¦
            print("   â†’ é…ç½®ï¼šå¼ºåŒ–Focal Loss (alpha=0.25, gamma=3.0) - ä¸“æ³¨äºéš¾åˆ†ç±»æ ·æœ¬")
            
        elif preset == 'overconfidence_prevention':
            # é˜²æ­¢è¿‡åº¦è‡ªä¿¡é…ç½®
            self.args.loss_type = 'combined'
            self.args.label_smoothing = 0.1
            self.args.confidence_penalty_beta = 0.1
            print("   â†’ é…ç½®ï¼šç»„åˆæŸå¤± (æ ‡ç­¾å¹³æ»‘ + ç½®ä¿¡åº¦æƒ©ç½š) - é˜²æ­¢è¿‡åº¦è‡ªä¿¡")
            
        # === æ–°å¢ä¼˜åŒ–é¢„è®¾é…ç½® ===
        elif preset == 'hvac_similar_optimized':
            # HVACç›¸ä¼¼ç±»åˆ« + é«˜æ€§èƒ½ä¼˜åŒ–ï¼ˆæ¨èï¼‰
            self.args.loss_type = 'label_smoothing_optimized'
            self.args.label_smoothing = 0.10
            self.args.use_timm_loss = True
            print("   â†’ é…ç½®ï¼šä¼˜åŒ–æ ‡ç­¾å¹³æ»‘ (smoothing=0.15, timmåŠ é€Ÿ) - HVACç›¸ä¼¼ç±»åˆ«æœ€ä½³é€‰æ‹©")
            
        elif preset == 'hvac_adaptive':
            # HVACè‡ªé€‚åº”å¹³æ»‘
            self.args.loss_type = 'adaptive_smoothing'
            self.args.adaptive_initial_smoothing = 0.2
            self.args.adaptive_final_smoothing = 0.08
            self.args.adaptive_decay_epochs = 30
            print("   â†’ é…ç½®ï¼šè‡ªé€‚åº”æ ‡ç­¾å¹³æ»‘ (0.2â†’0.08, 30è½®è¡°å‡) - è®­ç»ƒè¿‡ç¨‹åŠ¨æ€è°ƒæ•´")
            
        elif preset == 'hvac_hard_samples':
            # HVACéš¾æ ·æœ¬èšç„¦ + æ ‡ç­¾å¹³æ»‘
            self.args.loss_type = 'hybrid_focal'
            self.args.focal_alpha = 0.8
            self.args.focal_gamma = 2.5
            self.args.label_smoothing = 0.1
            print("   â†’ é…ç½®ï¼šæ··åˆFocal Loss (Î±=0.8, Î³=2.5, smoothing=0.1) - éš¾æ ·æœ¬+ç›¸ä¼¼ç±»åˆ«")
            
        elif preset == 'production_optimized':
            # ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
            self.args.loss_type = 'label_smoothing_optimized'
            self.args.label_smoothing = 0.12
            self.args.use_timm_loss = True
            print("   â†’ é…ç½®ï¼šç”Ÿäº§ç¯å¢ƒä¼˜åŒ– (smoothing=0.12, timmé«˜æ€§èƒ½) - å¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡")
            
        else:
            print(f"   âš ï¸ æœªçŸ¥çš„é¢„è®¾é…ç½®: {preset}")

    def _get_class_weights(self):
        """
        è·å–ç±»åˆ«æƒé‡ï¼ˆé»˜è®¤ç¦ç”¨ï¼Œé€‚ç”¨äºå¹³è¡¡æ•°æ®é›†ï¼‰
        
        Returns:
            torch.Tensor or None: ç±»åˆ«æƒé‡å¼ é‡
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç±»åˆ«æƒé‡
        enable_class_weights = getattr(self.args, 'enable_class_weights', False)
        
        if not enable_class_weights:
            # é»˜è®¤ä¸ä½¿ç”¨ç±»åˆ«æƒé‡ï¼ˆé€‚ç”¨äºå¹³è¡¡æ•°æ®é›†ï¼‰
            return None
        
        print("ğŸ”§ å¯ç”¨ç±»åˆ«æƒé‡åŠŸèƒ½")
        
        # è·å–ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šçš„æƒé‡
        class_weights = getattr(self.args, 'class_weights', None)
        
        if class_weights is not None:
            # è§£æç”¨æˆ·æä¾›çš„æƒé‡å­—ç¬¦ä¸²
            try:
                if isinstance(class_weights, str):
                    weights = [float(w.strip()) for w in class_weights.split(',')]
                    if len(weights) != self.args.num_class:
                        print(f"âš ï¸ ç±»åˆ«æƒé‡æ•°é‡({len(weights)})ä¸ç±»åˆ«æ•°é‡({self.args.num_class})ä¸åŒ¹é…ï¼Œå¿½ç•¥æƒé‡è®¾ç½®")
                        return None
                    class_weights = torch.tensor(weights, dtype=torch.float32)
                    print(f"ğŸ“Š ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ç±»åˆ«æƒé‡: {weights}")
                    return class_weights
            except ValueError as e:
                print(f"âš ï¸ ç±»åˆ«æƒé‡è§£æå¤±è´¥: {e}ï¼Œå°è¯•è‡ªåŠ¨è®¡ç®—")
        
        # è‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆä»…å½“å¯ç”¨æ—¶ï¼‰
        print("ğŸ” å°è¯•ä»è®­ç»ƒæ•°æ®è‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡...")
        try:
            if hasattr(self, 'train_data') and hasattr(self.train_data, 'labels'):
                # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
                unique_labels, counts = np.unique(self.train_data.labels, return_counts=True)
                
                # æ£€æŸ¥æ•°æ®æ˜¯å¦å¹³è¡¡
                count_diff = counts.max() - counts.min()
                if count_diff <= len(unique_labels):  # å…è®¸å°å¹…åº¦å·®å¼‚
                    print(f"ğŸ“Š æ•°æ®é›†è¾ƒä¸ºå¹³è¡¡ï¼ˆæ ·æœ¬æ•°å·®å¼‚â‰¤{len(unique_labels)}ï¼‰ï¼Œå»ºè®®ä¸ä½¿ç”¨ç±»åˆ«æƒé‡")
                    print(f"   å„ç±»åˆ«æ ·æœ¬æ•°: {dict(zip(unique_labels, counts))}")
                    return None
                
                # è®¡ç®—å¹³è¡¡æƒé‡: weight = total_samples / (n_classes * class_count)
                total_samples = len(self.train_data.labels)
                n_classes = len(unique_labels)
                weights = total_samples / (n_classes * counts)
                
                # å½’ä¸€åŒ–æƒé‡ï¼Œä½¿å¾—å¹³å‡æƒé‡ä¸º1
                weights = weights / weights.mean()
                
                class_weights = torch.zeros(self.args.num_class, dtype=torch.float32)
                for label, weight in zip(unique_labels, weights):
                    class_weights[int(label)] = weight
                
                print(f"ğŸ“Š è‡ªåŠ¨è®¡ç®—çš„ç±»åˆ«æƒé‡: {class_weights.tolist()}")
                print(f"   å„ç±»åˆ«æ ·æœ¬æ•°: {dict(zip(unique_labels, counts))}")
                return class_weights
                
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡å¤±è´¥: {e}")
        
        print("ğŸ’¡ å»ºè®®ï¼šå¯¹äºå¹³è¡¡æ•°æ®é›†ï¼Œé€šå¸¸ä¸éœ€è¦ä½¿ç”¨ç±»åˆ«æƒé‡")
        return None

    def _select_criterion(self):
        """
        é€‰æ‹©æŸå¤±å‡½æ•°
        
        æ”¯æŒå¤šç§æŸå¤±å‡½æ•°ä»¥è§£å†³ç±»åˆ«ç›¸ä¼¼æ€§ã€ç±»åˆ«ä¸å¹³è¡¡ç­‰é—®é¢˜:
        - 'ce': æ ‡å‡†äº¤å‰ç†µæŸå¤±
        - 'label_smoothing': æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µï¼ˆæ¨èç”¨äºç›¸ä¼¼ç±»åˆ«ï¼‰
        - 'focal': Focal Lossï¼ˆæ¨èç”¨äºéš¾åˆ†ç±»æ ·æœ¬ï¼‰
        - 'confidence_penalty': ç½®ä¿¡åº¦æƒ©ç½šæŸå¤±ï¼ˆé˜²æ­¢è¿‡åº¦è‡ªä¿¡ï¼‰
        - 'combined': ç»„åˆæŸå¤±ï¼ˆæ ‡ç­¾å¹³æ»‘ + ç½®ä¿¡åº¦æƒ©ç½šï¼‰
        """
        # å¤„ç†é¢„è®¾é…ç½®
        loss_preset = getattr(self.args, 'loss_preset', None)
        if loss_preset:
            self._apply_loss_preset(loss_preset)
            
        loss_type = getattr(self.args, 'loss_type', 'ce')
        
        # å¤„ç†ç±»åˆ«æƒé‡
        class_weights = self._get_class_weights()
        
        if loss_type == 'ce':
            print("ğŸ“ ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±")
            criterion = nn.CrossEntropyLoss(weight=class_weights)
            
        elif loss_type == 'label_smoothing':
            smoothing = getattr(self.args, 'label_smoothing', 0.1)
            use_timm = getattr(self.args, 'use_timm_loss', True)
            print(f"ğŸ“ ä½¿ç”¨æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤± (smoothing={smoothing})")
            print("   â†’ é€‚ç”¨äºç±»åˆ«ç›¸ä¼¼æ€§è¾ƒé«˜çš„æƒ…å†µï¼Œé˜²æ­¢è¿‡åº¦è‡ªä¿¡")
            criterion = LabelSmoothingCrossEntropy(
                smoothing=smoothing, 
                num_classes=self.args.num_class,
                weight=class_weights,
                use_timm=use_timm
            )
            
        elif loss_type == 'label_smoothing_optimized':
            smoothing = getattr(self.args, 'label_smoothing', 0.15)
            print(f"ğŸ“ ä½¿ç”¨ä¼˜åŒ–æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤± (smoothing={smoothing})")
            print("   â†’ é«˜æ€§èƒ½å®ç°ï¼Œæ¯”æ ‡å‡†å®ç°å¿«10-20%")
            criterion = LabelSmoothingCrossEntropy(
                smoothing=smoothing,
                num_classes=self.args.num_class,
                weight=class_weights,
                use_timm=True
            )
            
        elif loss_type == 'hybrid_focal':
            alpha = getattr(self.args, 'focal_alpha', 0.8)
            gamma = getattr(self.args, 'focal_gamma', 2.5)
            smoothing = getattr(self.args, 'label_smoothing', 0.1)
            print(f"ğŸ“ ä½¿ç”¨æ··åˆFocal Loss (Î±={alpha}, Î³={gamma}, smoothing={smoothing})")
            print("   â†’ ç»“åˆéš¾æ ·æœ¬èšç„¦å’Œæ ‡ç­¾å¹³æ»‘çš„ä¼˜åŠ¿")
            criterion = HybridFocalLoss(
                alpha=alpha,
                gamma=gamma,
                smoothing=smoothing,
                weight=class_weights
            )
            
        elif loss_type == 'adaptive_smoothing':
            initial_smoothing = getattr(self.args, 'adaptive_initial_smoothing', 0.2)
            final_smoothing = getattr(self.args, 'adaptive_final_smoothing', 0.05)
            decay_epochs = getattr(self.args, 'adaptive_decay_epochs', 30)
            print(f"ğŸ“ ä½¿ç”¨è‡ªé€‚åº”æ ‡ç­¾å¹³æ»‘æŸå¤±")
            print(f"   â†’ åŠ¨æ€è°ƒæ•´: {initial_smoothing} â†’ {final_smoothing} (è¡°å‡å‘¨æœŸ: {decay_epochs})")
            
            base_loss = LabelSmoothingCrossEntropy(
                smoothing=initial_smoothing,
                num_classes=self.args.num_class,
                weight=class_weights,
                use_timm=True
            )
            criterion = AdaptiveLossScheduler(
                base_loss=base_loss,
                initial_smoothing=initial_smoothing,
                final_smoothing=final_smoothing,
                decay_epochs=decay_epochs
            )
            
        elif loss_type == 'focal':
            alpha = getattr(self.args, 'focal_alpha', 1.0)
            gamma = getattr(self.args, 'focal_gamma', 2.0)
            print(f"ğŸ“ ä½¿ç”¨Focal Loss (alpha={alpha}, gamma={gamma})")
            print("   â†’ é€‚ç”¨äºéš¾åˆ†ç±»æ ·æœ¬å’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜")
            criterion = FocalLoss(
                alpha=alpha,
                gamma=gamma,
                weight=class_weights
            )
            
        elif loss_type == 'confidence_penalty':
            beta = getattr(self.args, 'confidence_penalty_beta', 0.1)
            print(f"ğŸ“ ä½¿ç”¨ç½®ä¿¡åº¦æƒ©ç½šæŸå¤± (beta={beta})")
            print("   â†’ é˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼Œé¼“åŠ±æ›´å¹³è¡¡çš„é¢„æµ‹")
            criterion = ConfidencePenaltyLoss(beta=beta)
            
        elif loss_type == 'combined':
            smoothing = getattr(self.args, 'label_smoothing', 0.1)
            penalty_beta = getattr(self.args, 'confidence_penalty_beta', 0.05)
            print(f"ğŸ“ ä½¿ç”¨ç»„åˆæŸå¤± (æ ‡ç­¾å¹³æ»‘: {smoothing}, ç½®ä¿¡åº¦æƒ©ç½š: {penalty_beta})")
            print("   â†’ ç»¼åˆè§£å†³æ–¹æ¡ˆï¼šç¼“è§£ç±»åˆ«ç›¸ä¼¼æ€§ + é˜²æ­¢è¿‡åº¦è‡ªä¿¡")
            
            label_smooth_loss = LabelSmoothingCrossEntropy(
                smoothing=smoothing, 
                num_classes=self.args.num_class,
                weight=class_weights
            )
            penalty_loss = ConfidencePenaltyLoss(beta=penalty_beta)
            
            losses = {
                'label_smoothing': (label_smooth_loss, 1.0),
                'confidence_penalty': (penalty_loss, 0.5)
            }
            criterion = CombinedLoss(losses)
            
        else:
            print(f"âš ï¸  æœªçŸ¥çš„æŸå¤±å‡½æ•°ç±»å‹: {loss_type}ï¼Œä½¿ç”¨é»˜è®¤äº¤å‰ç†µ")
            criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        # æ˜¾ç¤ºç±»åˆ«æƒé‡çŠ¶æ€
        if class_weights is not None:
            print(f"   ç±»åˆ«æƒé‡: å·²å¯ç”¨ {class_weights.tolist()}")
        else:
            print(f"   ç±»åˆ«æƒé‡: ç¦ç”¨ï¼ˆé»˜è®¤ï¼Œé€‚ç”¨äºå¹³è¡¡æ•°æ®é›†ï¼‰")
        
        return criterion
    
    def _select_evaluation_criterion(self):
        """
        é€‰æ‹©è¯„ä¼°æŸå¤±å‡½æ•°
        
        æ ¹æ®ä¸»æµæœºå™¨å­¦ä¹ è§„èŒƒï¼ŒéªŒè¯å’Œæµ‹è¯•é˜¶æ®µåº”ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±æ¥æä¾›ï¼š
        1. å®¢è§‚çš„æ€§èƒ½è¯„ä¼°
        2. ä¸åŒæ–¹æ³•é—´çš„å…¬å¹³æ¯”è¾ƒ
        3. çœŸå®çš„æ³›åŒ–èƒ½åŠ›æŒ‡æ ‡
        4. é¿å…è®­ç»ƒæŠ€å·§å¯¹è¯„ä¼°æŒ‡æ ‡çš„å½±å“
        
        Returns:
            nn.Module: æ ‡å‡†äº¤å‰ç†µæŸå¤±å‡½æ•°
        """
        # è·å–ç±»åˆ«æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        class_weights = self._get_class_weights()
        
        # å§‹ç»ˆä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±è¿›è¡Œè¯„ä¼°
        eval_criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')
        
        print("ğŸ¯ éªŒè¯/æµ‹è¯•é˜¶æ®µä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼ˆéµå¾ªä¸»æµMLè§„èŒƒï¼‰")
        if class_weights is not None:
            print(f"   è¯„ä¼°æŸå¤±ç±»åˆ«æƒé‡: å·²å¯ç”¨ {class_weights.tolist()}")
        else:
            print(f"   è¯„ä¼°æŸå¤±ç±»åˆ«æƒé‡: ç¦ç”¨")
        
        return eval_criterion
    
    def _mixup_criterion(self, pred, labels_a, labels_b, lam):
        """MixupæŸå¤±å‡½æ•°"""
        return lam * self.criterion(pred, labels_a) + (1 - lam) * self.criterion(pred, labels_b)

    def vali(self):
        total_loss = []
        all_preds = []
        all_labels = []
        self.model.eval()
        
        # ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±è¿›è¡Œè¯„ä¼°ï¼ˆéµå¾ªä¸»æµMLè§„èŒƒï¼‰
        eval_criterion = self._select_evaluation_criterion()
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.vali_loader):
                try:
                    if hasattr(batch, 'x') and hasattr(batch, 'y'):
                        # GNNæ•°æ®
                        if batch.y.numel() == 0:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                            continue
                        batch=batch.to(self.device)
                        out = self.model(batch)
                        loss = eval_criterion(out, batch.y)  # ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±
                        pred = out.argmax(dim=1)
                        
                        # å¤„ç†æ ‡ç­¾å½¢çŠ¶
                        label_squeezed = batch.y.long().squeeze(-1) if batch.y.dim() > 1 else batch.y.long()
                        
                        # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                            print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                            print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                            continue
                        
                        # ç¡®ä¿ç»´åº¦ä¸€è‡´
                        if pred.shape != label_squeezed.shape:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                            print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                            continue
                        
                        all_preds.extend(pred.cpu().numpy())
                        all_labels.extend(label_squeezed.cpu().numpy())
                    elif len(batch) == 3:
                        # åŒè·¯GAFæ•°æ®ï¼ˆæ—§ç‰ˆæœ¬æ ¼å¼ï¼‰
                        sum_data, diff_data, label = batch
                        # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                        if sum_data.size(0) == 0 or diff_data.size(0) == 0 or label.numel() == 0:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                            continue
                        
                        sum_data = sum_data.float().to(self.device)
                        diff_data = diff_data.float().to(self.device)
                        label = label.to(self.device)
                        
                        # è¿›ä¸€æ­¥æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if torch.isnan(sum_data).any() or torch.isinf(sum_data).any() or torch.isnan(diff_data).any() or torch.isinf(diff_data).any():
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
                            continue
                        
                        out = self.model(sum_data, diff_data)
                        
                        # æ£€æŸ¥è¾“å‡ºå’Œæ ‡ç­¾çš„ç»´åº¦åŒ¹é…
                        if out.size(0) != label.size(0):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é… (out: {out.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                            continue
                        
                        # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼Œé¿å…é”™è¯¯åœ°squeezeæ‰æ‰¹æ¬¡ç»´åº¦
                        label_for_loss = label.long()
                        if label_for_loss.dim() > 1 and label_for_loss.size(-1) == 1:
                            label_for_loss = label_for_loss.squeeze(-1)
                        loss = eval_criterion(out, label_for_loss)  # ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±
                        pred = out.argmax(dim=1)
                        
                        # å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼šå®‰å…¨å¤„ç†æ ‡é‡æ ‡ç­¾
                        if label.dim() == 0:  # æ ‡é‡æ ‡ç­¾
                            label_squeezed = label.long().unsqueeze(0)  # è½¬æ¢ä¸º[1]
                        else:
                            label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                        
                        # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                            print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                            print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                            continue
                        
                        # ç¡®ä¿ç»´åº¦ä¸€è‡´
                        if pred.shape != label_squeezed.shape:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                            print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                            continue
                        
                        all_preds.extend(pred.cpu().numpy())
                        all_labels.extend(label_squeezed.cpu().numpy())
                    elif len(batch) == 4:
                        # å¢å¼ºåŒè·¯GAFæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬æ ¼å¼ï¼‰
                        sum_data, diff_data, time_series_data, label = batch
                        # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                        if sum_data.size(0) == 0 or diff_data.size(0) == 0 or time_series_data.size(0) == 0 or label.numel() == 0:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                            continue
                        
                        sum_data = sum_data.float().to(self.device)
                        diff_data = diff_data.float().to(self.device)
                        time_series_data = time_series_data.float().to(self.device)
                        label = label.to(self.device)
                        
                        # è¿›ä¸€æ­¥æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if (torch.isnan(sum_data).any() or torch.isinf(sum_data).any() or 
                            torch.isnan(diff_data).any() or torch.isinf(diff_data).any() or
                            torch.isnan(time_series_data).any() or torch.isinf(time_series_data).any()):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
                            continue
                        
                        out = self.model(sum_data, diff_data, time_series_data)
                        
                        # æ£€æŸ¥è¾“å‡ºå’Œæ ‡ç­¾çš„ç»´åº¦åŒ¹é…
                        if out.size(0) != label.size(0):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é… (out: {out.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                            continue
                        
                        # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼Œé¿å…é”™è¯¯åœ°squeezeæ‰æ‰¹æ¬¡ç»´åº¦
                        label_for_loss = label.long()
                        if label_for_loss.dim() > 1 and label_for_loss.size(-1) == 1:
                            label_for_loss = label_for_loss.squeeze(-1)
                        loss = eval_criterion(out, label_for_loss)  # ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±
                        pred = out.argmax(dim=1)
                        
                        # å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼šå®‰å…¨å¤„ç†æ ‡é‡æ ‡ç­¾
                        if label.dim() == 0:  # æ ‡é‡æ ‡ç­¾
                            label_squeezed = label.long().unsqueeze(0)  # è½¬æ¢ä¸º[1]
                        else:
                            label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                        
                        # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                            print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                            print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                            continue
                        
                        # ç¡®ä¿ç»´åº¦ä¸€è‡´
                        if pred.shape != label_squeezed.shape:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                            print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                            continue
                        
                        all_preds.extend(pred.cpu().numpy())
                        all_labels.extend(label_squeezed.cpu().numpy())
                    else:
                        # æ™®é€šåˆ†ç±»æ•°æ®
                        batch_x, label = batch
                        # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                        if batch_x.size(0) == 0 or label.numel() == 0:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} ä¸ºç©º (batch_x: {batch_x.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                            continue
                        
                        batch_x = batch_x.float().to(self.device)
                        label = label.to(self.device)
                        
                        # è¿›ä¸€æ­¥æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if torch.isnan(batch_x).any() or torch.isinf(batch_x).any():
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
                            continue
                        
                        out = self.model(batch_x)
                        
                        # æ£€æŸ¥è¾“å‡ºå’Œæ ‡ç­¾çš„ç»´åº¦åŒ¹é…
                        if out.size(0) != label.size(0):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é… (out: {out.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                            continue
                        
                        # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼Œé¿å…é”™è¯¯åœ°squeezeæ‰æ‰¹æ¬¡ç»´åº¦
                        label_for_loss = label.long()
                        if label_for_loss.dim() > 1 and label_for_loss.size(-1) == 1:
                            label_for_loss = label_for_loss.squeeze(-1)
                        loss = eval_criterion(out, label_for_loss)  # ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±
                        pred = out.argmax(dim=1)
                        
                        # å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼šå®‰å…¨å¤„ç†æ ‡é‡æ ‡ç­¾
                        if label.dim() == 0:  # æ ‡é‡æ ‡ç­¾
                            label_squeezed = label.long().unsqueeze(0)  # è½¬æ¢ä¸º[1]
                        else:
                            label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                        
                        # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                            print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                            print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                            continue
                        
                        # ç¡®ä¿ç»´åº¦ä¸€è‡´
                        if pred.shape != label_squeezed.shape:
                            print(f"è­¦å‘Š: éªŒè¯æ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                            print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                            continue
                        
                        all_preds.extend(pred.cpu().numpy())
                        all_labels.extend(label_squeezed.cpu().numpy())
                    total_loss.append(loss.item())
                    
                except Exception as e:
                    print(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºç°é”™è¯¯: {e}")
                    # æ ¹æ®ä¸åŒçš„æ•°æ®æ ¼å¼æ˜¾ç¤ºç›¸åº”çš„å˜é‡ä¿¡æ¯
                    if 'batch_x' in locals():
                        print(f"æ‰¹æ¬¡ä¿¡æ¯: batch_x shape: {batch_x.shape}, label shape: {label.shape if 'label' in locals() else 'N/A'}")
                    elif 'sum_data' in locals() and 'diff_data' in locals():
                        if 'time_series_data' in locals():
                            print(f"æ‰¹æ¬¡ä¿¡æ¯: sum_data shape: {sum_data.shape}, diff_data shape: {diff_data.shape}, time_series_data shape: {time_series_data.shape}, label shape: {label.shape if 'label' in locals() else 'N/A'}")
                        else:
                            print(f"æ‰¹æ¬¡ä¿¡æ¯: sum_data shape: {sum_data.shape}, diff_data shape: {diff_data.shape}, label shape: {label.shape if 'label' in locals() else 'N/A'}")
                    elif hasattr(batch, 'x') and hasattr(batch, 'y'):
                        print(f"æ‰¹æ¬¡ä¿¡æ¯: GNN batch.x shape: {batch.x.shape}, batch.y shape: {batch.y.shape}")
                    else:
                        print(f"æ‰¹æ¬¡ä¿¡æ¯: æœªçŸ¥æ•°æ®æ ¼å¼, batch type: {type(batch)}, label shape: {label.shape if 'label' in locals() else 'N/A'}")
                    continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„éªŒè¯æ•°æ®
        if len(all_labels) == 0 or len(all_preds) == 0:
            print("è­¦å‘Š: éªŒè¯é›†ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æŒ‡æ ‡")
            return {
                'loss': float('inf'),
                'accuracy': 0.0,
                'f1_macro': 0.0,
                'f1_weighted': 0.0,
                'precision': 0.0,
                'recall': 0.0
            }
        
        # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
        avg_loss = np.average(total_loss) if total_loss else float('inf')
        acc = accuracy_score(all_labels, all_preds)
        
        # è®¡ç®—F1åˆ†æ•°ï¼ˆå¤„ç†å¤šåˆ†ç±»å’ŒäºŒåˆ†ç±»æƒ…å†µï¼‰
        if self.args.num_class <= 2:
            # äºŒåˆ†ç±»æƒ…å†µ
            f1_macro = f1_score(all_labels, all_preds, average='binary')
            f1_weighted = f1_score(all_labels, all_preds, average='binary')
            precision = precision_score(all_labels, all_preds, average='binary')
            recall = recall_score(all_labels, all_preds, average='binary')
        else:
            # å¤šåˆ†ç±»æƒ…å†µ
            f1_macro = f1_score(all_labels, all_preds, average='macro')
            f1_weighted = f1_score(all_labels, all_preds, average='weighted')
            precision = precision_score(all_labels, all_preds, average='macro')
            recall = recall_score(all_labels, all_preds, average='macro')
        
        metrics = {
            'loss': avg_loss,
            'accuracy': acc,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall
        }
        
        self.model.train()
        return metrics

    def train(self):
        # è®°å½•å®éªŒå¼€å§‹ä¿¡æ¯
        self.log_config()
        self.log_model_info()
        self.log_data_info()
        
        path = os.path.join(self.args.checkpoints, self.setting)
        if not os.path.exists(path):    
            os.makedirs(path)
        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)
        model_optim = self._select_optimizer()
        lr_scheduler = self._select_lr_scheduler(model_optim)  # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        
        # è®°å½•å®é™…ä½¿ç”¨çš„è°ƒåº¦å™¨ç±»å‹
        actual_scheduler_type = getattr(self.args, 'lr_scheduler_type', 'f1_based')
        
        self.criterion = self._select_criterion()
        
        # æŒ‰ç…§ä¸»æµMLè§„èŒƒè¯´æ˜æŸå¤±å‡½æ•°ä½¿ç”¨ç­–ç•¥
        self.log_info("\n" + "="*80)
        self.log_info("ğŸ“‹ æŸå¤±å‡½æ•°ä½¿ç”¨ç­–ç•¥ï¼ˆéµå¾ªä¸»æµæœºå™¨å­¦ä¹ è§„èŒƒï¼‰:")
        self.log_info(f"   ğŸ¯ è®­ç»ƒé˜¶æ®µ: ä½¿ç”¨ {self.args.loss_type if hasattr(self.args, 'loss_type') else 'ce'} æŸå¤±å‡½æ•°")
        self.log_info(f"      â†’ ç›®çš„ï¼šä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œè§£å†³ç‰¹å®šé—®é¢˜ï¼ˆç±»åˆ«ä¸å¹³è¡¡ã€ç›¸ä¼¼æ€§ç­‰ï¼‰")
        self.log_info(f"   ğŸ“Š éªŒè¯é˜¶æ®µ: ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±å‡½æ•°")
        self.log_info(f"      â†’ ç›®çš„ï¼šå®¢è§‚è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¾¿äºä¸åŒæ–¹æ³•æ¯”è¾ƒ")
        self.log_info(f"   ğŸ“ˆ æœ€ç»ˆæŠ¥å‘Š: åŸºäºæ ‡å‡†äº¤å‰ç†µæŸå¤±çš„æŒ‡æ ‡")
        self.log_info(f"      â†’ ç›®çš„ï¼šæä¾›å¯ä¿¡ã€å¯æ¯”è¾ƒçš„æ€§èƒ½æŒ‡æ ‡")
        self.log_info("="*80 + "\n")
        
        # æ¢¯åº¦ç´¯ç§¯é…ç½®
        gradient_accumulation_steps = getattr(self.args, 'gradient_accumulation_steps', 1)
        if gradient_accumulation_steps > 1:
            self.log_info(f"ğŸ”„ æ¢¯åº¦ç´¯ç§¯é…ç½®: æ¯{gradient_accumulation_steps}è½®ç´¯ç§¯ä¸€æ¬¡æ¢¯åº¦")
            self.log_info(f"   å®é™…batch_size: {self.args.batch_size}")
            self.log_info(f"   æœ‰æ•ˆbatch_size: {self.args.batch_size * gradient_accumulation_steps}")
        else:
            self.log_info(f"ğŸ”„ æ¢¯åº¦ç´¯ç§¯: å·²ç¦ç”¨ï¼ˆgradient_accumulation_steps=1ï¼‰")
        
        train_losses = []
        train_accs = []
        learning_rates = []  # æ·»åŠ å­¦ä¹ ç‡è®°å½•
        val_metrics_history = {
            'loss': [],
            'accuracy': [],
            'f1_macro': [],
            'f1_weighted': [],
            'precision': [],
            'recall': []
        }
        
        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡è¿½è¸ª
        best_val_f1 = 0.0
        

        
        for epoch in range(self.args.train_epochs):
            # æ›´æ–°è‡ªé€‚åº”æŸå¤±è°ƒåº¦å™¨ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
            if hasattr(self.criterion, 'set_epoch'):
                self.criterion.set_epoch(epoch)
            
            total_loss = 0
            correct = 0
            total = 0
            accumulated_loss = 0  # ç´¯ç§¯çš„æŸå¤±
            
            self.model.train()
            if gradient_accumulation_steps > 1:
                train_bar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.args.train_epochs} [GA:{gradient_accumulation_steps}]', ncols=100)
                # æ¢¯åº¦ç´¯ç§¯æ¨¡å¼ï¼šåˆå§‹åŒ–æ¢¯åº¦æ¸…é›¶
                model_optim.zero_grad()
            else:
                train_bar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.args.train_epochs}', ncols=100)
            
            for batch_idx, batch in enumerate(train_bar):
                # åˆ¤æ–­æ˜¯å›¾ç±»å‹çš„æ•°æ®é›†è¿˜æ˜¯å›¾åƒæ•°æ®é›†
                if hasattr(batch, 'x') and hasattr(batch, 'y'):
                    # GNNæ•°æ®
                    if batch.y.numel() == 0:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    batch=batch.to(self.device)
                    out = self.model(batch)
                    loss = self.criterion(out, batch.y)
                    pred = out.argmax(dim=1)
                    correct += (pred == batch.y).sum().item()
                    total += batch.y.size(0)
                elif len(batch) == 3:
                    # åŒè·¯GAFæ•°æ®ï¼ˆæ—§ç‰ˆæœ¬æ ¼å¼ï¼‰
                    sum_data, diff_data, label = batch
                    # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                    if sum_data.size(0) == 0 or diff_data.size(0) == 0 or label.numel() == 0:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                        
                    sum_data = sum_data.float().to(self.device)
                    diff_data = diff_data.float().to(self.device)
                    label = label.to(self.device)
                    
                    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                    if torch.isnan(sum_data).any() or torch.isinf(sum_data).any() or torch.isnan(diff_data).any() or torch.isinf(diff_data).any():
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
                        continue
                    
                    # å¸¸è§„è®­ç»ƒï¼ˆæ•°æ®å¢å¼ºå·²åœ¨æ•°æ®é›†ä¸­å®Œæˆï¼‰
                    out = self.model(sum_data, diff_data)
                    
                    # æ£€æŸ¥è¾“å‡ºå’Œæ ‡ç­¾çš„ç»´åº¦åŒ¹é…
                    if out.size(0) != label.size(0):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é… (out: {out.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                        continue
                    
                    # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼Œé¿å…é”™è¯¯åœ°squeezeæ‰æ‰¹æ¬¡ç»´åº¦
                    label_for_loss = label.long()
                    if label_for_loss.dim() > 1 and label_for_loss.size(-1) == 1:
                        label_for_loss = label_for_loss.squeeze(-1)
                    loss = self.criterion(out, label_for_loss)
                    pred = out.argmax(dim=1)
                    # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼šç¡®ä¿æ ‡ç­¾æ˜¯1ç»´çš„
                    if label.dim() == 0:  # æ ‡é‡æ ‡ç­¾
                        label_squeezed = label.long().unsqueeze(0)  # è½¬æ¢ä¸º[1]
                    else:
                        label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                    
                    # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                        print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                        print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                        continue
                    
                    # ç¡®ä¿ç»´åº¦ä¸€è‡´
                    if pred.shape != label_squeezed.shape:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                        print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                        continue
                    
                    correct += (pred == label_squeezed).sum().item()
                    total += label_squeezed.size(0)
                elif len(batch) == 4:
                    # å¢å¼ºåŒè·¯GAFæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬æ ¼å¼ï¼‰
                    sum_data, diff_data, time_series_data, label = batch
                    # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                    if sum_data.size(0) == 0 or diff_data.size(0) == 0 or time_series_data.size(0) == 0 or label.numel() == 0:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                        
                    sum_data = sum_data.float().to(self.device)
                    diff_data = diff_data.float().to(self.device)
                    time_series_data = time_series_data.float().to(self.device)
                    label = label.to(self.device)
                    
                    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                    if (torch.isnan(sum_data).any() or torch.isinf(sum_data).any() or 
                        torch.isnan(diff_data).any() or torch.isinf(diff_data).any() or
                        torch.isnan(time_series_data).any() or torch.isinf(time_series_data).any()):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
                        continue
                    
                    # å¸¸è§„è®­ç»ƒï¼ˆä½¿ç”¨ç»Ÿè®¡ç‰¹å¾å¢å¼ºï¼‰
                    out = self.model(sum_data, diff_data, time_series_data)
                    
                    # æ£€æŸ¥è¾“å‡ºå’Œæ ‡ç­¾çš„ç»´åº¦åŒ¹é…
                    if out.size(0) != label.size(0):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é… (out: {out.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                        continue
                    
                    # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼Œé¿å…é”™è¯¯åœ°squeezeæ‰æ‰¹æ¬¡ç»´åº¦
                    label_for_loss = label.long()
                    if label_for_loss.dim() > 1 and label_for_loss.size(-1) == 1:
                        label_for_loss = label_for_loss.squeeze(-1)
                    loss = self.criterion(out, label_for_loss)
                    pred = out.argmax(dim=1)
                    # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼šç¡®ä¿æ ‡ç­¾æ˜¯1ç»´çš„
                    if label.dim() == 0:  # æ ‡é‡æ ‡ç­¾
                        label_squeezed = label.long().unsqueeze(0)  # è½¬æ¢ä¸º[1]
                    else:
                        label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                    
                    # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                        print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                        print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                        continue
                    
                    # ç¡®ä¿ç»´åº¦ä¸€è‡´
                    if pred.shape != label_squeezed.shape:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                        print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                        continue
                    
                    correct += (pred == label_squeezed).sum().item()
                    total += label_squeezed.size(0)
                else:
                    # æ™®é€šåˆ†ç±»æ•°æ®
                    batch_x, label = batch
                    # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                    if batch_x.size(0) == 0 or label.numel() == 0:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} ä¸ºç©º (batch_x: {batch_x.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                        continue
                        
                    batch_x = batch_x.float().to(self.device)
                    label = label.to(self.device)
                    
                    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                    if torch.isnan(batch_x).any() or torch.isinf(batch_x).any():
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
                        continue
                    
                    out = self.model(batch_x)
                    
                    # æ£€æŸ¥è¾“å‡ºå’Œæ ‡ç­¾çš„ç»´åº¦åŒ¹é…
                    if out.size(0) != label.size(0):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é… (out: {out.shape}, label: {label.shape})ï¼Œè·³è¿‡")
                        continue
                    
                    # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼Œé¿å…é”™è¯¯åœ°squeezeæ‰æ‰¹æ¬¡ç»´åº¦
                    label_for_loss = label.long()
                    if label_for_loss.dim() > 1 and label_for_loss.size(-1) == 1:
                        label_for_loss = label_for_loss.squeeze(-1)
                    loss = self.criterion(out, label_for_loss)
                    pred = out.argmax(dim=1)
                    
                    # ç¡®ä¿æ ‡ç­¾å’Œé¢„æµ‹å€¼å½¢çŠ¶ä¸€è‡´ï¼Œå¹¶æ£€æŸ¥æ ‡ç­¾å€¼èŒƒå›´
                    # å®‰å…¨å¤„ç†æ ‡ç­¾å½¢çŠ¶ï¼šç¡®ä¿æ ‡ç­¾æ˜¯1ç»´çš„
                    if label.dim() == 0:  # æ ‡é‡æ ‡ç­¾
                        label_squeezed = label.long().unsqueeze(0)  # è½¬æ¢ä¸º[1]
                    else:
                        label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                    
                    # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if torch.any(label_squeezed < 0) or torch.any(label_squeezed >= out.size(1)):
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´")
                        print(f"æ ‡ç­¾èŒƒå›´: [{label_squeezed.min().item()}, {label_squeezed.max().item()}]")
                        print(f"æœŸæœ›èŒƒå›´: [0, {out.size(1)-1}]")
                        continue
                    
                    # ç¡®ä¿ç»´åº¦ä¸€è‡´
                    if pred.shape != label_squeezed.shape:
                        print(f"è­¦å‘Š: è®­ç»ƒæ‰¹æ¬¡ {batch_idx} é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…")
                        print(f"pred shape: {pred.shape}, label shape: {label_squeezed.shape}")
                        continue
                    
                    correct += (pred == label_squeezed).sum().item()
                    total += label_squeezed.size(0)
                
                if gradient_accumulation_steps > 1:
                    # æ¢¯åº¦ç´¯ç§¯æ¨¡å¼ï¼šå°†æŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                    loss = loss / gradient_accumulation_steps
                    loss.backward()
                    
                    # ç´¯ç§¯æŸå¤±ç”¨äºæ˜¾ç¤º
                    accumulated_loss += loss.item()
                    total_loss += loss.item() * gradient_accumulation_steps  # æ¢å¤åŸå§‹æŸå¤±å¤§å°ç”¨äºç»Ÿè®¡
                    
                    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç´¯ç§¯æ­¥æ•°æˆ–æ˜¯æœ€åä¸€ä¸ªbatch
                    is_accumulation_step = (batch_idx + 1) % gradient_accumulation_steps == 0
                    is_last_batch = batch_idx == len(self.train_loader) - 1
                    
                    if is_accumulation_step or is_last_batch:
                        # è¿›è¡Œæ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–°
                        nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        model_optim.step()
                        model_optim.zero_grad()  # æ¸…é›¶æ¢¯åº¦ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡ç´¯ç§¯
                        
                        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºç´¯ç§¯çš„æŸå¤±
                        display_loss = accumulated_loss * gradient_accumulation_steps if is_accumulation_step else accumulated_loss * gradient_accumulation_steps / ((batch_idx % gradient_accumulation_steps) + 1)
                        accumulated_loss = 0  # é‡ç½®ç´¯ç§¯æŸå¤±
                    else:
                        # åªè®¡ç®—ç´¯ç§¯æŸå¤±ï¼Œä¸æ›´æ–°æ˜¾ç¤º
                        display_loss = accumulated_loss * gradient_accumulation_steps / ((batch_idx % gradient_accumulation_steps) + 1)
                    
                    train_bar.set_postfix({
                        'Loss': f'{display_loss:.4f}',
                        'Acc': f'{100*correct/total:.2f}%' if total > 0 else 'Acc: 0.00%',
                        'GA': f'{(batch_idx % gradient_accumulation_steps) + 1}/{gradient_accumulation_steps}'
                    })
                else:
                    # æ­£å¸¸è®­ç»ƒæ¨¡å¼ï¼ˆæ— æ¢¯åº¦ç´¯ç§¯ï¼‰
                    loss.backward()
                    nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)
                    model_optim.step()
                    model_optim.zero_grad()
                    
                    total_loss += loss.item()
                    train_bar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{100*correct/total:.2f}%' if total > 0 else 'Acc: 0.00%'
                    })
            
            train_loss = total_loss / len(self.train_loader)
            train_acc = correct / total
            val_metrics = self.vali()
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = model_optim.param_groups[0]['lr']
            
            # ä¿å­˜è®­ç»ƒå†å²
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            learning_rates.append(current_lr)
            for key in val_metrics_history.keys():
                val_metrics_history[key].append(val_metrics[key])
            
            # æ‰“å°è¯¦ç»†çš„è®­ç»ƒä¿¡æ¯
            print(f"Epoch: {epoch+1}")
            print(f"  è®­ç»ƒ - Loss({self.args.loss_type if hasattr(self.args, 'loss_type') else 'ce'}): {train_loss:.4f}, Acc: {train_acc:.4f}")
            print(f"  éªŒè¯ - Loss(æ ‡å‡†CE): {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}")
            print(f"        F1(macro): {val_metrics['f1_macro']:.4f}, F1(weighted): {val_metrics['f1_weighted']:.4f}")
            print(f"        Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}")
            print("-" * 80)
            
            # è®°å½•æ¯è½®çš„è¯¦ç»†æŒ‡æ ‡åˆ°æ—¥å¿—
            self.log_epoch_metrics(epoch, train_loss, train_acc, val_metrics, current_lr, actual_scheduler_type)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ™ºèƒ½é€‰æ‹©ç›‘æ§æŒ‡æ ‡ï¼‰
            try:
                if actual_scheduler_type in ['composite_f1_priority', 'composite_weighted', 'composite_loss_priority']:
                    # å¤åˆè°ƒåº¦å™¨éœ€è¦ä¸¤ä¸ªå‚æ•°
                    if hasattr(lr_scheduler, 'step') and len(lr_scheduler.step.__code__.co_varnames) > 2:
                        lr_scheduler.step(val_metrics['loss'], val_metrics['f1_weighted'])
                    else:
                        # å›é€€ç­–ç•¥ï¼šå¦‚æœä¸æ˜¯çœŸæ­£çš„å¤åˆè°ƒåº¦å™¨ï¼Œä½¿ç”¨F1åˆ†æ•°
                        self.log_warning(f"å¤åˆè°ƒåº¦å™¨è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°F1åˆ†æ•°è°ƒåº¦")
                        lr_scheduler.step(val_metrics['f1_weighted'])
                elif actual_scheduler_type == 'f1_based':
                    # F1è°ƒåº¦å™¨ï¼šç›‘æ§F1åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
                    lr_scheduler.step(val_metrics['f1_weighted'])
                else:
                    # æŸå¤±è°ƒåº¦å™¨ï¼ˆé»˜è®¤ï¼‰ï¼šç›‘æ§éªŒè¯æŸå¤±ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                    lr_scheduler.step(val_metrics['loss'])
            except Exception as e:
                self.log_error(f"å­¦ä¹ ç‡è°ƒåº¦å™¨è°ƒç”¨å¤±è´¥: {e}")
                self.log_warning(f"å›é€€åˆ°æŸå¤±è°ƒåº¦ç­–ç•¥")
                # å®‰å…¨å›é€€ï¼šä½¿ç”¨éªŒè¯æŸå¤±
                if hasattr(lr_scheduler, 'step'):
                    lr_scheduler.step(val_metrics['loss'])
            
            new_lr = model_optim.param_groups[0]['lr']
            
            # å¦‚æœå­¦ä¹ ç‡å‘ç”Ÿå˜åŒ–ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯å¹¶è®°å½•åˆ°æ—¥å¿—
            if new_lr != current_lr:
                if actual_scheduler_type == 'f1_based':
                    print(f"ğŸ¯ åŸºäºF1åˆ†æ•°è°ƒæ•´å­¦ä¹ ç‡: {current_lr:.6f} â†’ {new_lr:.6f}")
                    print(f"   å½“å‰F1åˆ†æ•°: {val_metrics['f1_weighted']:.4f}")
                elif actual_scheduler_type.startswith('composite'):
                    print(f"ğŸ”„ å¤åˆæŒ‡æ ‡è°ƒæ•´å­¦ä¹ ç‡: {current_lr:.6f} â†’ {new_lr:.6f}")
                    print(f"   F1åˆ†æ•°: {val_metrics['f1_weighted']:.4f}, éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")
                else:
                    print(f"ğŸ“‰ åŸºäºéªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡: {current_lr:.6f} â†’ {new_lr:.6f}")
                    print(f"   å½“å‰éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")
                
                # è®°å½•å­¦ä¹ ç‡å˜åŒ–åˆ°æ—¥å¿—
                self.log_learning_rate_change(current_lr, new_lr, actual_scheduler_type, val_metrics)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best_model = False
            if val_metrics['f1_weighted'] > best_val_f1:
                best_val_f1 = val_metrics['f1_weighted']
                is_best_model = True
            
            # è®°å½•ç®€è¦æ€»ç»“åˆ°æ—¥å¿—
            self.log_epoch_summary(epoch, train_loss, train_acc, val_metrics, new_lr, is_best_model)
            
            # ä½¿ç”¨F1åˆ†æ•°è¿›è¡Œæ—©åœï¼ˆä¹Ÿå¯ä»¥é€‰æ‹©ä½¿ç”¨å‡†ç¡®ç‡æˆ–å…¶ä»–æŒ‡æ ‡ï¼‰
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨åŠ æƒF1åˆ†æ•°ï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†ç±»åˆ«ä¸å¹³è¡¡
            early_stopping(-val_metrics['f1_weighted'], self.model, path)
            if early_stopping.early_stop:
                print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                self.log_info(f"â¹ï¸ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break
        # è®­ç»ƒç»“æŸååŠ è½½æœ€ä¼˜æ¨¡å‹
        best_model_path = os.path.join(path, 'checkpoint.pth')
        self.model.load_state_dict(torch.load(best_model_path))
        print("ğŸ”„ å·²åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œæ­£åœ¨è¯„ä¼°çœŸå®æ€§èƒ½...")
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹é‡æ–°è¯„ä¼°éªŒè¯é›†ï¼Œè·å–çœŸå®çš„æœ€ä½³æ€§èƒ½æŒ‡æ ‡
        best_model_metrics = self.vali()
        
        # æ„å»ºæ›´å®Œæ•´çš„è®­ç»ƒå†å²è®°å½•
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'learning_rates': learning_rates,  # æ·»åŠ å­¦ä¹ ç‡å†å²
            'val_metrics': val_metrics_history,
            'best_val_acc': max(val_metrics_history['accuracy']) if val_metrics_history['accuracy'] else 0,
            'best_val_f1_macro': max(val_metrics_history['f1_macro']) if val_metrics_history['f1_macro'] else 0,
            'best_val_f1_weighted': max(val_metrics_history['f1_weighted']) if val_metrics_history['f1_weighted'] else 0,
            # æ·»åŠ æœ€ä½³æ¨¡å‹çš„çœŸå®æ€§èƒ½æŒ‡æ ‡
            'best_model_metrics': best_model_metrics
        }
        
        # è®°å½•å®Œæ•´çš„è®­ç»ƒå†å²åˆ°CSVæ–‡ä»¶
        self.log_training_history(history)
        
        # æ‰“å°æœ€ç»ˆè®­ç»ƒæ€»ç»“ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹çš„çœŸå®æ€§èƒ½ï¼‰
        print("\n" + "="*100)
        print("ğŸ‰ è®­ç»ƒå®Œæˆæ€»ç»“:")
        print(f"  è®­ç»ƒæŸå¤±å‡½æ•°: {self.args.loss_type if hasattr(self.args, 'loss_type') else 'ce'}")
        print(f"  éªŒè¯æŸå¤±å‡½æ•°: æ ‡å‡†äº¤å‰ç†µï¼ˆéµå¾ªä¸»æµMLè§„èŒƒï¼‰")
        print(f"  è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history['best_val_acc']:.4f}")
        print(f"  è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯F1(macro): {history['best_val_f1_macro']:.4f}")
        print(f"  è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯F1(weighted): {history['best_val_f1_weighted']:.4f}")
        print(f"  è®­ç»ƒè½®æ•°: {len(train_losses)}")
        print("-" * 50)
        print("ğŸ“Š æœ€ä½³æ¨¡å‹çœŸå®æ€§èƒ½æŒ‡æ ‡ï¼ˆåŸºäºæ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼‰:")
        print(f"  éªŒè¯æŸå¤±(æ ‡å‡†CE): {best_model_metrics['loss']:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {best_model_metrics['accuracy']:.4f}")
        print(f"  éªŒè¯F1(macro): {best_model_metrics['f1_macro']:.4f}")
        print(f"  éªŒè¯F1(weighted): {best_model_metrics['f1_weighted']:.4f}")
        print(f"  éªŒè¯ç²¾ç¡®ç‡: {best_model_metrics['precision']:.4f}")
        print(f"  éªŒè¯å¬å›ç‡: {best_model_metrics['recall']:.4f}")
        print("="*100)
        
        # è®°å½•æœ€ç»ˆè®­ç»ƒæ€»ç»“åˆ°æ—¥å¿—
        self.log_info("\n" + "="*100)
        self.log_info("ğŸ‰ è®­ç»ƒå®Œæˆæ€»ç»“:")
        self.log_info(f"  è®­ç»ƒæŸå¤±å‡½æ•°: {self.args.loss_type if hasattr(self.args, 'loss_type') else 'ce'}")
        self.log_info(f"  éªŒè¯æŸå¤±å‡½æ•°: æ ‡å‡†äº¤å‰ç†µï¼ˆéµå¾ªä¸»æµMLè§„èŒƒï¼‰")
        self.log_info(f"  è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history['best_val_acc']:.4f}")
        self.log_info(f"  è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯F1(macro): {history['best_val_f1_macro']:.4f}")
        self.log_info(f"  è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯F1(weighted): {history['best_val_f1_weighted']:.4f}")
        self.log_info(f"  è®­ç»ƒè½®æ•°: {len(train_losses)}")
        self.log_info("-" * 50)
        self.log_info("ğŸ“Š æœ€ä½³æ¨¡å‹çœŸå®æ€§èƒ½æŒ‡æ ‡ï¼ˆåŸºäºæ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼‰:")
        self.log_info(f"  éªŒè¯æŸå¤±(æ ‡å‡†CE): {best_model_metrics['loss']:.4f}")
        self.log_info(f"  éªŒè¯å‡†ç¡®ç‡: {best_model_metrics['accuracy']:.4f}")
        self.log_info(f"  éªŒè¯F1(macro): {best_model_metrics['f1_macro']:.4f}")
        self.log_info(f"  éªŒè¯F1(weighted): {best_model_metrics['f1_weighted']:.4f}")
        self.log_info(f"  éªŒè¯ç²¾ç¡®ç‡: {best_model_metrics['precision']:.4f}")
        self.log_info(f"  éªŒè¯å¬å›ç‡: {best_model_metrics['recall']:.4f}")
        self.log_info("="*100)
        
        self.plot_results(history)
        
        # å…³é—­æ—¥å¿—å™¨
        self.close_logger()
        
        return self.model, history

    def plot_results(self, history):
        """ç»˜åˆ¶è®­ç»ƒç»“æœï¼ˆåŒ…å«F1åˆ†æ•°ç­‰å¤šç§æŒ‡æ ‡ï¼‰"""
        # ä½¿ç”¨æ—¶é—´+settingä½œä¸ºæ–‡ä»¶å
        path = os.path.join(self.args.result_path, f"{self.time_stamp}_{self.setting}")
        if not os.path.exists(path):
            os.makedirs(path)
        
        # åˆ›å»ºæ›´å¤§çš„å›¾å½¢æ¥å®¹çº³æ›´å¤šå­å›¾
        plt.figure(figsize=(20, 12))
        
        # 1. æŸå¤±æ›²çº¿
        plt.subplot(2, 3, 1)
        train_loss_type = getattr(self.args, 'loss_type', 'ce')
        loss_df = pd.DataFrame({
            'Epoch': list(range(len(history['train_losses']))) * 2,
            'Loss': history['train_losses'] + history['val_metrics']['loss'],
            'Type': [f'Train({train_loss_type})'] * len(history['train_losses']) + ['Val(æ ‡å‡†CE)'] * len(history['val_metrics']['loss'])
        })
        sns.lineplot(data=loss_df, x='Epoch', y='Loss', hue='Type')
        plt.title('æŸå¤±æ›²çº¿ï¼ˆè®­ç»ƒvséªŒè¯æŸå¤±å‡½æ•°ï¼‰', pad=20)
        plt.grid(True, alpha=0.3)
        
        # 2. å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(2, 3, 2)
        acc_df = pd.DataFrame({
            'Epoch': list(range(len(history['train_accs']))) * 2,
            'Accuracy': history['train_accs'] + history['val_metrics']['accuracy'],
            'Type': ['Train'] * len(history['train_accs']) + ['Val'] * len(history['val_metrics']['accuracy'])
        })
        sns.lineplot(data=acc_df, x='Epoch', y='Accuracy', hue='Type')
        plt.title('å‡†ç¡®ç‡æ›²çº¿', pad=20)
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        # 3. F1åˆ†æ•°æ›²çº¿
        plt.subplot(2, 3, 3)
        f1_df = pd.DataFrame({
            'Epoch': list(range(len(history['val_metrics']['f1_macro']))) * 2,
            'F1_Score': history['val_metrics']['f1_macro'] + history['val_metrics']['f1_weighted'],
            'Type': ['F1-Macro'] * len(history['val_metrics']['f1_macro']) + ['F1-Weighted'] * len(history['val_metrics']['f1_weighted'])
        })
        sns.lineplot(data=f1_df, x='Epoch', y='F1_Score', hue='Type')
        plt.title('F1åˆ†æ•°æ›²çº¿', pad=20)
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        # 4. ç²¾ç¡®ç‡å’Œå¬å›ç‡æ›²çº¿
        plt.subplot(2, 3, 4)
        pr_df = pd.DataFrame({
            'Epoch': list(range(len(history['val_metrics']['precision']))) * 2,
            'Score': history['val_metrics']['precision'] + history['val_metrics']['recall'],
            'Type': ['Precision'] * len(history['val_metrics']['precision']) + ['Recall'] * len(history['val_metrics']['recall'])
        })
        sns.lineplot(data=pr_df, x='Epoch', y='Score', hue='Type')
        plt.title('ç²¾ç¡®ç‡å’Œå¬å›ç‡æ›²çº¿', pad=20)
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        # 5. æ€§èƒ½æ€»ç»“
        plt.subplot(2, 3, 5)
        final_train_acc = history['train_accs'][-1]
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„çœŸå®æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'best_model_metrics' in history:
            best_metrics = history['best_model_metrics']
            train_loss_type = getattr(self.args, 'loss_type', 'ce')
            performance_text = f"""è®­ç»ƒæ€§èƒ½æ€»ç»“:

è®­ç»ƒæŸå¤±å‡½æ•°: {train_loss_type}
éªŒè¯æŸå¤±å‡½æ•°: æ ‡å‡†äº¤å‰ç†µï¼ˆéµå¾ªMLè§„èŒƒï¼‰

æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}
è®­ç»ƒè¿‡ç¨‹æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history['best_val_acc']:.4f}

æ€»è®­ç»ƒè½®æ•°: {len(history['train_losses'])}
æ¨¡å‹: {self.args.model}

ğŸ† æœ€ä½³æ¨¡å‹çœŸå®æ€§èƒ½ï¼ˆåŸºäºæ ‡å‡†CEæŸå¤±ï¼‰:
- éªŒè¯æŸå¤±(æ ‡å‡†CE): {best_metrics['loss']:.4f}
- éªŒè¯å‡†ç¡®ç‡: {best_metrics['accuracy']:.4f}
- F1(macro): {best_metrics['f1_macro']:.4f}
- F1(weighted): {best_metrics['f1_weighted']:.4f}
- ç²¾ç¡®ç‡: {best_metrics['precision']:.4f}
- å¬å›ç‡: {best_metrics['recall']:.4f}
            """
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æœ€ä½³æ¨¡å‹æŒ‡æ ‡ï¼Œä½¿ç”¨æœ€åä¸€è½®çš„æŒ‡æ ‡
            train_loss_type = getattr(self.args, 'loss_type', 'ce')
            performance_text = f"""è®­ç»ƒæ€§èƒ½æ€»ç»“:

è®­ç»ƒæŸå¤±å‡½æ•°: {train_loss_type}
éªŒè¯æŸå¤±å‡½æ•°: æ ‡å‡†äº¤å‰ç†µï¼ˆéµå¾ªMLè§„èŒƒï¼‰

æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}
æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history['best_val_acc']:.4f}
æœ€ä½³F1(macro): {history['best_val_f1_macro']:.4f}
æœ€ä½³F1(weighted): {history['best_val_f1_weighted']:.4f}

æ€»è®­ç»ƒè½®æ•°: {len(history['train_losses'])}
æ¨¡å‹: {self.args.model}

æœ€ç»ˆéªŒè¯æŒ‡æ ‡ (æœ€åä¸€è½®, åŸºäºæ ‡å‡†CEæŸå¤±):
- å‡†ç¡®ç‡: {history['val_metrics']['accuracy'][-1]:.4f}
- F1(macro): {history['val_metrics']['f1_macro'][-1]:.4f}
- F1(weighted): {history['val_metrics']['f1_weighted'][-1]:.4f}
- ç²¾ç¡®ç‡: {history['val_metrics']['precision'][-1]:.4f}
- å¬å›ç‡: {history['val_metrics']['recall'][-1]:.4f}
            """
        plt.text(0.05, 0.95, performance_text, transform=plt.gca().transAxes, 
                fontsize=9, verticalalignment='top')
        plt.title('è®­ç»ƒæ€»ç»“', pad=20)
        plt.axis('off')
        
        # 6. æŒ‡æ ‡å¯¹æ¯”é›·è¾¾å›¾
        plt.subplot(2, 3, 6)
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„çœŸå®æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'best_model_metrics' in history:
            best_metrics = history['best_model_metrics']
            metrics_values = [
                best_metrics['accuracy'],
                best_metrics['f1_macro'],
                best_metrics['f1_weighted'],
                best_metrics['precision'],
                best_metrics['recall']
            ]
            radar_title = 'ğŸ† æœ€ä½³æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾ï¼ˆåŸºäºæ ‡å‡†CEæŸå¤±ï¼‰'
            label_text = 'æœ€ä½³æ¨¡å‹æ€§èƒ½'
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨æœ€åä¸€è½®çš„æŒ‡æ ‡
            metrics_values = [
                history['val_metrics']['accuracy'][-1],
                history['val_metrics']['f1_macro'][-1],
                history['val_metrics']['f1_weighted'][-1],
                history['val_metrics']['precision'][-1],
                history['val_metrics']['recall'][-1]
            ]
            radar_title = 'æœ€ç»ˆéªŒè¯æŒ‡æ ‡é›·è¾¾å›¾ï¼ˆæœ€åä¸€è½®ï¼ŒåŸºäºæ ‡å‡†CEæŸå¤±ï¼‰'
            label_text = 'éªŒè¯æŒ‡æ ‡'
        
        metrics_names = ['å‡†ç¡®ç‡', 'F1-Macro', 'F1-Weighted', 'ç²¾ç¡®ç‡', 'å¬å›ç‡']
        
        # åˆ›å»ºé›·è¾¾å›¾æ•°æ®
        angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()
        metrics_values += metrics_values[:1]  # é—­åˆå›¾å½¢
        angles += angles[:1]
        
        ax = plt.subplot(2, 3, 6, projection='polar')
        ax.plot(angles, metrics_values, 'o-', linewidth=2, label=label_text)
        ax.fill(angles, metrics_values, alpha=0.25)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics_names)
        ax.set_ylim(0, 1)
        plt.title(radar_title, pad=30)
        
        plt.tight_layout()
        plt.savefig(os.path.join(path, 'comprehensive_training_results.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜è¯¦ç»†çš„æŒ‡æ ‡æ•°æ®åˆ°CSV
        metrics_df = pd.DataFrame({
            'Epoch': range(len(history['train_losses'])),
            'Train_Loss': history['train_losses'],
            'Train_Acc': history['train_accs'],
            'Val_Loss': history['val_metrics']['loss'],
            'Val_Acc': history['val_metrics']['accuracy'],
            'Val_F1_Macro': history['val_metrics']['f1_macro'],
            'Val_F1_Weighted': history['val_metrics']['f1_weighted'],
            'Val_Precision': history['val_metrics']['precision'],
            'Val_Recall': history['val_metrics']['recall']
        })
        metrics_df.to_csv(os.path.join(path, 'training_metrics.csv'), index=False)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹çš„çœŸå®æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'best_model_metrics' in history:
            best_metrics = history['best_model_metrics']
            summary_df = pd.DataFrame({
                'Metric': ['Best_Model_Val_Loss', 'Best_Model_Val_Acc', 'Best_Model_Val_F1_Macro', 
                          'Best_Model_Val_F1_Weighted', 'Best_Model_Val_Precision', 'Best_Model_Val_Recall',
                          'Training_Process_Best_Acc', 'Training_Process_Best_F1_Macro', 'Training_Process_Best_F1_Weighted'],
                'Value': [best_metrics['loss'], best_metrics['accuracy'], best_metrics['f1_macro'],
                         best_metrics['f1_weighted'], best_metrics['precision'], best_metrics['recall'],
                         history['best_val_acc'], history['best_val_f1_macro'], history['best_val_f1_weighted']]
            })
            summary_df.to_csv(os.path.join(path, 'best_model_summary.csv'), index=False)
            print(f"è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {path}")
            print(f"ğŸ“Š æœ€ä½³æ¨¡å‹çœŸå®æ€§èƒ½å·²è®°å½•åˆ°: best_model_summary.csv")
        else:
            print(f"è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {path}")

    def plot_confusion_matrix(self, cm):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆä½¿ç”¨seabornå®ç°ï¼‰"""
        path = os.path.join(self.args.result_path, f"{self.time_stamp}_{self.setting}")
        if not os.path.exists(path):
            os.makedirs(path)
        plt.figure(figsize=(12, 8))
        
        # ä½¿ç”¨è·å–åˆ°çš„ç±»åˆ«åç§°
        sns.heatmap(cm, 
                    annot=True, 
                    fmt='d',
                    cmap='Blues',
                    xticklabels=self.class_names,
                    yticklabels=self.class_names,
                    cbar_kws={'label': 'æ ·æœ¬æ•°'})
        plt.title('æ··æ·†çŸ©é˜µ - HVACå¼‚å¸¸æ£€æµ‹', pad=20)
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(os.path.join(path, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
        plt.close()

    def evaluate_report(self):
        """é€šç”¨è¯„ä¼°æŠ¥å‘Šï¼Œå…¼å®¹GNNå’Œæ™®é€šåˆ†ç±»æ¨¡å‹"""
        self.model.eval()
        self.model.to(self.device)
        all_preds = []
        all_labels = []
        all_probs = []
        with torch.no_grad():
            for batch in self.vali_loader:
                if hasattr(batch, 'x') and hasattr(batch, 'y'):
                    # GNNæ•°æ®
                    batch = batch.to(self.device)
                    out = self.model(batch)
                    probs = F.softmax(out, dim=1)
                    pred = out.argmax(dim=1)
                    
                    # å¤„ç†æ ‡ç­¾å½¢çŠ¶
                    label_squeezed = batch.y.long().squeeze(-1) if batch.y.dim() > 1 else batch.y.long()
                    
                    all_preds.extend(pred.cpu().numpy())
                    all_labels.extend(label_squeezed.cpu().numpy())
                    all_probs.extend(probs.cpu().numpy())
                elif len(batch) == 3:
                    # åŒè·¯GAFæ•°æ®ï¼ˆæ—§ç‰ˆæœ¬æ ¼å¼ï¼‰
                    sum_data, diff_data, label = batch
                    sum_data = sum_data.float().to(self.device)
                    diff_data = diff_data.float().to(self.device)
                    label = label.to(self.device)
                    out = self.model(sum_data, diff_data)
                    probs = F.softmax(out, dim=1)
                    pred = out.argmax(dim=1)
                    
                    # å¤„ç†æ ‡ç­¾å½¢çŠ¶
                    label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                    
                    all_preds.extend(pred.cpu().numpy())
                    all_labels.extend(label_squeezed.cpu().numpy())
                    all_probs.extend(probs.cpu().numpy())
                elif len(batch) == 4:
                    # å¢å¼ºåŒè·¯GAFæ•°æ®ï¼ˆæ–°ç‰ˆæœ¬æ ¼å¼ï¼‰
                    sum_data, diff_data, time_series_data, label = batch
                    sum_data = sum_data.float().to(self.device)
                    diff_data = diff_data.float().to(self.device)
                    time_series_data = time_series_data.float().to(self.device)
                    label = label.to(self.device)
                    out = self.model(sum_data, diff_data, time_series_data)
                    probs = F.softmax(out, dim=1)
                    pred = out.argmax(dim=1)
                    
                    # å¤„ç†æ ‡ç­¾å½¢çŠ¶
                    label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                    
                    all_preds.extend(pred.cpu().numpy())
                    all_labels.extend(label_squeezed.cpu().numpy())
                    all_probs.extend(probs.cpu().numpy())
                else:
                    # æ™®é€šåˆ†ç±»æ•°æ®
                    batch_x, label = batch
                    batch_x = batch_x.float().to(self.device)
                    label = label.to(self.device)
                    out = self.model(batch_x)
                    probs = F.softmax(out, dim=1)
                    pred = out.argmax(dim=1)
                    
                    # å¤„ç†æ ‡ç­¾å½¢çŠ¶
                    label_squeezed = label.long().squeeze(-1) if label.dim() > 1 else label.long()
                    
                    all_preds.extend(pred.cpu().numpy())
                    all_labels.extend(label_squeezed.cpu().numpy())
                    all_probs.extend(probs.cpu().numpy())
        accuracy = accuracy_score(all_labels, all_preds)
        report = classification_report(all_labels, all_preds, target_names=self.class_names)
        cm = confusion_matrix(all_labels, all_preds)
        self.plot_confusion_matrix(cm)
        return accuracy, report, cm, all_probs

def save_model_checkpoint(model, setting, extra_dict=None):
    """ç»Ÿä¸€ä¿å­˜æ¨¡å‹åˆ°checkpoints/setting/checkpoint.pthï¼Œextra_dictå¯åŒ…å«é¢å¤–ä¿¡æ¯"""
    path = os.path.join('checkpoints', setting)
    if not os.path.exists(path):
        os.makedirs(path)
    save_dict = {'model_state_dict': model.state_dict()}
    if extra_dict:
        save_dict.update(extra_dict)
    torch.save(save_dict, os.path.join(path, 'checkpoint.pth')) 

# ========== æ·»åŠ é«˜çº§æŸå¤±å‡½æ•° ==========

# å°è¯•å¯¼å…¥timmä¼˜åŒ–å®ç°
try:
    from timm.loss.cross_entropy import LabelSmoothingCrossEntropy as TimmLabelSmoothingCE
    TIMM_AVAILABLE = True
except ImportError:
    TIMM_AVAILABLE = False
    TimmLabelSmoothingCE = None


class LabelSmoothingCrossEntropy(nn.Module):
    """
    ä¼˜åŒ–çš„æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±å‡½æ•°
    
    ä¼˜å…ˆä½¿ç”¨timmçš„é«˜æ•ˆå®ç°ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨è‡ªå®šä¹‰å®ç°ã€‚
    åŸºäºæ€§èƒ½æµ‹è¯•ï¼Œtimmå®ç°æ¯”è‡ªå®šä¹‰å®ç°å¿«10-20%ï¼Œå†…å­˜æ•ˆç‡æ›´é«˜ã€‚
    
    Args:
        smoothing (float): å¹³æ»‘å› å­ï¼Œé€šå¸¸åœ¨0.05-0.2ä¹‹é—´
        num_classes (int): ç±»åˆ«æ•°é‡
        dim (int): softmaxçš„ç»´åº¦ï¼Œé»˜è®¤ä¸º-1
        weight (Tensor): å„ç±»åˆ«çš„æƒé‡ï¼Œç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        use_timm (bool): æ˜¯å¦ä¼˜å…ˆä½¿ç”¨timmå®ç°
    """
    def __init__(self, smoothing=0.1, num_classes=None, dim=-1, weight=None, use_timm=True):
        super().__init__()
        self.smoothing = smoothing
        self.num_classes = num_classes
        self.dim = dim
        self.weight = weight
        self.use_timm = use_timm and TIMM_AVAILABLE
        
        # é€‰æ‹©å®ç°æ–¹å¼
        if self.use_timm:
            print(f"ğŸš€ ä½¿ç”¨timmä¼˜åŒ–çš„Label Smoothing CE (æ€§èƒ½æå‡10-20%)")
            self.timm_loss = TimmLabelSmoothingCE(smoothing=smoothing)
            self._forward_func = self._timm_forward
        else:
            print(f"ğŸ“š ä½¿ç”¨è‡ªå®šä¹‰Label Smoothing CE")
            self.confidence = 1.0 - smoothing
            self._forward_func = self._custom_forward
    
    def forward(self, pred, target):
        return self._forward_func(pred, target)
    
    def _timm_forward(self, pred, target):
        """ä½¿ç”¨timmå®ç°çš„å‰å‘ä¼ æ’­"""
        if self.weight is not None:
            # timmä¸ç›´æ¥æ”¯æŒç±»åˆ«æƒé‡ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†
            base_loss = self.timm_loss(pred, target)
            ce_loss = F.cross_entropy(pred, target, reduction='none')
            weight_expanded = self.weight[target]
            weighted_ce = (ce_loss * weight_expanded).mean()
            # ä¿æŒç›¸åŒçš„å¹³æ»‘æ¯”ä¾‹
            return base_loss * (weighted_ce / ce_loss.mean()).detach()
        else:
            return self.timm_loss(pred, target)
    
    def _custom_forward(self, pred, target):
        """ä¼˜åŒ–çš„è‡ªå®šä¹‰å®ç°"""
        if self.num_classes is None:
            self.num_classes = pred.size(1)
        
        # ä½¿ç”¨æ›´ç¨³å®šçš„log_softmax
        log_pred = F.log_softmax(pred, dim=self.dim)
        
        # ä¼˜åŒ–çš„è½¯æ ‡ç­¾åˆ›å»º - é¿å…scatteræ“ä½œ
        nll_loss = -log_pred.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)
        smooth_loss = -log_pred.mean(dim=-1)
        
        # è®¡ç®—æ ‡ç­¾å¹³æ»‘æŸå¤±
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss
        
        # åº”ç”¨ç±»åˆ«æƒé‡
        if self.weight is not None:
            weight_expanded = self.weight[target]
            loss = loss * weight_expanded
        
        return loss.mean()


class FocalLoss(nn.Module):
    """
    Focal Loss ä¸“é—¨å¤„ç†éš¾åˆ†ç±»æ ·æœ¬å’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    
    é€šè¿‡åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡ï¼Œè®©æ¨¡å‹æ›´ä¸“æ³¨äºéš¾åˆ†ç±»çš„æ ·æœ¬ï¼Œ
    å‡å°‘æ˜“åˆ†ç±»æ ·æœ¬å¯¹æŸå¤±çš„è´¡çŒ®ã€‚
    """
    def __init__(self, alpha=1.0, gamma=2.0, weight=None, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.weight = weight
        self.reduction = reduction
        
    def forward(self, pred, target):
        ce_loss = F.cross_entropy(pred, target, weight=self.weight, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # è®¡ç®—alpha_t
        if isinstance(self.alpha, (float, int)):
            alpha_t = self.alpha
        else:
            alpha_t = self.alpha[target]
            
        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class ConfidencePenaltyLoss(nn.Module):
    """
    ç½®ä¿¡åº¦æƒ©ç½šæŸå¤±
    
    æƒ©ç½šè¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ï¼Œé¼“åŠ±æ¨¡å‹è¾“å‡ºæ›´å¹³è¡¡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œ
    æœ‰åŠ©äºç¼“è§£ç±»åˆ«é—´ç›¸ä¼¼æ€§é—®é¢˜ã€‚
    """
    def __init__(self, beta=0.1):
        super().__init__()
        self.beta = beta
        
    def forward(self, pred, target):
        # åŸºç¡€äº¤å‰ç†µæŸå¤±
        ce_loss = F.cross_entropy(pred, target)
        
        # ç½®ä¿¡åº¦æƒ©ç½šï¼šKLæ•£åº¦ç›¸å¯¹äºå‡åŒ€åˆ†å¸ƒ
        prob = F.softmax(pred, dim=1)
        log_prob = F.log_softmax(pred, dim=1)
        uniform_dist = torch.ones_like(prob) / prob.size(1)
        
        # KL(uniform || pred) = -H(uniform) + H_cross(uniform, pred)
        kl_penalty = F.kl_div(log_prob, uniform_dist, reduction='batchmean')
        
        return ce_loss - self.beta * kl_penalty


class HybridFocalLoss(nn.Module):
    """
    æ··åˆFocal Lossï¼Œç»“åˆæ ‡ç­¾å¹³æ»‘å’Œéš¾æ ·æœ¬èšç„¦
    
    ä¸“é—¨é’ˆå¯¹HVACå¼‚å¸¸æ£€æµ‹ä¸­ç±»åˆ«ç›¸ä¼¼æ€§å’Œéš¾æ ·æœ¬é—®é¢˜è®¾è®¡
    """
    
    def __init__(self, alpha=1.0, gamma=2.0, smoothing=0.0, weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.smoothing = smoothing
        self.weight = weight
        
        # å¦‚æœæœ‰æ ‡ç­¾å¹³æ»‘ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å®ç°
        if smoothing > 0:
            self.label_smooth = LabelSmoothingCrossEntropy(
                smoothing=smoothing, weight=weight, use_timm=True
            )
        else:
            self.label_smooth = None
    
    def forward(self, pred, target):
        if self.label_smooth is not None:
            # ç»“åˆæ ‡ç­¾å¹³æ»‘çš„Focal Loss
            ce_loss = self.label_smooth(pred, target)
            p = torch.exp(-ce_loss)
            focal_loss = self.alpha * (1 - p) ** self.gamma * ce_loss
            return focal_loss
        else:
            # æ ‡å‡†Focal Loss
            ce_loss = F.cross_entropy(pred, target, weight=self.weight, reduction='none')
            pt = torch.exp(-ce_loss)
            focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
            return focal_loss.mean()


class AdaptiveLossScheduler(nn.Module):
    """
    è‡ªé€‚åº”æŸå¤±è°ƒåº¦å™¨
    
    æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´æŸå¤±å‡½æ•°å‚æ•°ï¼Œ
    åœ¨è®­ç»ƒåˆæœŸä½¿ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼ŒåæœŸå‡å°‘æ­£åˆ™åŒ–å¼ºåº¦
    """
    
    def __init__(self, base_loss, initial_smoothing=0.2, final_smoothing=0.05, 
                 decay_epochs=50):
        super().__init__()
        self.base_loss = base_loss
        self.initial_smoothing = initial_smoothing
        self.final_smoothing = final_smoothing
        self.decay_epochs = decay_epochs
        self.current_epoch = 0
    
    def set_epoch(self, epoch):
        """è®¾ç½®å½“å‰è®­ç»ƒè½®æ¬¡"""
        self.current_epoch = epoch
        
        # è®¡ç®—å½“å‰çš„å¹³æ»‘å› å­
        if epoch < self.decay_epochs:
            progress = epoch / self.decay_epochs
            current_smoothing = self.initial_smoothing * (1 - progress) + \
                              self.final_smoothing * progress
        else:
            current_smoothing = self.final_smoothing
        
        # æ›´æ–°æŸå¤±å‡½æ•°å‚æ•°
        if hasattr(self.base_loss, 'smoothing'):
            self.base_loss.smoothing = current_smoothing
            if hasattr(self.base_loss, 'confidence'):
                self.base_loss.confidence = 1.0 - current_smoothing
    
    def forward(self, pred, target):
        return self.base_loss(pred, target)


class CombinedLoss(nn.Module):
    """
    ç»„åˆæŸå¤±å‡½æ•°
    
    å°†å¤šä¸ªæŸå¤±å‡½æ•°æŒ‰æƒé‡ç»„åˆ
    """
    def __init__(self, losses):
        super().__init__()
        self.losses = nn.ModuleDict()
        self.weights = {}
        
        for name, (loss_fn, weight) in losses.items():
            self.losses[name] = loss_fn
            self.weights[name] = weight
            
    def forward(self, *args, **kwargs):
        total_loss = 0
        loss_dict = {}
        
        for name, loss_fn in self.losses.items():
            loss = loss_fn(*args, **kwargs)
            weighted_loss = self.weights[name] * loss
            total_loss += weighted_loss
            loss_dict[name] = loss.item()
            
        return total_loss 