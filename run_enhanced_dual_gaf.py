#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆåŒè·¯GAFç½‘ç»œè¿è¡Œè„šæœ¬
é›†æˆç»Ÿè®¡ç‰¹å¾æå–å™¨å’Œå¤šæ¨¡æ€èåˆåŠŸèƒ½
"""

import argparse
import torch
import random
import numpy as np
import os
from exp.exp import Exp

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆåŒè·¯GAFç½‘ç»œè®­ç»ƒ')
    
    # åŸºç¡€é…ç½®
    parser.add_argument('--model', type=str, required=True, default='DualGAFNet', help='æ¨¡å‹åç§°')
    parser.add_argument('--data', type=str, required=True, default='DualGAF', help='æ•°æ®é›†åç§°')
    parser.add_argument('--root_path', type=str, default='./dataset/SAHU/', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--result_path', type=str, default='./result/', help='ç»“æœä¿å­˜è·¯å¾„')
    
    # æ•°æ®é…ç½®
    parser.add_argument('--seq_len', type=int, default=96, help='æ—¶é—´åºåˆ—é•¿åº¦')
    parser.add_argument('--step', type=int, default=96, help='æ»‘åŠ¨çª—å£æ­¥é•¿')
    parser.add_argument('--enc_in', type=int, default=26, help='è¾“å…¥ç‰¹å¾ç»´åº¦')
    parser.add_argument('--num_class', type=int, default=5, help='åˆ†ç±»ç±»åˆ«æ•°')
    parser.add_argument('--test_size', type=float, default=0.3, help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--data_type_method', type=str, default='uint8', choices=['float32', 'uint8', 'uint16'], help='æ•°æ®ç±»å‹è½¬æ¢æ–¹æ³•')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--feature_dim', type=int, default=64, help='ç‰¹å¾ç»´åº¦')
    parser.add_argument('--extractor_type', type=str, default='large_kernel', 
                       choices=['large_kernel', 'inception', 'dilated', 'multiscale'], help='ç‰¹å¾æå–å™¨ç±»å‹')
    parser.add_argument('--fusion_type', type=str, default='adaptive',
                       choices=['adaptive', 'concat', 'add', 'mul', 'weighted_add', 'bidirectional', 'gated'], help='GAFèåˆç±»å‹')
    parser.add_argument('--attention_type', type=str, default='channel',
                       choices=['channel', 'spatial', 'cbam', 'self', 'none'], help='æ³¨æ„åŠ›ç±»å‹')
    parser.add_argument('--classifier_type', type=str, default='mlp',
                       choices=['mlp', 'simple'], help='åˆ†ç±»å™¨ç±»å‹')
    
    # ç»Ÿè®¡ç‰¹å¾é…ç½®ï¼ˆæ–°å¢ï¼‰
    parser.add_argument('--use_statistical_features', action='store_true', default=True, help='æ˜¯å¦ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾')
    parser.add_argument('--stat_type', type=str, default='comprehensive',
                       choices=['basic', 'comprehensive', 'correlation_focused'], help='ç»Ÿè®¡ç‰¹å¾ç±»å‹')
    parser.add_argument('--multimodal_fusion_strategy', type=str, default='concat',
                       choices=['concat', 'attention', 'gated', 'adaptive'], help='å¤šæ¨¡æ€èåˆç­–ç•¥')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--train_epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--learning_rate', type=float, default=0.0001, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    
    # è®¾å¤‡é…ç½®
    parser.add_argument('--use_gpu', action='store_true', default=True, help='ä½¿ç”¨GPU')
    parser.add_argument('--gpu', type=int, default=0, help='GPUè®¾å¤‡å·')
    parser.add_argument('--gpu_type', type=str, default='cuda', choices=['cuda', 'mps'], help='GPUç±»å‹')
    parser.add_argument('--use_multi_gpu', action='store_true', help='ä½¿ç”¨å¤šGPU')
    parser.add_argument('--devices', type=str, default='0,1,2,3', help='å¤šGPUè®¾å¤‡å·')
    
    # å…¶ä»–é…ç½®
    parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--seed', type=int, default=2021, help='éšæœºç§å­')
    parser.add_argument('--des', type=str, default='enhanced_dual_gaf', help='å®éªŒæè¿°')
    
    # å¹¶è¡Œå¤„ç†ä¼˜åŒ–é…ç½® (Parallel Processing Optimization)
    parser.add_argument('--n_jobs', type=int, default=-1, 
                        help='å¹¶è¡Œå¤„ç†è¿›ç¨‹æ•°ï¼Œ-1è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ (Number of parallel jobs, -1 for auto-detect)')
    parser.add_argument('--use_multiprocessing', action='store_true', default=True,
                        help='å¯ç”¨å¤šè¿›ç¨‹å¤„ç†GAFç”Ÿæˆå’Œæ•°æ®è½¬æ¢ (Enable multiprocessing for GAF generation and data conversion)')
    parser.add_argument('--chunk_size', type=int, default=100,
                        help='å¹¶è¡Œå¤„ç†çš„æ•°æ®å—å¤§å° (Chunk size for parallel processing)')
    parser.add_argument('--disable_parallel', action='store_true', default=False,
                        help='ç¦ç”¨æ‰€æœ‰å¹¶è¡Œå¤„ç†ä¼˜åŒ–ï¼Œç”¨äºè°ƒè¯• (Disable all parallel processing optimizations for debugging)')
    parser.add_argument('--use_shared_memory', action='store_true', default=True,
                        help='å¯ç”¨å…±äº«å†…å­˜ä¼˜åŒ–ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡å¼€é”€ (Enable shared memory optimization for faster inter-process communication)')
    parser.add_argument('--disable_shared_memory', action='store_true', default=False,
                        help='ç¦ç”¨å…±äº«å†…å­˜ä¼˜åŒ–ï¼Œå›é€€åˆ°æ ‡å‡†å¤šè¿›ç¨‹ (Disable shared memory optimization, fallback to standard multiprocessing)')
    
    # HVACä¿¡å·åˆ†ç»„é…ç½®ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--use_hvac_groups', action='store_true', help='ä½¿ç”¨HVACä¿¡å·åˆ†ç»„')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    fix_seed = args.seed
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)
    
    # å¤„ç†å¹¶è¡Œå¤„ç†å‚æ•°
    import multiprocessing as mp
    import sys
    if args.disable_parallel:
        # ç¦ç”¨æ‰€æœ‰å¹¶è¡Œå¤„ç†
        args.use_multiprocessing = False
        args.use_shared_memory = False
        args.n_jobs = 1
        print("âš ï¸  å¹¶è¡Œå¤„ç†å·²ç¦ç”¨ (disable_parallel=True)")
    else:
        # è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        if args.n_jobs == -1:
            args.n_jobs = min(mp.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
        elif args.n_jobs <= 0:
            args.n_jobs = 1
        
        # å¤„ç†å…±äº«å†…å­˜é…ç½®
        if args.disable_shared_memory:
            args.use_shared_memory = False
            print("ğŸ”§ å…±äº«å†…å­˜ä¼˜åŒ–å·²ç¦ç”¨")
        elif sys.version_info < (3, 8):
            args.use_shared_memory = False
            print("âš ï¸  Pythonç‰ˆæœ¬è¿‡ä½ï¼Œç¦ç”¨å…±äº«å†…å­˜ä¼˜åŒ– (éœ€è¦Python 3.8+)")
        
        # æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´å‚æ•°
        available_memory_gb = None
        try:
            import psutil
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
        except ImportError:
            print("âš ï¸  psutilæœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨æ£€æµ‹å†…å­˜å¤§å°")
        
        # æ ¹æ®å†…å­˜å¤§å°è°ƒæ•´chunk_size - é’ˆå¯¹é«˜ç«¯æœåŠ¡å™¨ä¼˜åŒ–
        if available_memory_gb is not None:
            cpu_cores = mp.cpu_count()
            
            if available_memory_gb < 8:  # å°å†…å­˜ç³»ç»Ÿ
                args.chunk_size = min(args.chunk_size, 50)
                args.n_jobs = min(args.n_jobs, 2)
                print(f"ğŸ”§ æ£€æµ‹åˆ°å°å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œè°ƒæ•´å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
            elif available_memory_gb < 32:  # ä¸­ç­‰å†…å­˜ç³»ç»Ÿ
                args.chunk_size = max(args.chunk_size, 200)
                args.n_jobs = min(args.n_jobs, 8)
                print(f"ğŸ”§ æ£€æµ‹åˆ°ä¸­ç­‰å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œè°ƒæ•´å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
            elif available_memory_gb < 64:  # å¤§å†…å­˜ç³»ç»Ÿ
                args.chunk_size = max(args.chunk_size, 400)
                args.n_jobs = min(args.n_jobs, 16)
                print(f"ğŸš€ æ£€æµ‹åˆ°å¤§å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œä¼˜åŒ–å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
            else:  # è¶…å¤§å†…å­˜æœåŠ¡å™¨ (æ‚¨çš„é…ç½®)
                # é’ˆå¯¹32æ ¸128GBçš„é«˜ç«¯é…ç½®
                if cpu_cores >= 32:
                    # ä½¿ç”¨é€‚ä¸­çš„è¿›ç¨‹æ•°ï¼Œé¿å…è¿‡åº¦å¹¶è¡Œ
                    args.n_jobs = min(args.n_jobs, 20)  # ç•™å‡ºä¸€äº›æ ¸å¿ƒç»™ç³»ç»Ÿ
                    args.chunk_size = max(args.chunk_size, 800)  # å¤§å—å¤§å°å‡å°‘å¼€é”€
                    print(f"ğŸš€ æ£€æµ‹åˆ°é«˜ç«¯æœåŠ¡å™¨ ({available_memory_gb:.1f}GB, {cpu_cores}æ ¸)ï¼Œé«˜æ€§èƒ½é…ç½®ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
                else:
                    args.chunk_size = max(args.chunk_size, 600)
                    print(f"ğŸš€ æ£€æµ‹åˆ°è¶…å¤§å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œä¼˜åŒ–å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
        
        shared_memory_status = "å¯ç”¨" if args.use_shared_memory else "ç¦ç”¨"
        print(f"âš¡ å¹¶è¡Œå¤„ç†é…ç½® - è¿›ç¨‹æ•°: {args.n_jobs}, å¤šè¿›ç¨‹: {args.use_multiprocessing}, å—å¤§å°: {args.chunk_size}, å…±äº«å†…å­˜: {shared_memory_status}")
    
    # è®¾ç½®HVACä¿¡å·åˆ†ç»„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.use_hvac_groups:
        args.hvac_groups = [
            ['SA_TEMP','OA_TEMP','MA_TEMP','RA_TEMP','ZONE_TEMP_1','ZONE_TEMP_2','ZONE_TEMP_3','ZONE_TEMP_4','ZONE_TEMP_5'],  # æ¸©åº¦ç»„
            ['OA_CFM','RA_CFM','SA_CFM'],                 # æµé‡ç»„
            ['SA_SP', 'SA_SPSPT'],                          # è®¾å®šç‚¹ç»„
            ['SF_WAT', 'RF_WAT'],                  # é˜€é—¨ç»„
            ['SF_SPD','RF_SPD','SF_CS','RF_CS'],                  # é£æœºç»„
            ['CHWC_VLV_DM','CHWC_VLV'],                  # å†·æ°´é˜€ç»„
            ['OA_DMPR_DM','RA_DMPR_DM','OA_DMPR','RA_DMPR'],     # é£é—¨ç»„
        ]
    else:
        args.hvac_groups = None
    
    # æ„å»ºå®éªŒè®¾ç½®å­—ç¬¦ä¸²
    setting = f'{args.data}_{args.model}_{args.des}_sl{args.seq_len}_step{args.step}_' \
              f'gaf{args.fusion_type}_fd{args.feature_dim}_dtype{args.data_type_method}_' \
              f'{args.extractor_type}-{args.attention_type}-{args.classifier_type}'
    
    if args.use_statistical_features:
        setting += f'-stat{args.stat_type}-fusion{args.multimodal_fusion_strategy}'
    
    if args.use_hvac_groups:
        setting += '-grouped'
    
    print('='*100)
    print(f'å®éªŒè®¾ç½®: {setting}')
    print('='*100)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("\nğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: {args.data}")
    print(f"  æ¨¡å‹: {args.model}")
    print(f"  åºåˆ—é•¿åº¦: {args.seq_len}")
    print(f"  ç‰¹å¾ç»´åº¦: {args.feature_dim}")
    print(f"  ç‰¹å¾æå–å™¨: {args.extractor_type}")
    print(f"  GAFèåˆç±»å‹: {args.fusion_type}")
    print(f"  æ³¨æ„åŠ›ç±»å‹: {args.attention_type}")
    print(f"  åˆ†ç±»å™¨ç±»å‹: {args.classifier_type}")
    print(f"  ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾: {args.use_statistical_features}")
    if args.use_statistical_features:
        print(f"  ç»Ÿè®¡ç‰¹å¾ç±»å‹: {args.stat_type}")
        print(f"  å¤šæ¨¡æ€èåˆç­–ç•¥: {args.multimodal_fusion_strategy}")
    print(f"  ä½¿ç”¨HVACåˆ†ç»„: {args.use_hvac_groups}")
    
    print(f"\nğŸš€ è®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  è®­ç»ƒè½®æ•°: {args.train_epochs}")
    print(f"  æ•°æ®ç±»å‹: {args.data_type_method}")
    
    print(f"\nâš¡ å¹¶è¡Œä¼˜åŒ–é…ç½®:")
    print(f"  å¹¶è¡Œè¿›ç¨‹æ•°: {args.n_jobs}")
    print(f"  ä½¿ç”¨å¤šè¿›ç¨‹: {args.use_multiprocessing}")
    print(f"  æ•°æ®å—å¤§å°: {args.chunk_size}")
    print(f"  å¹¶è¡Œå¤„ç†: {'ç¦ç”¨' if args.disable_parallel else 'å¯ç”¨'}")
    print(f"  å…±äº«å†…å­˜ä¼˜åŒ–: {'å¯ç”¨' if args.use_shared_memory else 'ç¦ç”¨'}")
    print('='*100)
    
    # åˆ›å»ºå®éªŒå¯¹è±¡å¹¶è¿è¡Œ
    exp = Exp(args, setting)
    
    print('>>>>>>>å¼€å§‹è®­ç»ƒ : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
    model, history = exp.train()
    
    print('>>>>>>>å¼€å§‹æµ‹è¯• : {}>>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
    exp.evaluate_report()
    
    print('>>>>>>>è®­ç»ƒå’Œæµ‹è¯•å®Œæˆ : {}>>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))

if __name__ == '__main__':
    main() 