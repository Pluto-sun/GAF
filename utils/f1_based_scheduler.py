import torch
import torch.optim as optim
from torch.optim.lr_scheduler import _LRScheduler
import warnings
import numpy as np

class F1BasedReduceLROnPlateau:
    """
    åŸºäºF1åˆ†æ•°çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    å½“F1åˆ†æ•°åœæ»ä¸å‰æ—¶ï¼Œè‡ªåŠ¨é™ä½å­¦ä¹ ç‡ã€‚
    è¿™æ¯”åŸºäºæŸå¤±çš„è°ƒåº¦å™¨æ›´é€‚åˆåˆ†ç±»ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯ï¼š
    1. ä¸å¹³è¡¡æ•°æ®é›†
    2. å¤šåˆ†ç±»ä»»åŠ¡
    3. æŸå¤±å‡½æ•°ä¸çœŸå®æ€§èƒ½æŒ‡æ ‡ä¸å®Œå…¨ä¸€è‡´çš„åœºæ™¯
    
    Args:
        optimizer (Optimizer): è¦è°ƒæ•´çš„ä¼˜åŒ–å™¨
        mode (str): 'max' å› ä¸ºF1åˆ†æ•°è¶Šé«˜è¶Šå¥½
        factor (float): å­¦ä¹ ç‡ç¼©æ”¾å› å­ï¼Œæ–°å­¦ä¹ ç‡ = æ—§å­¦ä¹ ç‡ * factor
        patience (int): ç­‰å¾…è½®æ•°ï¼ŒF1åˆ†æ•°æ— æ”¹å–„æ—¶çš„å®¹å¿è½®æ•°
        threshold (float): æ”¹å–„é˜ˆå€¼ï¼Œåªæœ‰è¶…è¿‡è¿™ä¸ªé˜ˆå€¼æ‰ç®—æ”¹å–„
        threshold_mode (str): 'rel'(ç›¸å¯¹) æˆ– 'abs'(ç»å¯¹)
        cooldown (int): å‡å°‘å­¦ä¹ ç‡åçš„å†·å´æœŸ
        min_lr (float): å­¦ä¹ ç‡ä¸‹é™
        eps (float): å­¦ä¹ ç‡æ”¹å˜çš„æœ€å°å€¼
        verbose (bool): æ˜¯å¦æ‰“å°è°ƒæ•´ä¿¡æ¯
    """
    
    def __init__(self, optimizer, mode='max', factor=0.5, patience=5,
                 threshold=0.01, threshold_mode='rel', cooldown=0,
                 min_lr=1e-6, eps=1e-8, verbose=True):
        
        if factor >= 1.0:
            raise ValueError('Factor should be < 1.0.')
        self.factor = factor
        
        if not isinstance(optimizer, torch.optim.Optimizer):
            raise TypeError(f'{type(optimizer).__name__} is not an Optimizer')
        self.optimizer = optimizer
        
        if isinstance(min_lr, (list, tuple)):
            if len(min_lr) != len(optimizer.param_groups):
                raise ValueError(f"expected {len(optimizer.param_groups)} min_lrs, got {len(min_lr)}")
            self.min_lrs = list(min_lr)
        else:
            self.min_lrs = [min_lr] * len(optimizer.param_groups)
        
        self.patience = patience
        self.verbose = verbose
        self.cooldown = cooldown
        self.cooldown_counter = 0
        self.mode = mode
        self.threshold = threshold
        self.threshold_mode = threshold_mode
        self.best = None
        self.num_bad_epochs = None
        self.mode_worse = None  # è¡¨ç¤º"æ›´å·®"çš„æ¯”è¾ƒå‡½æ•°
        self.eps = eps
        self.last_epoch = 0
        self._init_is_better(mode=mode, threshold=threshold, threshold_mode=threshold_mode)
        self._reset()

    def _reset(self):
        """é‡ç½®å†…éƒ¨çŠ¶æ€"""
        self.best = self.mode_worse
        self.cooldown_counter = 0
        self.num_bad_epochs = 0

    def step(self, metrics):
        """
        æ­¥è¿›å‡½æ•°
        
        Args:
            metrics (float): å½“å‰çš„F1åˆ†æ•°
        """
        current = float(metrics)
        self.last_epoch += 1

        if self.best is None or self.is_better(current, self.best):
            self.best = current
            self.num_bad_epochs = 0
        else:
            self.num_bad_epochs += 1

        if self.in_cooldown:
            self.cooldown_counter -= 1
            self.num_bad_epochs = 0  # å†·å´æœŸé—´å¿½ç•¥bad epochs

        if self.num_bad_epochs > self.patience:
            self._reduce_lr(self.last_epoch)
            self.cooldown_counter = self.cooldown
            self.num_bad_epochs = 0

    def _reduce_lr(self, epoch):
        """é™ä½å­¦ä¹ ç‡"""
        for i, param_group in enumerate(self.optimizer.param_groups):
            old_lr = float(param_group['lr'])
            new_lr = max(old_lr * self.factor, self.min_lrs[i])
            if old_lr - new_lr > self.eps:
                param_group['lr'] = new_lr
                if self.verbose:
                    print(f'ğŸ¯ åŸºäºF1åˆ†æ•°è°ƒæ•´å­¦ä¹ ç‡: {old_lr:.6f} â†’ {new_lr:.6f}')
                    print(f'   F1åˆ†æ•°å·²{self.patience}è½®æ— æ˜¾è‘—æ”¹å–„ (å½“å‰F1: {float(self.best):.4f})')

    @property
    def in_cooldown(self):
        return self.cooldown_counter > 0

    def is_better(self, a, best):
        if self.mode == 'max' and self.threshold_mode == 'rel':
            # å¯¹äºmaxæ¨¡å¼ï¼Œéœ€è¦ç›¸å¯¹æ”¹å–„ï¼šcurrent >= best * (1 + threshold)
            rel_epsilon = 1. + self.threshold
            return a >= best * rel_epsilon

        elif self.mode == 'max' and self.threshold_mode == 'abs':
            # å¯¹äºmaxæ¨¡å¼ï¼Œéœ€è¦ç»å¯¹æ”¹å–„ï¼šcurrent >= best + threshold
            return a >= best + self.threshold

        else:
            raise ValueError('mode must be max for F1 score')

    def _init_is_better(self, mode, threshold, threshold_mode):
        if mode not in {'max'}:
            raise ValueError('mode must be max for F1 score')
        if threshold_mode not in {'rel', 'abs'}:
            raise ValueError('threshold_mode must be one of rel, abs')

        if mode == 'max':
            self.mode_worse = -float('inf')

    def state_dict(self):
        """è¿”å›è°ƒåº¦å™¨çŠ¶æ€"""
        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}

    def load_state_dict(self, state_dict):
        """åŠ è½½è°ƒåº¦å™¨çŠ¶æ€"""
        self.__dict__.update(state_dict)


class CompositeScheduler:
    """
    å¤åˆè°ƒåº¦å™¨ï¼ŒåŒæ—¶æ”¯æŒåŸºäºæŸå¤±å’ŒF1åˆ†æ•°çš„è°ƒåº¦
    
    å¯ä»¥è®¾ç½®æƒé‡æ¥å¹³è¡¡ä¸¤ç§æŒ‡æ ‡çš„å½±å“ï¼Œæˆ–è€…è®¾ç½®ä¼˜å…ˆçº§ç­–ç•¥
    """
    
    def __init__(self, optimizer, strategy='f1_priority', 
                 loss_weight=0.3, f1_weight=0.7,
                 loss_params=None, f1_params=None):
        """
        Args:
            optimizer: ä¼˜åŒ–å™¨
            strategy (str): è°ƒåº¦ç­–ç•¥
                - 'f1_priority': ä¼˜å…ˆè€ƒè™‘F1åˆ†æ•°
                - 'loss_priority': ä¼˜å…ˆè€ƒè™‘æŸå¤±
                - 'weighted': åŠ æƒç»“åˆä¸¤ä¸ªæŒ‡æ ‡
                - 'strict_f1': ä»…ä½¿ç”¨F1åˆ†æ•°
                - 'strict_loss': ä»…ä½¿ç”¨æŸå¤±
            loss_weight (float): æŸå¤±æƒé‡ï¼ˆweightedç­–ç•¥æ—¶ä½¿ç”¨ï¼‰
            f1_weight (float): F1åˆ†æ•°æƒé‡ï¼ˆweightedç­–ç•¥æ—¶ä½¿ç”¨ï¼‰
            loss_params (dict): æŸå¤±è°ƒåº¦å™¨å‚æ•°
            f1_params (dict): F1è°ƒåº¦å™¨å‚æ•°
        """
        self.strategy = strategy
        self.loss_weight = loss_weight
        self.f1_weight = f1_weight
        
        # é»˜è®¤å‚æ•°
        default_loss_params = {
            'mode': 'min', 'factor': 0.5, 'patience': 5,
            'threshold': 1e-3, 'cooldown': 2, 'min_lr': 1e-6
        }
        default_f1_params = {
            'mode': 'max', 'factor': 0.5, 'patience': 5, 
            'threshold': 0.01, 'cooldown': 2, 'min_lr': 1e-6
        }
        
        # æ›´æ–°å‚æ•°
        if loss_params:
            default_loss_params.update(loss_params)
        if f1_params:
            default_f1_params.update(f1_params)
        
        # åˆ›å»ºè°ƒåº¦å™¨
        if strategy in ['f1_priority', 'weighted', 'loss_priority']:
            self.loss_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, **default_loss_params, verbose=False
            )
            self.f1_scheduler = F1BasedReduceLROnPlateau(
                optimizer, **default_f1_params, verbose=False
            )
        elif strategy == 'strict_f1':
            self.f1_scheduler = F1BasedReduceLROnPlateau(
                optimizer, **default_f1_params, verbose=True
            )
            self.loss_scheduler = None
        elif strategy == 'strict_loss':
            self.loss_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, **default_loss_params, verbose=True
            )
            self.f1_scheduler = None
        
        self.last_loss_lr = [group['lr'] for group in optimizer.param_groups]
        self.last_f1_lr = [group['lr'] for group in optimizer.param_groups]
        
        print(f"ğŸ“Š å¤åˆå­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–:")
        print(f"   ç­–ç•¥: {strategy}")
        if strategy == 'weighted':
            print(f"   æƒé‡: æŸå¤±={loss_weight:.1f}, F1={f1_weight:.1f}")
        print(f"   æŸå¤±è°ƒåº¦å™¨: {'å¯ç”¨' if self.loss_scheduler else 'ç¦ç”¨'}")
        print(f"   F1è°ƒåº¦å™¨: {'å¯ç”¨' if self.f1_scheduler else 'ç¦ç”¨'}")
    
    def step(self, loss_metrics, f1_metrics):
        """
        æ‰§è¡Œè°ƒåº¦æ­¥éª¤
        
        Args:
            loss_metrics (float): éªŒè¯æŸå¤±
            f1_metrics (float): F1åˆ†æ•°
        """
        if hasattr(self, 'loss_scheduler') and self.loss_scheduler:
            current_lr = [group['lr'] for group in self.loss_scheduler.optimizer.param_groups]
        else:
            current_lr = [group['lr'] for group in self.f1_scheduler.optimizer.param_groups]
        
        if self.strategy == 'strict_f1':
            if self.f1_scheduler:
                self.f1_scheduler.step(f1_metrics)
            
        elif self.strategy == 'strict_loss':
            if self.loss_scheduler:
                self.loss_scheduler.step(loss_metrics)
            
        elif self.strategy == 'f1_priority':
            # å…ˆå°è¯•F1è°ƒåº¦ï¼Œå¦‚æœæ²¡æœ‰è°ƒæ•´ï¼Œå†å°è¯•æŸå¤±è°ƒåº¦
            old_lr = current_lr[0]
            self.f1_scheduler.step(f1_metrics)
            new_lr = self.f1_scheduler.optimizer.param_groups[0]['lr']
            
            if new_lr == old_lr:  # F1è°ƒåº¦å™¨æ²¡æœ‰è°ƒæ•´
                self.loss_scheduler.step(loss_metrics)
                final_lr = self.loss_scheduler.optimizer.param_groups[0]['lr']
                if final_lr != new_lr:
                    print(f"ğŸ”„ æŸå¤±è°ƒåº¦å™¨ä»‹å…¥: F1æ— æ”¹å–„ä½†æŸå¤±éœ€è¦è°ƒæ•´")
            
        elif self.strategy == 'loss_priority':
            # å…ˆå°è¯•æŸå¤±è°ƒåº¦ï¼Œå¦‚æœæ²¡æœ‰è°ƒæ•´ï¼Œå†å°è¯•F1è°ƒåº¦
            old_lr = current_lr[0]
            self.loss_scheduler.step(loss_metrics)
            new_lr = self.loss_scheduler.optimizer.param_groups[0]['lr']
            
            if new_lr == old_lr:  # æŸå¤±è°ƒåº¦å™¨æ²¡æœ‰è°ƒæ•´
                self.f1_scheduler.step(f1_metrics)
                final_lr = self.f1_scheduler.optimizer.param_groups[0]['lr']
                if final_lr != new_lr:
                    print(f"ğŸ¯ F1è°ƒåº¦å™¨ä»‹å…¥: æŸå¤±æ­£å¸¸ä½†F1éœ€è¦è°ƒæ•´")
                    
        elif self.strategy == 'weighted':
            # åŠ æƒç­–ç•¥ï¼šåˆ›å»ºå¤åˆæŒ‡æ ‡
            # æ ‡å‡†åŒ–æŒ‡æ ‡ï¼ˆå‡è®¾F1åœ¨0-1ä¹‹é—´ï¼ŒæŸå¤±éœ€è¦å½’ä¸€åŒ–ï¼‰
            normalized_loss = min(loss_metrics / 2.0, 1.0)  # ç®€å•å½’ä¸€åŒ–
            composite_metric = self.loss_weight * (1 - normalized_loss) + self.f1_weight * f1_metrics
            
            # ä½¿ç”¨F1è°ƒåº¦å™¨å¤„ç†å¤åˆæŒ‡æ ‡
            self.f1_scheduler.step(composite_metric)
        
        # è®°å½•å­¦ä¹ ç‡å˜åŒ–
        new_lr = current_lr
        if hasattr(self, 'loss_scheduler') and self.loss_scheduler:
            new_lr = [group['lr'] for group in self.loss_scheduler.optimizer.param_groups]
        elif hasattr(self, 'f1_scheduler') and self.f1_scheduler:
            new_lr = [group['lr'] for group in self.f1_scheduler.optimizer.param_groups]
        
        if new_lr != current_lr:
            print(f"ğŸ“ˆ å­¦ä¹ ç‡å·²è°ƒæ•´è‡³: {new_lr[0]:.6f}")


def create_lr_scheduler(optimizer, scheduler_type='f1_based', **kwargs):
    """
    å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚å‡½æ•°
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_type (str): è°ƒåº¦å™¨ç±»å‹
            - 'f1_based': åŸºäºF1åˆ†æ•°
            - 'loss_based': åŸºäºæŸå¤±ï¼ˆæ ‡å‡†ï¼‰
            - 'composite_f1_priority': å¤åˆè°ƒåº¦å™¨ï¼ŒF1ä¼˜å…ˆ
            - 'composite_loss_priority': å¤åˆè°ƒåº¦å™¨ï¼ŒæŸå¤±ä¼˜å…ˆ
            - 'composite_weighted': å¤åˆè°ƒåº¦å™¨ï¼ŒåŠ æƒç»“åˆ
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        è°ƒåº¦å™¨å®ä¾‹
    """
    if scheduler_type == 'f1_based':
        return F1BasedReduceLROnPlateau(optimizer, **kwargs)
    
    elif scheduler_type == 'loss_based':
        default_params = {
            'mode': 'min', 'factor': 0.5, 'patience': 5,
            'threshold': 1e-3, 'cooldown': 2, 'min_lr': 1e-6
        }
        default_params.update(kwargs)
        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, **default_params)
    
    elif scheduler_type.startswith('composite_'):
        strategy = scheduler_type.replace('composite_', '')
        return CompositeScheduler(optimizer, strategy=strategy, **kwargs)
    
    else:
        raise ValueError(f"Unknown scheduler type: {scheduler_type}")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("="*80)
    print("æµ‹è¯•åŸºäºF1åˆ†æ•°çš„å­¦ä¹ ç‡è°ƒåº¦å™¨")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œä¼˜åŒ–å™¨
    import torch.nn as nn
    model = nn.Linear(10, 5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„è°ƒåº¦å™¨
    schedulers = {
        'F1è°ƒåº¦å™¨': create_lr_scheduler(optimizer, 'f1_based', patience=3),
        'æŸå¤±è°ƒåº¦å™¨': create_lr_scheduler(optimizer, 'loss_based', patience=3),
        'F1ä¼˜å…ˆå¤åˆ': create_lr_scheduler(optimizer, 'composite_f1_priority', 
                                        f1_params={'patience': 3}, loss_params={'patience': 3})
    }
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡å˜åŒ–
    print("\næ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹:")
    print("è½®æ¬¡  | F1åˆ†æ•° | éªŒè¯æŸå¤± | å­¦ä¹ ç‡å˜åŒ–")
    print("-" * 50)
    
    # æ¨¡æ‹ŸF1åˆ†æ•°åœæ»ä½†æŸå¤±è¿˜åœ¨éœ‡è¡çš„æƒ…å†µ
    f1_scores = [0.65, 0.72, 0.78, 0.82, 0.83, 0.83, 0.83, 0.82, 0.83, 0.83]
    val_losses = [1.2, 1.0, 0.8, 0.6, 0.5, 0.52, 0.48, 0.51, 0.49, 0.50]
    
    for scheduler_name, scheduler in schedulers.items():
        print(f"\nğŸ”§ æµ‹è¯•: {scheduler_name}")
        # é‡ç½®ä¼˜åŒ–å™¨å­¦ä¹ ç‡
        for group in optimizer.param_groups:
            group['lr'] = 0.001
        
        for epoch, (f1, loss) in enumerate(zip(f1_scores, val_losses)):
            old_lr = optimizer.param_groups[0]['lr']
            
            if hasattr(scheduler, 'step') and len(scheduler.step.__code__.co_varnames) > 2:
                # å¤åˆè°ƒåº¦å™¨
                scheduler.step(loss, f1)
            elif 'F1' in scheduler_name:
                scheduler.step(f1)
            else:
                scheduler.step(loss)
            
            new_lr = optimizer.param_groups[0]['lr']
            lr_change = "âœ“" if new_lr != old_lr else " "
            
            print(f"  {epoch+1:2d}  | {f1:.3f}  |  {loss:.3f}   | {old_lr:.6f} â†’ {new_lr:.6f} {lr_change}")
    
    print("\n" + "="*80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("F1è°ƒåº¦å™¨èƒ½æ›´å¥½åœ°æ£€æµ‹åˆ°F1åˆ†æ•°åœæ»ï¼ŒåŠæ—¶è°ƒæ•´å­¦ä¹ ç‡")
    print("="*80) 