import argparse
import os
import torch
import torch.backends
from exp.exp import Exp
from utils.print_args import print_args
from utils.memory_cleaner import MemoryCleaner
import random
import numpy as np
import atexit
import signal
import sys

def cleanup_handler(signum=None, frame=None):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºç¨‹åºè¢«ä¸­æ–­æ—¶æ¸…ç†å†…å­˜"""
    print(f"\nç¨‹åºè¢«ä¸­æ–­ (ä¿¡å·: {signum})ï¼Œæ­£åœ¨æ¸…ç†å†…å­˜...")
    MemoryCleaner.full_cleanup()
    MemoryCleaner.print_memory_info()
    sys.exit(0)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, cleanup_handler)   # Ctrl+C
signal.signal(signal.SIGTERM, cleanup_handler)  # ç»ˆæ­¢ä¿¡å·

if __name__ == '__main__':
    # ç¨‹åºå¼€å§‹æ—¶æ£€æŸ¥å’Œæ¸…ç†å†…å­˜
    print("=== ç¨‹åºå¯åŠ¨ ===")
    # MemoryCleaner.print_memory_info()
    # MemoryCleaner.full_cleanup()
    
    fix_seed = 2021
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)

    parser = argparse.ArgumentParser(description='GAF Classification')

    # basic config
    parser.add_argument('--task_name', type=str, required=False, default='classification',
                        help='task name, options:[classification]')
    parser.add_argument('--is_training', type=int, required=False, default=1, help='status')
    parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')
    parser.add_argument('--model', type=str, required=True, default='Autoformer',
                        help='model name, options: [Autoformer, Transformer, TimesNet]')

    # data loader
    parser.add_argument('--data', type=str, required=True, default='SAHU', help='dataset type')
    parser.add_argument('--root_path', type=str, default='./dataset/SAHU/', help='root path of the data file')
    parser.add_argument('--step', type=int, required=True, help='slide step')
    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')
    parser.add_argument('--num_class', type=int, default=2, help='number of classes for classification')
    parser.add_argument('--result_path', type=str, default='./result/', help='result path')
    parser.add_argument('--test_size', type=float, default=0.2, help='test size')
    parser.add_argument('--val_size', type=float, default=0.2, help='validation size')
    parser.add_argument('--rows', type=int, default=None, help='roll size')
    # model define
    parser.add_argument('--seq_len', type=int, default=64, help='input sequence length')
    parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')
    parser.add_argument('--d_model', type=int, default=512, help='dimension of model')
    parser.add_argument('--n_heads', type=int, default=8, help='num of heads')
    parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')
    parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')
    parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')
    parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')
    parser.add_argument('--factor', type=int, default=1, help='attn factor')
    parser.add_argument('--distil', action='store_false',
                        help='whether to use distilling in encoder, using this argument means not using distilling',
                        default=True)
    parser.add_argument('--dropout', type=float, default=0.1, help='dropout')
    parser.add_argument('--embed', type=str, default='timeF',
                        help='time features encoding, options:[timeF, fixed, learned]')
    parser.add_argument('--activation', type=str, default='gelu', help='activation')
    parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')
    parser.add_argument('--gaf_method', type=str, default='summation', help='GAF method; summation or difference')
    parser.add_argument('--feature_dim', type=int, default=32, help='feature dimension')
    parser.add_argument('--data_type_method', type=str, default='uint8', 
                        help='Data type conversion method; options: [float32, uint8, uint16]')
    # GNN
    parser.add_argument('--use_attention', type=bool, default=True, help='use attention')
    parser.add_argument('--hidden_dim', type=int, default=64, help='hidden dimension')
    parser.add_argument('--sample_size', type=int, default=1000, help='sample size')
    
    # 1D-CNNå‚æ•°
    parser.add_argument('--cnn_dropout_rate', type=float, default=0.3, 
                        help='Dropout rate for 1D-CNN model')
    parser.add_argument('--cnn_use_batch_norm', type=bool, default=True, 
                        help='Whether to use BatchNorm in 1D-CNN model')
    # æ·»åŠ é€šé“åˆ†ç»„å‚æ•°
    parser.add_argument('--channel_groups', type=str, default=None,
                        help='Channel grouping for ClusteredResNet. Format: "0,1,2|3,4,5|6,7,8"')
    parser.add_argument('--hvac_groups', type=str, default=None,
                        help='HVAC signal grouping for MultiImageFeatureNet. Format: "SA_TEMP,OA_TEMP|OA_CFM,RA_CFM|SA_SP"')
    # é€šé“å‹ç¼©å‚æ•°
    parser.add_argument('--use_channel_compression', action='store_true', default=False, help='use channel compression')
    parser.add_argument('--compression_strategy', type=str, default='signal_compression', help='compression strategy')
    parser.add_argument('--compression_ratio', type=float, default=0.7, help='compression ratio')
    parser.add_argument('--compression_channels', type=int, default=None, help='compression channels')
    parser.add_argument('--adaptive_compression_ratios', type=list, default=[0.5, 0.7, 0.8], help='adaptive compression ratios')
    parser.add_argument('--hvac_group_compression_ratios', type=list, default=None, help='hvac group compression ratios')
    # åŒè·¯ç½‘ç»œæ¨¡å—é…ç½®å‚æ•°
    parser.add_argument('--extractor_type', type=str, default='large_kernel',
                        help='Feature extractor type for DualGAFNet. Options: [large_kernel, inception, dilated, multiscale]')
    parser.add_argument('--fusion_type', type=str, default='adaptive',
                        help='Feature fusion type for DualGAFNet. Options: [adaptive, concat, bidirectional, gated, add, mul, weighted_add]')
    parser.add_argument('--attention_type', type=str, default='channel',
                        help='Attention type for DualGAFNet. Options: [channel, spatial, cbam, self, none]')
    parser.add_argument('--classifier_type', type=str, default='mlp',
                        help='Classifier type for DualGAFNet. Options: [mlp, simple, residual, residual_bottleneck, residual_dense, feature_compression, hierarchical, efficient_mlp, efficient_simple, global_pooling, conv1d, separable]')
    
    # ç»Ÿè®¡ç‰¹å¾é…ç½®ï¼ˆæ–°å¢ï¼‰
    parser.add_argument('--use_statistical_features', action='store_true', default=False, help='æ˜¯å¦ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾')
    parser.add_argument('--stat_type', type=str, default='comprehensive',
                       choices=['basic', 'comprehensive', 'correlation_focused'], help='ç»Ÿè®¡ç‰¹å¾ç±»å‹')
    parser.add_argument('--multimodal_fusion_strategy', type=str, default='concat',
                       choices=['concat', 'attention', 'gated', 'adaptive','film'], help='å¤šæ¨¡æ€èåˆç­–ç•¥')

    # ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾é…ç½®ï¼ˆæ–°å¢ï¼‰
    parser.add_argument('--use_signal_level_stats', action='store_true', default=False, 
                       help='æ˜¯å¦ä½¿ç”¨ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ï¼ˆå¯¹æ¯”å®éªŒï¼Œæ›¿ä»£å…¨å±€ç»Ÿè®¡ç‰¹å¾ï¼‰')
    parser.add_argument('--signal_stat_type', type=str, default='comprehensive',
                       choices=['basic', 'comprehensive', 'extended'], help='ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ç±»å‹')
    parser.add_argument('--signal_stat_fusion_strategy', type=str, default='concat_project',
                       choices=['concat_project', 'attention_fusion', 'gated_fusion', 'residual_fusion', 'cross_attention', 'adaptive_fusion'],
                       help='ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾èåˆç­–ç•¥')
    parser.add_argument('--signal_stat_feature_dim', type=int, default=32,
                       help='ä¿¡å·çº§ç»Ÿè®¡ç‰¹å¾ç»´åº¦')

    # æ¶ˆèå®éªŒé…ç½®å‚æ•°
    parser.add_argument('--use_diff_branch', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨å·®åˆ†åˆ†æ”¯è¿›è¡ŒGAFèåˆï¼ˆæ¶ˆèå®éªŒå¼€å…³ï¼‰ã€‚Falseæ—¶ä»…ä½¿ç”¨sumåˆ†æ”¯')
    parser.add_argument('--ablation_mode', type=str, default='none',
                       choices=['none', 'no_diff', 'no_stat', 'no_attention', 'minimal'],
                       help='æ¶ˆèå®éªŒæ¨¡å¼å¿«æ·è®¾ç½®: none(å®Œæ•´æ¨¡å‹), no_diff(æ— å·®åˆ†åˆ†æ”¯), no_stat(æ— ç»Ÿè®¡ç‰¹å¾), no_attention(æ— æ³¨æ„åŠ›), minimal(æœ€ç®€æ¨¡å‹)')
    
    # SimpleGAFNetç‰¹æœ‰å‚æ•°ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰
    parser.add_argument('--backbone_type', type=str, default='resnet18',
                       choices=['simple_cnn', 'resnet18', 'resnet34', 'resnet50', 'inception'],
                       help='SimpleGAFNetçš„ä¸»å¹²æ¶æ„ç±»å‹: simple_cnn(è½»é‡çº§), resnet18(æ¨è), resnet34(æ·±å±‚), resnet50(å®éªŒæ€§), inception(å¤šå°ºåº¦)')
    parser.add_argument('--use_sum_branch', action='store_true', default=True,
                       help='SimpleGAFNeté€‰æ‹©GAFåˆ†æ”¯: True(ä½¿ç”¨summation GAF), False(ä½¿ç”¨difference GAF)')

    # optimization
    parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')
    parser.add_argument('--cuda_debug', action='store_true', default=False,
                        help='å¯ç”¨CUDAè°ƒè¯•æ¨¡å¼ï¼ˆç”¨äºè¯Šæ–­CUDAé”™è¯¯ï¼‰')
    parser.add_argument('--gpu_memory_fraction', type=float, default=0.9,
                        help='GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹é™åˆ¶ (0.7-0.95)')
    parser.add_argument('--safe_mode', action='store_true', default=False,
                        help='å®‰å…¨æ¨¡å¼ï¼šå¯ç”¨æ‰€æœ‰å†…å­˜ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†')
    parser.add_argument('--drop_last_batch', action='store_true', default=True, 
                        help='ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batchï¼ˆé¿å…BatchNormé”™è¯¯ï¼‰')
    parser.add_argument('--itr', type=int, default=1, help='experiments times')
    parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')
    parser.add_argument('--patience', type=int, default=15, help='early stopping patience')
    parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')
    parser.add_argument('--des', type=str, default='test', help='exp description')
    parser.add_argument('--loss', type=str, default='MSE', help='loss function')
    parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®ï¼ˆåŸºäºF1åˆ†æ•°çš„æ”¹è¿›æ–¹æ¡ˆï¼‰
    parser.add_argument('--lr_scheduler_type', type=str, default='f1_based',
                        choices=['f1_based', 'loss_based', 'composite_f1_priority', 'composite_weighted'],
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹: f1_based(åŸºäºF1åˆ†æ•°ï¼Œæ¨è), loss_based(åŸºäºæŸå¤±), composite_f1_priority(F1ä¼˜å…ˆå¤åˆ), composite_weighted(åŠ æƒå¤åˆ)')
    parser.add_argument('--lr_loss_weight', type=float, default=0.3,
                        help='å¤åˆè°ƒåº¦å™¨ä¸­æŸå¤±æƒé‡ (ä»…composite_weightedæ—¶ä½¿ç”¨)')
    parser.add_argument('--lr_f1_weight', type=float, default=0.7,
                        help='å¤åˆè°ƒåº¦å™¨ä¸­F1æƒé‡ (ä»…composite_weightedæ—¶ä½¿ç”¨)')
    parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)
    
    # æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–ï¼ˆé’ˆå¯¹å°batchè®­ç»ƒï¼‰
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, 
                        help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚è®¾ä¸º>1æ—¶å®ç°æ¢¯åº¦ç´¯ç§¯ï¼Œæœ‰æ•ˆbatch_size = batch_size * gradient_accumulation_steps')
    parser.add_argument('--enable_auto_gradient_accumulation', action='store_true', default=False,
                        help='æ ¹æ®batch_sizeè‡ªåŠ¨å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆbatch_size<8æ—¶è‡ªåŠ¨è®¾ä¸º2ï¼Œ<4æ—¶è‡ªåŠ¨è®¾ä¸º4ï¼‰')
    
    # é«˜çº§æŸå¤±å‡½æ•°é…ç½®ï¼ˆç”¨äºè§£å†³ç±»åˆ«ç›¸ä¼¼æ€§é—®é¢˜ï¼‰
    parser.add_argument('--loss_type', type=str, default='ce', 
                        choices=['ce', 'label_smoothing', 'focal', 'confidence_penalty', 'combined',
                                'label_smoothing_optimized', 'hybrid_focal', 'adaptive_smoothing'],
                        help='æŸå¤±å‡½æ•°ç±»å‹: ce(äº¤å‰ç†µ), label_smoothing(æ ‡ç­¾å¹³æ»‘), focal(ç„¦ç‚¹æŸå¤±), confidence_penalty(ç½®ä¿¡åº¦æƒ©ç½š), combined(ç»„åˆæŸå¤±), label_smoothing_optimized(ä¼˜åŒ–æ ‡ç­¾å¹³æ»‘), hybrid_focal(æ··åˆç„¦ç‚¹æŸå¤±), adaptive_smoothing(è‡ªé€‚åº”å¹³æ»‘)')
    
    # æ ‡ç­¾å¹³æ»‘å‚æ•°
    parser.add_argument('--label_smoothing', type=float, default=0.1,
                        help='æ ‡ç­¾å¹³æ»‘å› å­ (0.0-0.5)ï¼Œæ¨èå€¼ï¼šç›¸ä¼¼ç±»åˆ«0.1-0.2ï¼Œå·®å¼‚è¾ƒå¤§ç±»åˆ«0.05-0.1')
    
    # Focal Losså‚æ•°
    parser.add_argument('--focal_alpha', type=float, default=1.0,
                        help='Focal Lossçš„alphaå‚æ•°ï¼Œç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬')
    parser.add_argument('--focal_gamma', type=float, default=2.0,
                        help='Focal Lossçš„gammaå‚æ•°ï¼Œæ§åˆ¶éš¾æ ·æœ¬çš„èšç„¦ç¨‹åº¦ (1.0-3.0)')
    
    # ç½®ä¿¡åº¦æƒ©ç½šå‚æ•°
    parser.add_argument('--confidence_penalty_beta', type=float, default=0.1,
                        help='ç½®ä¿¡åº¦æƒ©ç½šå¼ºåº¦ (0.01-0.2)ï¼Œè¾ƒé«˜å€¼ä¼šå‡å°‘è¿‡åº¦è‡ªä¿¡')
    
    # ä¼˜åŒ–æŸå¤±å‡½æ•°å‚æ•°ï¼ˆåŸºäºæ€§èƒ½æµ‹è¯•ï¼štimmå®ç°æ¯”è‡ªå®šä¹‰å®ç°å¿«10-20%ï¼‰
    parser.add_argument('--use_timm_loss', action='store_true', default=True,
                        help='ä¼˜å…ˆä½¿ç”¨timmä¼˜åŒ–å®ç°ï¼Œæ€§èƒ½æå‡10-20%ï¼Œå†…å­˜æ•ˆç‡æ›´é«˜')
    parser.add_argument('--adaptive_initial_smoothing', type=float, default=0.2,
                        help='è‡ªé€‚åº”æ ‡ç­¾å¹³æ»‘åˆå§‹å€¼ï¼ˆè®­ç»ƒå¼€å§‹æ—¶ï¼‰')
    parser.add_argument('--adaptive_final_smoothing', type=float, default=0.05,
                        help='è‡ªé€‚åº”æ ‡ç­¾å¹³æ»‘æœ€ç»ˆå€¼ï¼ˆè®­ç»ƒç»“æŸæ—¶ï¼‰')
    parser.add_argument('--adaptive_decay_epochs', type=int, default=30,
                        help='è‡ªé€‚åº”å¹³æ»‘è¡°å‡å‘¨æœŸï¼ˆepochæ•°ï¼‰')
    
    # ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰- é»˜è®¤ç¦ç”¨ï¼Œé€‚ç”¨äºå¹³è¡¡æ•°æ®é›†
    parser.add_argument('--enable_class_weights', action='store_true', 
                        help='å¯ç”¨ç±»åˆ«æƒé‡ï¼ˆä»…å½“æ•°æ®ä¸å¹³è¡¡æ—¶ä½¿ç”¨ï¼‰')
    parser.add_argument('--class_weights', type=str, default=None,
                        help='æ‰‹åŠ¨æŒ‡å®šç±»åˆ«æƒé‡ï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„æ•°å­—ï¼Œå¦‚ "1.0,2.0,1.5"')
    
    # æŸå¤±å‡½æ•°é¢„è®¾é…ç½®
    parser.add_argument('--loss_preset', type=str, default=None,
                        choices=['hvac_similar', 'imbalanced_focus', 'hard_samples', 'overconfidence_prevention',
                                'hvac_similar_optimized', 'hvac_adaptive', 'hvac_hard_samples', 'production_optimized'],
                        help='æŸå¤±å‡½æ•°é¢„è®¾é…ç½®: hvac_similar(HVACç›¸ä¼¼ç±»åˆ«), imbalanced_focus(ç±»åˆ«ä¸å¹³è¡¡), hard_samples(éš¾åˆ†ç±»æ ·æœ¬), overconfidence_prevention(é˜²æ­¢è¿‡åº¦è‡ªä¿¡), hvac_similar_optimized(HVACç›¸ä¼¼ç±»åˆ«+é«˜æ€§èƒ½ä¼˜åŒ–,æ¨è), hvac_adaptive(HVACè‡ªé€‚åº”å¹³æ»‘), hvac_hard_samples(HVACéš¾æ ·æœ¬èšç„¦), production_optimized(ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–)')

    # parallel processing optimization (å¹¶è¡Œå¤„ç†ä¼˜åŒ–)
    parser.add_argument('--n_jobs', type=int, default=16, 
                        help='Number of parallel jobs for data processing. -1 means auto-detect (min(cpu_count, 8))')
    parser.add_argument('--use_multiprocessing', action='store_true', default=False,
                        help='Enable multiprocessing for GAF generation and data conversion')
    parser.add_argument('--chunk_size', type=int, default=800,
                        help='Chunk size for parallel processing. Larger values use more memory but reduce overhead')
    parser.add_argument('--disable_parallel', action='store_true', default=True,
                        help='Disable all parallel processing optimizations (useful for debugging)')
    parser.add_argument('--use_shared_memory', action='store_true', default=True,
                        help='Enable shared memory optimization for faster inter-process communication')
    parser.add_argument('--disable_shared_memory', action='store_true', default=False,
                        help='Disable shared memory optimization (fallback to standard multiprocessing)')

    # GPU
    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')
    parser.add_argument('--gpu', type=int, default=0, help='gpu')
    parser.add_argument('--gpu_type', type=str, default='cuda', help='gpu type')  # cuda or mps
    parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
    parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')

    args = parser.parse_args()
    
    # å¤„ç†å¹¶è¡Œå¤„ç†å‚æ•°
    import multiprocessing as mp
    import sys
    if args.disable_parallel:
        # ç¦ç”¨æ‰€æœ‰å¹¶è¡Œå¤„ç†
        args.use_multiprocessing = False
        args.use_shared_memory = False
        args.n_jobs = 1
        print("ğŸ”§ å¹¶è¡Œå¤„ç†å·²ç¦ç”¨ (disable_parallel=True)")
    else:
        # è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        if args.n_jobs == -1:
            args.n_jobs = min(mp.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
        elif args.n_jobs <= 0:
            args.n_jobs = 1
        
        # å¤„ç†å…±äº«å†…å­˜é…ç½®
        if args.disable_shared_memory:
            args.use_shared_memory = False
            print("ğŸ”§ å…±äº«å†…å­˜ä¼˜åŒ–å·²ç¦ç”¨")
        elif sys.version_info < (3, 8):
            args.use_shared_memory = False
            print("âš ï¸ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œç¦ç”¨å…±äº«å†…å­˜ä¼˜åŒ– (éœ€è¦Python 3.8+)")
    
    # ğŸ”§ CUDAè°ƒè¯•å’ŒGPUå†…å­˜ç®¡ç†é…ç½®
    if args.cuda_debug or args.safe_mode:
        print("\nğŸ”§ å¯ç”¨CUDAè°ƒè¯•å’Œå®‰å…¨æ¨¡å¼é…ç½®...")
        import os
        
        # è®¾ç½®CUDAè°ƒè¯•ç¯å¢ƒå˜é‡
        if args.cuda_debug:
            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
            print("âœ… å·²å¯ç”¨CUDA_LAUNCH_BLOCKING=1 (åŒæ­¥æ‰§è¡Œï¼Œä¾¿äºè°ƒè¯•)")
        
        # GPUå†…å­˜ä¼˜åŒ–
        if torch.cuda.is_available():
            current_device = torch.cuda.current_device()
            total_memory = torch.cuda.get_device_properties(current_device).total_memory
            total_memory_gb = total_memory / (1024**3)
            
            print(f"ğŸ¯ GPUä¿¡æ¯: {torch.cuda.get_device_name(current_device)}")
            print(f"ğŸ“Š æ€»æ˜¾å­˜: {total_memory_gb:.2f} GB")
            
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            if args.safe_mode:
                # å¯ç”¨å†…å­˜æ± å¤ç”¨å’Œé¢„åˆ†é…
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
                print("âœ… å·²å¯ç”¨CUDAå†…å­˜ä¼˜åŒ–ç­–ç•¥")
                
                # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
                if hasattr(torch.cuda, 'set_memory_fraction'):
                    torch.cuda.set_memory_fraction(args.gpu_memory_fraction)
                    print(f"âœ… GPUå†…å­˜é™åˆ¶: {args.gpu_memory_fraction*100:.0f}%")
                
            # æ¸…ç†åˆå§‹GPUå†…å­˜
            torch.cuda.empty_cache()
            if hasattr(torch.cuda, 'ipc_collect'):
                torch.cuda.ipc_collect()
            
            # æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
            allocated = torch.cuda.memory_allocated(current_device) / (1024**3)
            reserved = torch.cuda.memory_reserved(current_device) / (1024**3)
            print(f"ğŸ“ˆ å½“å‰GPUå†…å­˜ä½¿ç”¨: {allocated:.3f} GB (å·²åˆ†é…) / {reserved:.3f} GB (å·²ä¿ç•™)")
            
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUé…ç½®")
    
    # ğŸ”§ å®‰å…¨æ¨¡å¼é¢å¤–ä¼˜åŒ–
    if args.safe_mode:
        print("\nğŸ›¡ï¸ å®‰å…¨æ¨¡å¼ï¼šå¯ç”¨é¢å¤–çš„ç¨³å®šæ€§ä¼˜åŒ–...")
        
        # è‡ªåŠ¨è°ƒæ•´batch_sizeä»¥é€‚åº”å†…å­˜é™åˆ¶
        if torch.cuda.is_available():
            total_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if total_memory_gb < 6:  # å°æ˜¾å­˜GPU
                if args.batch_size > 16:
                    original_batch_size = args.batch_size
                    args.batch_size = 16
                    print(f"ğŸ”§ å°æ˜¾å­˜è®¾å¤‡ï¼Œæ‰¹æ¬¡å¤§å°è°ƒæ•´: {original_batch_size} â†’ {args.batch_size}")
            elif total_memory_gb < 8:  # ä¸­ç­‰æ˜¾å­˜GPU
                if args.batch_size > 32:
                    original_batch_size = args.batch_size
                    args.batch_size = 32
                    print(f"ğŸ”§ ä¸­ç­‰æ˜¾å­˜è®¾å¤‡ï¼Œæ‰¹æ¬¡å¤§å°è°ƒæ•´: {original_batch_size} â†’ {args.batch_size}")
        
        # å¼ºåˆ¶å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæ”¯æŒï¼‰
        print("âœ… å®‰å…¨æ¨¡å¼ä¼˜åŒ–å·²å¯ç”¨")
    
    print(f"\nğŸ“Š æœ€ç»ˆé…ç½®:")
    print(f"   CUDAè°ƒè¯•: {'å¯ç”¨' if args.cuda_debug else 'ç¦ç”¨'}")
    print(f"   å®‰å…¨æ¨¡å¼: {'å¯ç”¨' if args.safe_mode else 'ç¦ç”¨'}")
    print(f"   GPUå†…å­˜é™åˆ¶: {args.gpu_memory_fraction*100:.0f}%")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"   Workeræ•°é‡: {args.num_workers}")
    
    # æ ¹æ®ç³»ç»Ÿèµ„æºè°ƒæ•´å‚æ•°
    available_memory_gb = None
    try:
        import psutil
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
    except ImportError:
        print("âš ï¸ psutilæœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨æ£€æµ‹å†…å­˜å¤§å°")
    
    # æ ¹æ®å†…å­˜å¤§å°è°ƒæ•´chunk_size - é’ˆå¯¹é«˜ç«¯æœåŠ¡å™¨ä¼˜åŒ–
    if available_memory_gb is not None:
        cpu_cores = mp.cpu_count()
        
        if available_memory_gb < 8:  # å°å†…å­˜ç³»ç»Ÿ
            args.chunk_size = min(args.chunk_size, 50)
            args.n_jobs = min(args.n_jobs, 2)
            print(f"ğŸ’¾ æ£€æµ‹åˆ°å°å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œè°ƒæ•´å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
        elif available_memory_gb < 32:  # ä¸­ç­‰å†…å­˜ç³»ç»Ÿ
            args.chunk_size = max(args.chunk_size, 200)
            args.n_jobs = min(args.n_jobs, 8)
            print(f"ğŸ’¾ æ£€æµ‹åˆ°ä¸­ç­‰å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œè°ƒæ•´å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
        elif available_memory_gb < 64:  # å¤§å†…å­˜ç³»ç»Ÿ
            args.chunk_size = max(args.chunk_size, 400)
            args.n_jobs = min(args.n_jobs, 16)
            print(f"ğŸ’¾ æ£€æµ‹åˆ°å¤§å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œä¼˜åŒ–å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
        else:  # è¶…å¤§å†…å­˜æœåŠ¡å™¨ (æ‚¨çš„é…ç½®)
            # é’ˆå¯¹32æ ¸128GBçš„é«˜ç«¯é…ç½®
            if cpu_cores >= 32:
                # ä½¿ç”¨é€‚ä¸­çš„è¿›ç¨‹æ•°ï¼Œé¿å…è¿‡åº¦å¹¶è¡Œ
                args.n_jobs = min(args.n_jobs, 20)  # ç•™å‡ºä¸€äº›æ ¸å¿ƒç»™ç³»ç»Ÿ
                args.chunk_size = max(args.chunk_size, 800)  # å¤§å—å¤§å°å‡å°‘å¼€é”€
                print(f"ğŸš€ æ£€æµ‹åˆ°é«˜ç«¯æœåŠ¡å™¨ ({available_memory_gb:.1f}GB, {cpu_cores}æ ¸)ï¼Œé«˜æ€§èƒ½é…ç½®ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
            else:
                args.chunk_size = max(args.chunk_size, 600)
                print(f"ğŸ’¾ æ£€æµ‹åˆ°è¶…å¤§å†…å­˜ç³»ç»Ÿ ({available_memory_gb:.1f}GB)ï¼Œä¼˜åŒ–å‚æ•°ï¼šchunk_size={args.chunk_size}, n_jobs={args.n_jobs}")
    
    shared_memory_status = "å¯ç”¨" if args.use_shared_memory else "ç¦ç”¨"
    print(f"ğŸš€ å¹¶è¡Œå¤„ç†é…ç½® - è¿›ç¨‹æ•°: {args.n_jobs}, å¤šè¿›ç¨‹: {args.use_multiprocessing}, å—å¤§å°: {args.chunk_size}, å…±äº«å†…å­˜: {shared_memory_status}")
    
    # å¤„ç†BatchNormå‹å¥½é…ç½®
    print(f"ğŸ“Š DataLoaderé…ç½®: num_workers={args.num_workers}, batch_size={args.batch_size}")
    if args.drop_last_batch:
        print(f"ğŸ”§ å·²å¯ç”¨drop_last_batch=Trueï¼Œå°†ä¸¢å¼ƒä¸å®Œæ•´çš„æœ€åä¸€ä¸ªbatchï¼ˆé¿å…BatchNormåœ¨batch_size=1æ—¶å‡ºé”™ï¼‰")
    else:
        print(f"âš ï¸ drop_last_batch=Falseï¼Œå¯èƒ½åœ¨æœ€åä¸€ä¸ªbatchæ—¶é‡åˆ°BatchNormé”™è¯¯")
        print(f"ğŸ’¡ æç¤ºï¼šå¦‚æœé‡åˆ°BatchNormé”™è¯¯ï¼Œå»ºè®®è®¾ç½® --drop_last_batch")
    
    # å¤„ç†æ¶ˆèå®éªŒæ¨¡å¼ï¼ˆåœ¨å…¶ä»–å‚æ•°å¤„ç†ä¹‹å‰ï¼‰
    if args.ablation_mode != 'none':
        print(f"\nğŸ”¬ åº”ç”¨æ¶ˆèå®éªŒæ¨¡å¼: {args.ablation_mode}")
        
        if args.ablation_mode == 'no_diff':
            # ç§»é™¤å·®åˆ†åˆ†æ”¯
            args.use_diff_branch = False
            print(f"   âŒ ç¦ç”¨å·®åˆ†åˆ†æ”¯ (use_diff_branch=False)")
            
        elif args.ablation_mode == 'no_stat':
            # ç§»é™¤ç»Ÿè®¡ç‰¹å¾
            args.use_statistical_features = False
            print(f"   âŒ ç¦ç”¨ç»Ÿè®¡ç‰¹å¾ (use_statistical_features=False)")
            
        elif args.ablation_mode == 'no_attention':
            # ç§»é™¤æ³¨æ„åŠ›æœºåˆ¶
            args.attention_type = 'none'
            print(f"   âŒ ç¦ç”¨æ³¨æ„åŠ›æœºåˆ¶ (attention_type=none)")
            
        elif args.ablation_mode == 'minimal':
            # æœ€ç®€åŒ–æ¨¡å‹ï¼šç§»é™¤æ‰€æœ‰é«˜çº§ç»„ä»¶
            args.use_diff_branch = False
            args.use_statistical_features = False
            args.attention_type = 'none'
            print(f"   âŒ ç¦ç”¨å·®åˆ†åˆ†æ”¯ (use_diff_branch=False)")
            print(f"   âŒ ç¦ç”¨ç»Ÿè®¡ç‰¹å¾ (use_statistical_features=False)")
            print(f"   âŒ ç¦ç”¨æ³¨æ„åŠ›æœºåˆ¶ (attention_type=none)")
            print(f"   ğŸ’¡ ç°åœ¨ä½¿ç”¨æœ€ç®€åŒ–æ¨¡å‹ï¼ˆä»…sumåˆ†æ”¯ + åŸºç¡€ResNetï¼‰")
        
        print(f"ğŸ”¬ æ¶ˆèå®éªŒé…ç½®å®Œæˆ\n")
    else:
        # æ˜¾ç¤ºå½“å‰å®Œæ•´æ¨¡å‹é…ç½®
        ablation_status = []
        if not args.use_diff_branch:
            ablation_status.append("å·®åˆ†åˆ†æ”¯å·²ç¦ç”¨")
        if not args.use_statistical_features:
            ablation_status.append("ç»Ÿè®¡ç‰¹å¾å·²ç¦ç”¨")
        if args.attention_type == 'none':
            ablation_status.append("æ³¨æ„åŠ›æœºåˆ¶å·²ç¦ç”¨")
        
        if ablation_status:
            print(f"\nğŸ”¬ æ‰‹åŠ¨æ¶ˆèé…ç½®: {' + '.join(ablation_status)}")
        else:
            print(f"\nğŸ”¬ ä½¿ç”¨å®Œæ•´æ¨¡å‹ï¼ˆæœªå¯ç”¨æ¶ˆèå®éªŒï¼‰")
    
    # å¤„ç†æ¢¯åº¦ç´¯ç§¯å‚æ•°
    if args.enable_auto_gradient_accumulation:
        if args.batch_size < 4:
            args.gradient_accumulation_steps = 4
            print(f"ğŸ”„ è‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯: batch_size={args.batch_size} < 4ï¼Œè®¾ç½®ç´¯ç§¯æ­¥æ•°ä¸º{args.gradient_accumulation_steps}")
        elif args.batch_size < 8:
            args.gradient_accumulation_steps = 2
            print(f"ğŸ”„ è‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯: batch_size={args.batch_size} < 8ï¼Œè®¾ç½®ç´¯ç§¯æ­¥æ•°ä¸º{args.gradient_accumulation_steps}")
        else:
            print(f"ğŸ”„ è‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯: batch_size={args.batch_size} >= 8ï¼Œæ— éœ€æ¢¯åº¦ç´¯ç§¯")
    
    if args.gradient_accumulation_steps > 1:
        effective_batch_size = args.batch_size * args.gradient_accumulation_steps
        print(f"ğŸ“Š æ¢¯åº¦ç´¯ç§¯å·²å¯ç”¨:")
        print(f"   å®é™…batch_size: {args.batch_size}")
        print(f"   ç´¯ç§¯æ­¥æ•°: {args.gradient_accumulation_steps}")
        print(f"   æœ‰æ•ˆbatch_size: {effective_batch_size}")
        print(f"   å»ºè®®: è¿™æœ‰åŠ©äºå‡å°‘å°batchå¸¦æ¥çš„æ¢¯åº¦å™ªå£°")
    
    # å¤„ç†é€šé“åˆ†ç»„å‚æ•°
    if args.channel_groups is not None:
        try:
            # å°†å­—ç¬¦ä¸²æ ¼å¼çš„åˆ†ç»„è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            args.channel_groups = [
                [int(x) for x in group.split(',')]
                for group in args.channel_groups.split('|')
            ]
        except Exception as e:
            print(f"Error parsing channel_groups: {e}")
            print("Using default channel grouping")
            args.channel_groups = None
    
    # å¤„ç†HVACä¿¡å·åˆ†ç»„å‚æ•°
    if args.hvac_groups is not None:
        try:
            # å°†å­—ç¬¦ä¸²æ ¼å¼çš„HVACåˆ†ç»„è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            args.hvac_groups = [
                [signal.strip() for signal in group.split(',')]
                for group in args.hvac_groups.split('|')
            ]
            print(f"HVAC Groups parsed: {len(args.hvac_groups)} groups")
            for i, group in enumerate(args.hvac_groups):
                print(f"  Group {i}: {group}")
        except Exception as e:
            print(f"Error parsing hvac_groups: {e}")
            print("Using default HVAC grouping")
            args.hvac_groups = None

    if torch.cuda.is_available() and args.use_gpu:
        args.device = torch.device('cuda:{}'.format(args.gpu))
        print('Using GPU')
    else:
        if hasattr(torch.backends, "mps"):
            args.device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
        else:
            args.device = torch.device("cpu")
        print('Using cpu or mps')

    if args.use_gpu and args.use_multi_gpu:
        args.devices = args.devices.replace(' ', '')
        device_ids = args.devices.split(',')
        args.device_ids = [int(id_) for id_ in device_ids]
        args.gpu = args.device_ids[0]

    print('Args in experiment:')
    print_args(args)

    print("CUDA_VISIBLE_DEVICES:", os.environ.get("CUDA_VISIBLE_DEVICES"))
    print("torch.cuda.device_count():", torch.cuda.device_count())
    print("torch.cuda.current_device():", torch.cuda.current_device())
    print("torch.cuda.get_device_name(0):", torch.cuda.get_device_name(0))
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    if args.is_training:
        for ii in range(args.itr):
            # setting record of experiments
            # æ„å»ºå®éªŒè®¾ç½®å­—ç¬¦ä¸²
            setting = f'{args.data}_{args.model}_{args.des}_sl{args.seq_len}_step{args.step}_' \
                    f'fd{args.feature_dim}_extractor-{args.extractor_type}_gaf-{args.fusion_type}_' \
                    f'attention-{args.attention_type}_classifier-{args.classifier_type}'
            
            if args.use_statistical_features:
                setting += f'_stat-{args.stat_type}_fusion-{args.multimodal_fusion_strategy}'
            
            if args.ablation_mode != 'none':
                setting += f'_ablation-{args.ablation_mode}'
            if args.use_signal_level_stats:
                setting += f'_sl-{args.signal_stat_type}_fusion-{args.signal_stat_fusion_strategy}'
            if args.hvac_groups:
                setting += '_grouped'
            if args.use_channel_compression:
                setting += f'_compression-{args.compression_strategy}_ratio-{args.compression_ratio}'
            exp = Exp(args,setting)  # set experiments
            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
            exp.train()
            exp.evaluate_report()

            print('>>>>>>>validating : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
            exp.vali()
            
            # æ¯æ¬¡å®éªŒåæ¸…ç†å†…å­˜
            # MemoryCleaner.full_cleanup()
            
            if args.gpu_type == 'mps':
                torch.backends.mps.empty_cache()
            elif args.gpu_type == 'cuda':
                torch.cuda.empty_cache()
    else:
        # æ„å»ºå®éªŒè®¾ç½®å­—ç¬¦ä¸²
        setting = f'{args.data}_{args.model}_{args.des}_sl{args.seq_len}_step{args.step}_' \
                f'fd{args.feature_dim}_extractor-{args.extractor_type}_gaf-{args.fusion_type}_' \
                f'attention-{args.attention_type}_classifier-{args.classifier_type}'
        
        if args.use_statistical_features:
            setting += f'_stat-{args.stat_type}_fusion-{args.multimodal_fusion_strategy}'
        
        if args.ablation_mode != 'none':
            setting += f'_ablation-{args.ablation_mode}'
        if args.use_signal_level_stats:
            setting += f'_sl-{args.signal_stat_type}_fusion-{args.signal_stat_fusion_strategy}'
        if args.hvac_groups:
            setting += '_grouped'
        
        exp = Exp(args, setting)  # set experiments
        ii = 0

        print('>>>>>>>validating : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
        exp.test(setting, test=1)
        if args.gpu_type == 'mps':
            torch.backends.mps.empty_cache()
        elif args.gpu_type == 'cuda':
            torch.cuda.empty_cache()
    
    # ç¨‹åºç»“æŸæ—¶çš„æœ€ç»ˆæ¸…ç†
    # print("\n=== ç¨‹åºå³å°†ç»“æŸ ===")
    # MemoryCleaner.full_cleanup()
    # MemoryCleaner.print_memory_info()
    print("=== ç¨‹åºç»“æŸ ===\n")
